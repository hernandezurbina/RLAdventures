{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d14876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52dea693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7275ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36982d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67fc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f193a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for step in range(200):\n",
    "    env.render(mode=\"human\")\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea69382",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddaa5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7b5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1, ) + num_observations))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation(\"linear\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b90818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f43cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14329728",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e0cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11ef619",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr=\"eps\",\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=20000\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df7a1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=num_actions, memory=memory, \n",
    "               nb_steps_warmup=10, target_model_update=100, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c570cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93426b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11/20000: episode: 1, duration: 3.651s, episode steps:  11, steps per second:   3, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    36/20000: episode: 2, duration: 0.433s, episode steps:  25, steps per second:  58, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.482980, mae: 0.550593, mean_q: 0.179799, mean_eps: 0.998965\n",
      "    56/20000: episode: 3, duration: 0.260s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.291637, mae: 0.528614, mean_q: 0.411374, mean_eps: 0.997952\n",
      "    71/20000: episode: 4, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.154829, mae: 0.539721, mean_q: 0.677994, mean_eps: 0.997165\n",
      "    86/20000: episode: 5, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.070728, mae: 0.574117, mean_q: 0.949365, mean_eps: 0.996490\n",
      "   104/20000: episode: 6, duration: 0.195s, episode steps:  18, steps per second:  93, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.134789, mae: 0.691735, mean_q: 1.120004, mean_eps: 0.995748\n",
      "   118/20000: episode: 7, duration: 0.154s, episode steps:  14, steps per second:  91, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.373074, mae: 1.078606, mean_q: 1.463797, mean_eps: 0.995028\n",
      "   145/20000: episode: 8, duration: 0.284s, episode steps:  27, steps per second:  95, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.158093, mae: 1.173564, mean_q: 2.170023, mean_eps: 0.994105\n",
      "   153/20000: episode: 9, duration: 0.121s, episode steps:   8, steps per second:  66, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.114319, mae: 1.134891, mean_q: 2.124122, mean_eps: 0.993318\n",
      "   170/20000: episode: 10, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.136754, mae: 1.136712, mean_q: 2.101988, mean_eps: 0.992755\n",
      "   208/20000: episode: 11, duration: 0.590s, episode steps:  38, steps per second:  64, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.197986, mae: 1.201909, mean_q: 2.124297, mean_eps: 0.991517\n",
      "   235/20000: episode: 12, duration: 0.289s, episode steps:  27, steps per second:  93, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.245203, mae: 1.654005, mean_q: 3.011458, mean_eps: 0.990055\n",
      "   247/20000: episode: 13, duration: 0.128s, episode steps:  12, steps per second:  94, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.164542, mae: 1.622949, mean_q: 3.081768, mean_eps: 0.989177\n",
      "   275/20000: episode: 14, duration: 0.288s, episode steps:  28, steps per second:  97, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.196340, mae: 1.614943, mean_q: 3.053006, mean_eps: 0.988278\n",
      "   301/20000: episode: 15, duration: 0.292s, episode steps:  26, steps per second:  89, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.149829, mae: 1.597882, mean_q: 3.116345, mean_eps: 0.987062\n",
      "   319/20000: episode: 16, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.455315, mae: 2.126784, mean_q: 3.686352, mean_eps: 0.986073\n",
      "   351/20000: episode: 17, duration: 0.326s, episode steps:  32, steps per second:  98, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.325213, mae: 2.153230, mean_q: 4.106519, mean_eps: 0.984947\n",
      "   379/20000: episode: 18, duration: 0.287s, episode steps:  28, steps per second:  98, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.239394, mae: 2.146818, mean_q: 4.205093, mean_eps: 0.983598\n",
      "   411/20000: episode: 19, duration: 0.324s, episode steps:  32, steps per second:  99, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.364672, mae: 2.291110, mean_q: 4.238355, mean_eps: 0.982247\n",
      "   420/20000: episode: 20, duration: 0.093s, episode steps:   9, steps per second:  97, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.608083, mae: 2.756918, mean_q: 5.236550, mean_eps: 0.981325\n",
      "   430/20000: episode: 21, duration: 0.107s, episode steps:  10, steps per second:  94, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.529758, mae: 2.731733, mean_q: 5.238270, mean_eps: 0.980897\n",
      "   445/20000: episode: 22, duration: 0.162s, episode steps:  15, steps per second:  92, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.456969, mae: 2.676075, mean_q: 5.067346, mean_eps: 0.980335\n",
      "   461/20000: episode: 23, duration: 0.187s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.435242, mae: 2.696158, mean_q: 5.204983, mean_eps: 0.979637\n",
      "   508/20000: episode: 24, duration: 0.501s, episode steps:  47, steps per second:  94, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 0.441529, mae: 2.753151, mean_q: 5.208378, mean_eps: 0.978220\n",
      "   519/20000: episode: 25, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.609500, mae: 3.255029, mean_q: 6.234233, mean_eps: 0.976915\n",
      "   543/20000: episode: 26, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.764234, mae: 3.214981, mean_q: 5.963015, mean_eps: 0.976128\n",
      "   555/20000: episode: 27, duration: 0.129s, episode steps:  12, steps per second:  93, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.691591, mae: 3.208168, mean_q: 6.046910, mean_eps: 0.975318\n",
      "   572/20000: episode: 28, duration: 0.186s, episode steps:  17, steps per second:  91, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.713709, mae: 3.206278, mean_q: 5.984508, mean_eps: 0.974665\n",
      "   588/20000: episode: 29, duration: 0.172s, episode steps:  16, steps per second:  93, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.533586, mae: 3.192279, mean_q: 6.028945, mean_eps: 0.973923\n",
      "   605/20000: episode: 30, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.629205, mae: 3.305745, mean_q: 6.125207, mean_eps: 0.973180\n",
      "   622/20000: episode: 31, duration: 0.181s, episode steps:  17, steps per second:  94, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.890228, mae: 3.726869, mean_q: 6.893898, mean_eps: 0.972415\n",
      "   634/20000: episode: 32, duration: 0.175s, episode steps:  12, steps per second:  68, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 1.063894, mae: 3.716787, mean_q: 6.786924, mean_eps: 0.971763\n",
      "   677/20000: episode: 33, duration: 0.533s, episode steps:  43, steps per second:  81, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.605 [0.000, 1.000],  loss: 0.696558, mae: 3.665588, mean_q: 6.897233, mean_eps: 0.970525\n",
      "   700/20000: episode: 34, duration: 0.278s, episode steps:  23, steps per second:  83, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.729095, mae: 3.675571, mean_q: 6.864812, mean_eps: 0.969040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   722/20000: episode: 35, duration: 0.341s, episode steps:  22, steps per second:  64, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.764654, mae: 4.002561, mean_q: 7.493076, mean_eps: 0.968028\n",
      "   739/20000: episode: 36, duration: 0.180s, episode steps:  17, steps per second:  95, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.808484, mae: 4.046058, mean_q: 7.659423, mean_eps: 0.967150\n",
      "   761/20000: episode: 37, duration: 0.232s, episode steps:  22, steps per second:  95, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.869872, mae: 4.035508, mean_q: 7.558168, mean_eps: 0.966273\n",
      "   781/20000: episode: 38, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.703331, mae: 4.025028, mean_q: 7.664268, mean_eps: 0.965328\n",
      "   798/20000: episode: 39, duration: 0.177s, episode steps:  17, steps per second:  96, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.637433, mae: 3.983801, mean_q: 7.615007, mean_eps: 0.964495\n",
      "   818/20000: episode: 40, duration: 0.228s, episode steps:  20, steps per second:  88, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.853584, mae: 4.305373, mean_q: 8.020814, mean_eps: 0.963663\n",
      "   831/20000: episode: 41, duration: 0.150s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.585264, mae: 4.297027, mean_q: 8.268289, mean_eps: 0.962920\n",
      "   844/20000: episode: 42, duration: 0.175s, episode steps:  13, steps per second:  74, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.691384, mae: 4.327942, mean_q: 8.387124, mean_eps: 0.962335\n",
      "   872/20000: episode: 43, duration: 0.359s, episode steps:  28, steps per second:  78, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.662358, mae: 4.307090, mean_q: 8.385702, mean_eps: 0.961413\n",
      "   929/20000: episode: 44, duration: 0.720s, episode steps:  57, steps per second:  79, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 0.895024, mae: 4.558696, mean_q: 8.720187, mean_eps: 0.959500\n",
      "   944/20000: episode: 45, duration: 0.206s, episode steps:  15, steps per second:  73, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.901626, mae: 4.779238, mean_q: 9.217676, mean_eps: 0.957880\n",
      "   961/20000: episode: 46, duration: 0.251s, episode steps:  17, steps per second:  68, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.723797, mae: 4.764373, mean_q: 9.204422, mean_eps: 0.957160\n",
      "   977/20000: episode: 47, duration: 0.236s, episode steps:  16, steps per second:  68, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.793045, mae: 4.811771, mean_q: 9.251258, mean_eps: 0.956417\n",
      "   989/20000: episode: 48, duration: 0.169s, episode steps:  12, steps per second:  71, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.854267, mae: 4.768490, mean_q: 9.172432, mean_eps: 0.955787\n",
      "  1028/20000: episode: 49, duration: 0.593s, episode steps:  39, steps per second:  66, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 1.030132, mae: 5.033783, mean_q: 9.629107, mean_eps: 0.954640\n",
      "  1052/20000: episode: 50, duration: 0.629s, episode steps:  24, steps per second:  38, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.665919, mae: 5.164914, mean_q: 10.107181, mean_eps: 0.953222\n",
      "  1063/20000: episode: 51, duration: 0.176s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.993933, mae: 5.135652, mean_q: 9.892349, mean_eps: 0.952435\n",
      "  1085/20000: episode: 52, duration: 0.269s, episode steps:  22, steps per second:  82, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.939431, mae: 5.087949, mean_q: 9.884305, mean_eps: 0.951692\n",
      "  1108/20000: episode: 53, duration: 0.298s, episode steps:  23, steps per second:  77, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.184444, mae: 5.269648, mean_q: 10.057708, mean_eps: 0.950680\n",
      "  1148/20000: episode: 54, duration: 0.526s, episode steps:  40, steps per second:  76, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.981319, mae: 5.512351, mean_q: 10.819241, mean_eps: 0.949263\n",
      "  1199/20000: episode: 55, duration: 0.832s, episode steps:  51, steps per second:  61, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.321156, mae: 5.503490, mean_q: 10.635350, mean_eps: 0.947215\n",
      "  1234/20000: episode: 56, duration: 0.496s, episode steps:  35, steps per second:  71, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.076900, mae: 5.794277, mean_q: 11.306642, mean_eps: 0.945280\n",
      "  1246/20000: episode: 57, duration: 0.191s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.226180, mae: 5.812875, mean_q: 11.409894, mean_eps: 0.944222\n",
      "  1261/20000: episode: 58, duration: 0.219s, episode steps:  15, steps per second:  69, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.886688, mae: 5.863408, mean_q: 11.555010, mean_eps: 0.943615\n",
      "  1289/20000: episode: 59, duration: 0.376s, episode steps:  28, steps per second:  74, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.134242, mae: 5.823617, mean_q: 11.370235, mean_eps: 0.942647\n",
      "  1326/20000: episode: 60, duration: 0.476s, episode steps:  37, steps per second:  78, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.303192, mae: 6.024763, mean_q: 11.680045, mean_eps: 0.941185\n",
      "  1356/20000: episode: 61, duration: 0.472s, episode steps:  30, steps per second:  64, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 1.147814, mae: 6.120714, mean_q: 11.986827, mean_eps: 0.939678\n",
      "  1383/20000: episode: 62, duration: 0.327s, episode steps:  27, steps per second:  83, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.314415, mae: 6.051411, mean_q: 11.684900, mean_eps: 0.938395\n",
      "  1409/20000: episode: 63, duration: 0.288s, episode steps:  26, steps per second:  90, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.143970, mae: 6.219755, mean_q: 12.064231, mean_eps: 0.937202\n",
      "  1432/20000: episode: 64, duration: 0.232s, episode steps:  23, steps per second:  99, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 1.200867, mae: 6.526651, mean_q: 12.834170, mean_eps: 0.936100\n",
      "  1441/20000: episode: 65, duration: 0.125s, episode steps:   9, steps per second:  72, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.279822, mae: 6.507951, mean_q: 12.745340, mean_eps: 0.935380\n",
      "  1462/20000: episode: 66, duration: 0.227s, episode steps:  21, steps per second:  93, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.314993, mae: 6.509444, mean_q: 12.717959, mean_eps: 0.934705\n",
      "  1476/20000: episode: 67, duration: 0.258s, episode steps:  14, steps per second:  54, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.906361, mae: 6.651197, mean_q: 13.146932, mean_eps: 0.933917\n",
      "  1489/20000: episode: 68, duration: 0.142s, episode steps:  13, steps per second:  92, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.388174, mae: 6.384472, mean_q: 12.481755, mean_eps: 0.933310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1518/20000: episode: 69, duration: 0.291s, episode steps:  29, steps per second:  99, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.226316, mae: 6.773291, mean_q: 13.250874, mean_eps: 0.932365\n",
      "  1543/20000: episode: 70, duration: 0.260s, episode steps:  25, steps per second:  96, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.186306, mae: 6.909901, mean_q: 13.662804, mean_eps: 0.931150\n",
      "  1567/20000: episode: 71, duration: 0.258s, episode steps:  24, steps per second:  93, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.234279, mae: 6.954340, mean_q: 13.740529, mean_eps: 0.930048\n",
      "  1589/20000: episode: 72, duration: 0.338s, episode steps:  22, steps per second:  65, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.255546, mae: 6.905722, mean_q: 13.634317, mean_eps: 0.929013\n",
      "  1628/20000: episode: 73, duration: 0.427s, episode steps:  39, steps per second:  91, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 1.097558, mae: 7.243345, mean_q: 14.360411, mean_eps: 0.927640\n",
      "  1652/20000: episode: 74, duration: 0.246s, episode steps:  24, steps per second:  97, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.351496, mae: 7.245658, mean_q: 14.435082, mean_eps: 0.926223\n",
      "  1663/20000: episode: 75, duration: 0.115s, episode steps:  11, steps per second:  96, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.907703, mae: 7.319883, mean_q: 14.679523, mean_eps: 0.925435\n",
      "  1673/20000: episode: 76, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.590999, mae: 7.215195, mean_q: 14.451966, mean_eps: 0.924962\n",
      "  1693/20000: episode: 77, duration: 0.203s, episode steps:  20, steps per second:  99, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.124278, mae: 7.290256, mean_q: 14.735288, mean_eps: 0.924288\n",
      "  1711/20000: episode: 78, duration: 0.210s, episode steps:  18, steps per second:  86, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 1.529076, mae: 7.437700, mean_q: 14.676088, mean_eps: 0.923432\n",
      "  1764/20000: episode: 79, duration: 0.542s, episode steps:  53, steps per second:  98, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.384325, mae: 7.751586, mean_q: 15.489529, mean_eps: 0.921835\n",
      "  1773/20000: episode: 80, duration: 0.106s, episode steps:   9, steps per second:  85, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 1.151207, mae: 7.692446, mean_q: 15.331992, mean_eps: 0.920440\n",
      "  1786/20000: episode: 81, duration: 0.136s, episode steps:  13, steps per second:  96, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 1.328753, mae: 7.778259, mean_q: 15.613214, mean_eps: 0.919945\n",
      "  1796/20000: episode: 82, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.737890, mae: 7.750430, mean_q: 15.602124, mean_eps: 0.919428\n",
      "  1824/20000: episode: 83, duration: 0.287s, episode steps:  28, steps per second:  98, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.458623, mae: 8.182711, mean_q: 16.265461, mean_eps: 0.918572\n",
      "  1850/20000: episode: 84, duration: 0.268s, episode steps:  26, steps per second:  97, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 1.732898, mae: 8.287125, mean_q: 16.419316, mean_eps: 0.917358\n",
      "  1891/20000: episode: 85, duration: 0.424s, episode steps:  41, steps per second:  97, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 1.580339, mae: 8.204783, mean_q: 16.352953, mean_eps: 0.915850\n",
      "  1906/20000: episode: 86, duration: 0.160s, episode steps:  15, steps per second:  94, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.966275, mae: 8.515844, mean_q: 16.832055, mean_eps: 0.914590\n",
      "  1932/20000: episode: 87, duration: 0.274s, episode steps:  26, steps per second:  95, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.336709, mae: 8.820078, mean_q: 17.636317, mean_eps: 0.913668\n",
      "  1956/20000: episode: 88, duration: 0.259s, episode steps:  24, steps per second:  93, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.765984, mae: 8.800981, mean_q: 17.515844, mean_eps: 0.912542\n",
      "  1976/20000: episode: 89, duration: 0.213s, episode steps:  20, steps per second:  94, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.130547, mae: 8.794682, mean_q: 17.428386, mean_eps: 0.911552\n",
      "  1993/20000: episode: 90, duration: 0.189s, episode steps:  17, steps per second:  90, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 1.880726, mae: 8.578469, mean_q: 17.138227, mean_eps: 0.910720\n",
      "  2004/20000: episode: 91, duration: 0.118s, episode steps:  11, steps per second:  93, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.169409, mae: 8.758243, mean_q: 17.502794, mean_eps: 0.910090\n",
      "  2047/20000: episode: 92, duration: 0.434s, episode steps:  43, steps per second:  99, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.784095, mae: 9.201512, mean_q: 18.393671, mean_eps: 0.908875\n",
      "  2100/20000: episode: 93, duration: 0.556s, episode steps:  53, steps per second:  95, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.415 [0.000, 1.000],  loss: 1.658398, mae: 9.125970, mean_q: 18.327534, mean_eps: 0.906715\n",
      "  2157/20000: episode: 94, duration: 0.577s, episode steps:  57, steps per second:  99, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 1.430065, mae: 9.637661, mean_q: 19.406376, mean_eps: 0.904240\n",
      "  2175/20000: episode: 95, duration: 0.193s, episode steps:  18, steps per second:  93, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.673534, mae: 9.629410, mean_q: 19.174845, mean_eps: 0.902552\n",
      "  2204/20000: episode: 96, duration: 0.318s, episode steps:  29, steps per second:  91, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.345 [0.000, 1.000],  loss: 1.795247, mae: 9.754347, mean_q: 19.525948, mean_eps: 0.901495\n",
      "  2214/20000: episode: 97, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.230724, mae: 10.047533, mean_q: 20.132587, mean_eps: 0.900618\n",
      "  2225/20000: episode: 98, duration: 0.123s, episode steps:  11, steps per second:  90, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 2.116246, mae: 9.932891, mean_q: 19.777463, mean_eps: 0.900145\n",
      "  2247/20000: episode: 99, duration: 0.227s, episode steps:  22, steps per second:  97, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 1.339227, mae: 9.847462, mean_q: 19.873289, mean_eps: 0.899402\n",
      "  2262/20000: episode: 100, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.339227, mae: 9.875998, mean_q: 19.787395, mean_eps: 0.898570\n",
      "  2284/20000: episode: 101, duration: 0.241s, episode steps:  22, steps per second:  91, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 1.423373, mae: 9.937506, mean_q: 20.175928, mean_eps: 0.897738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2305/20000: episode: 102, duration: 0.231s, episode steps:  21, steps per second:  91, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 1.254059, mae: 9.930593, mean_q: 20.095972, mean_eps: 0.896770\n",
      "  2337/20000: episode: 103, duration: 0.341s, episode steps:  32, steps per second:  94, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.269908, mae: 10.519327, mean_q: 21.401091, mean_eps: 0.895578\n",
      "  2353/20000: episode: 104, duration: 0.163s, episode steps:  16, steps per second:  98, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.151393, mae: 10.390877, mean_q: 21.222381, mean_eps: 0.894497\n",
      "  2369/20000: episode: 105, duration: 0.178s, episode steps:  16, steps per second:  90, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 2.356087, mae: 10.563463, mean_q: 21.519557, mean_eps: 0.893778\n",
      "  2387/20000: episode: 106, duration: 0.192s, episode steps:  18, steps per second:  94, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.895521, mae: 10.498623, mean_q: 21.312420, mean_eps: 0.893013\n",
      "  2409/20000: episode: 107, duration: 0.241s, episode steps:  22, steps per second:  91, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 2.039410, mae: 10.364659, mean_q: 20.857875, mean_eps: 0.892112\n",
      "  2420/20000: episode: 108, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.752597, mae: 10.803832, mean_q: 21.823799, mean_eps: 0.891370\n",
      "  2438/20000: episode: 109, duration: 0.199s, episode steps:  18, steps per second:  90, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 1.393300, mae: 11.023082, mean_q: 22.381105, mean_eps: 0.890717\n",
      "  2465/20000: episode: 110, duration: 0.278s, episode steps:  27, steps per second:  97, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 1.409561, mae: 10.578983, mean_q: 21.543675, mean_eps: 0.889705\n",
      "  2490/20000: episode: 111, duration: 0.315s, episode steps:  25, steps per second:  79, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 2.039726, mae: 10.920050, mean_q: 21.988144, mean_eps: 0.888535\n",
      "  2539/20000: episode: 112, duration: 0.526s, episode steps:  49, steps per second:  93, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.394843, mae: 11.104126, mean_q: 22.536636, mean_eps: 0.886870\n",
      "  2554/20000: episode: 113, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.796646, mae: 11.293061, mean_q: 22.988917, mean_eps: 0.885430\n",
      "  2567/20000: episode: 114, duration: 0.149s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.876165, mae: 11.359932, mean_q: 23.100692, mean_eps: 0.884800\n",
      "  2599/20000: episode: 115, duration: 0.358s, episode steps:  32, steps per second:  89, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.150167, mae: 11.456767, mean_q: 23.419520, mean_eps: 0.883787\n",
      "  2625/20000: episode: 116, duration: 0.290s, episode steps:  26, steps per second:  90, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.876862, mae: 11.890136, mean_q: 24.066785, mean_eps: 0.882483\n",
      "  2638/20000: episode: 117, duration: 0.140s, episode steps:  13, steps per second:  93, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 1.448907, mae: 11.903158, mean_q: 24.361143, mean_eps: 0.881605\n",
      "  2657/20000: episode: 118, duration: 0.220s, episode steps:  19, steps per second:  86, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.932410, mae: 11.797839, mean_q: 24.021399, mean_eps: 0.880885\n",
      "  2693/20000: episode: 119, duration: 0.390s, episode steps:  36, steps per second:  92, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.473268, mae: 11.826867, mean_q: 24.152256, mean_eps: 0.879648\n",
      "  2706/20000: episode: 120, duration: 0.156s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.551511, mae: 11.777651, mean_q: 23.964141, mean_eps: 0.878545\n",
      "  2722/20000: episode: 121, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 1.036479, mae: 12.329513, mean_q: 25.281124, mean_eps: 0.877892\n",
      "  2739/20000: episode: 122, duration: 0.187s, episode steps:  17, steps per second:  91, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 2.222421, mae: 12.126567, mean_q: 24.677938, mean_eps: 0.877150\n",
      "  2788/20000: episode: 123, duration: 0.523s, episode steps:  49, steps per second:  94, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.347 [0.000, 1.000],  loss: 1.770133, mae: 12.088756, mean_q: 24.699196, mean_eps: 0.875665\n",
      "  2820/20000: episode: 124, duration: 0.351s, episode steps:  32, steps per second:  91, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.753460, mae: 12.339251, mean_q: 25.248329, mean_eps: 0.873843\n",
      "  2841/20000: episode: 125, duration: 0.234s, episode steps:  21, steps per second:  90, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.840455, mae: 12.782786, mean_q: 25.996718, mean_eps: 0.872650\n",
      "  2900/20000: episode: 126, duration: 0.655s, episode steps:  59, steps per second:  90, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.765181, mae: 12.686055, mean_q: 25.916035, mean_eps: 0.870850\n",
      "  2915/20000: episode: 127, duration: 0.166s, episode steps:  15, steps per second:  90, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.393060, mae: 13.308001, mean_q: 26.887824, mean_eps: 0.869185\n",
      "  2978/20000: episode: 128, duration: 0.684s, episode steps:  63, steps per second:  92, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.794438, mae: 13.031177, mean_q: 26.544935, mean_eps: 0.867430\n",
      "  3096/20000: episode: 129, duration: 1.235s, episode steps: 118, steps per second:  96, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.945669, mae: 13.570072, mean_q: 27.708882, mean_eps: 0.863358\n",
      "  3113/20000: episode: 130, duration: 0.194s, episode steps:  17, steps per second:  88, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 2.436516, mae: 13.855875, mean_q: 28.132947, mean_eps: 0.860320\n",
      "  3177/20000: episode: 131, duration: 0.689s, episode steps:  64, steps per second:  93, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.059916, mae: 14.043935, mean_q: 28.680231, mean_eps: 0.858498\n",
      "  3253/20000: episode: 132, duration: 0.856s, episode steps:  76, steps per second:  89, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.866711, mae: 14.602756, mean_q: 29.953351, mean_eps: 0.855348\n",
      "  3274/20000: episode: 133, duration: 0.238s, episode steps:  21, steps per second:  88, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.579458, mae: 14.847516, mean_q: 30.442621, mean_eps: 0.853165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3301/20000: episode: 134, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.101375, mae: 15.051253, mean_q: 30.737646, mean_eps: 0.852085\n",
      "  3312/20000: episode: 135, duration: 0.132s, episode steps:  11, steps per second:  83, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.765360, mae: 15.207394, mean_q: 30.888405, mean_eps: 0.851230\n",
      "  3344/20000: episode: 136, duration: 0.385s, episode steps:  32, steps per second:  83, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.779641, mae: 15.182363, mean_q: 31.041633, mean_eps: 0.850262\n",
      "  3364/20000: episode: 137, duration: 0.217s, episode steps:  20, steps per second:  92, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.225126, mae: 14.941394, mean_q: 30.688201, mean_eps: 0.849093\n",
      "  3424/20000: episode: 138, duration: 0.641s, episode steps:  60, steps per second:  94, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.245944, mae: 15.311290, mean_q: 31.327640, mean_eps: 0.847293\n",
      "  3463/20000: episode: 139, duration: 0.417s, episode steps:  39, steps per second:  94, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.869459, mae: 15.415671, mean_q: 31.686647, mean_eps: 0.845065\n",
      "  3488/20000: episode: 140, duration: 0.277s, episode steps:  25, steps per second:  90, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 1.428996, mae: 15.931343, mean_q: 32.842981, mean_eps: 0.843625\n",
      "  3514/20000: episode: 141, duration: 0.274s, episode steps:  26, steps per second:  95, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.320973, mae: 16.180924, mean_q: 33.179142, mean_eps: 0.842477\n",
      "  3551/20000: episode: 142, duration: 0.389s, episode steps:  37, steps per second:  95, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 1.806119, mae: 16.052421, mean_q: 33.058013, mean_eps: 0.841060\n",
      "  3589/20000: episode: 143, duration: 0.404s, episode steps:  38, steps per second:  94, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 2.252082, mae: 16.267281, mean_q: 33.535833, mean_eps: 0.839372\n",
      "  3685/20000: episode: 144, duration: 1.014s, episode steps:  96, steps per second:  95, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.408673, mae: 16.671976, mean_q: 34.171891, mean_eps: 0.836358\n",
      "  3740/20000: episode: 145, duration: 0.633s, episode steps:  55, steps per second:  87, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.434122, mae: 17.158184, mean_q: 35.126146, mean_eps: 0.832960\n",
      "  3757/20000: episode: 146, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.339708, mae: 17.260385, mean_q: 35.316205, mean_eps: 0.831340\n",
      "  3779/20000: episode: 147, duration: 0.249s, episode steps:  22, steps per second:  88, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.197682, mae: 17.486435, mean_q: 35.875517, mean_eps: 0.830462\n",
      "  3814/20000: episode: 148, duration: 0.385s, episode steps:  35, steps per second:  91, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 3.312673, mae: 17.628872, mean_q: 36.155302, mean_eps: 0.829180\n",
      "  3859/20000: episode: 149, duration: 0.478s, episode steps:  45, steps per second:  94, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 3.589160, mae: 17.696826, mean_q: 36.283537, mean_eps: 0.827380\n",
      "  3902/20000: episode: 150, duration: 0.462s, episode steps:  43, steps per second:  93, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 2.444501, mae: 17.875852, mean_q: 36.823625, mean_eps: 0.825400\n",
      "  3916/20000: episode: 151, duration: 0.152s, episode steps:  14, steps per second:  92, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.926125, mae: 18.132457, mean_q: 37.541410, mean_eps: 0.824117\n",
      "  3946/20000: episode: 152, duration: 0.325s, episode steps:  30, steps per second:  92, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.549067, mae: 18.237229, mean_q: 37.481367, mean_eps: 0.823128\n",
      "  4006/20000: episode: 153, duration: 0.686s, episode steps:  60, steps per second:  87, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.062942, mae: 18.521899, mean_q: 38.295967, mean_eps: 0.821103\n",
      "  4030/20000: episode: 154, duration: 0.250s, episode steps:  24, steps per second:  96, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.894636, mae: 19.760418, mean_q: 40.486830, mean_eps: 0.819212\n",
      "  4054/20000: episode: 155, duration: 0.257s, episode steps:  24, steps per second:  93, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.145775, mae: 19.360646, mean_q: 39.650367, mean_eps: 0.818132\n",
      "  4075/20000: episode: 156, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 3.230123, mae: 19.385471, mean_q: 39.674302, mean_eps: 0.817120\n",
      "  4100/20000: episode: 157, duration: 0.264s, episode steps:  25, steps per second:  95, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.397305, mae: 19.272356, mean_q: 39.651840, mean_eps: 0.816085\n",
      "  4164/20000: episode: 158, duration: 0.692s, episode steps:  64, steps per second:  93, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 3.316321, mae: 19.945181, mean_q: 40.988130, mean_eps: 0.814082\n",
      "  4206/20000: episode: 159, duration: 0.463s, episode steps:  42, steps per second:  91, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.726260, mae: 19.754498, mean_q: 40.563018, mean_eps: 0.811697\n",
      "  4223/20000: episode: 160, duration: 0.193s, episode steps:  17, steps per second:  88, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 2.507780, mae: 20.667546, mean_q: 42.380028, mean_eps: 0.810370\n",
      "  4259/20000: episode: 161, duration: 0.385s, episode steps:  36, steps per second:  94, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.424525, mae: 20.477438, mean_q: 41.898330, mean_eps: 0.809177\n",
      "  4310/20000: episode: 162, duration: 0.572s, episode steps:  51, steps per second:  89, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.585692, mae: 20.544378, mean_q: 42.362712, mean_eps: 0.807220\n",
      "  4339/20000: episode: 163, duration: 0.311s, episode steps:  29, steps per second:  93, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.614050, mae: 20.978452, mean_q: 43.170183, mean_eps: 0.805420\n",
      "  4385/20000: episode: 164, duration: 0.490s, episode steps:  46, steps per second:  94, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.784529, mae: 21.025650, mean_q: 43.192643, mean_eps: 0.803733\n",
      "  4405/20000: episode: 165, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 4.421950, mae: 21.054342, mean_q: 43.195669, mean_eps: 0.802247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4490/20000: episode: 166, duration: 0.940s, episode steps:  85, steps per second:  90, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 3.780527, mae: 21.632459, mean_q: 44.539071, mean_eps: 0.799885\n",
      "  4522/20000: episode: 167, duration: 0.365s, episode steps:  32, steps per second:  88, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.183976, mae: 22.085423, mean_q: 45.599017, mean_eps: 0.797253\n",
      "  4531/20000: episode: 168, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3.814010, mae: 21.754590, mean_q: 44.960398, mean_eps: 0.796330\n",
      "  4546/20000: episode: 169, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.539459, mae: 22.641770, mean_q: 46.432733, mean_eps: 0.795790\n",
      "  4614/20000: episode: 170, duration: 0.718s, episode steps:  68, steps per second:  95, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.090519, mae: 22.342956, mean_q: 46.053262, mean_eps: 0.793922\n",
      "  4666/20000: episode: 171, duration: 0.557s, episode steps:  52, steps per second:  93, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.811501, mae: 22.550285, mean_q: 46.232700, mean_eps: 0.791223\n",
      "  4697/20000: episode: 172, duration: 0.338s, episode steps:  31, steps per second:  92, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4.442387, mae: 22.993624, mean_q: 47.158298, mean_eps: 0.789355\n",
      "  4785/20000: episode: 173, duration: 0.972s, episode steps:  88, steps per second:  91, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 4.440388, mae: 23.450159, mean_q: 48.279227, mean_eps: 0.786678\n",
      "  4819/20000: episode: 174, duration: 0.370s, episode steps:  34, steps per second:  92, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 5.254467, mae: 23.928079, mean_q: 49.173634, mean_eps: 0.783933\n",
      "  4832/20000: episode: 175, duration: 0.146s, episode steps:  13, steps per second:  89, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 4.013156, mae: 23.767839, mean_q: 48.402777, mean_eps: 0.782875\n",
      "  4888/20000: episode: 176, duration: 0.585s, episode steps:  56, steps per second:  96, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.916725, mae: 24.434110, mean_q: 50.192929, mean_eps: 0.781323\n",
      "  4904/20000: episode: 177, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 4.011471, mae: 24.337601, mean_q: 49.912466, mean_eps: 0.779702\n",
      "  4921/20000: episode: 178, duration: 0.189s, episode steps:  17, steps per second:  90, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 5.910132, mae: 25.091322, mean_q: 51.490921, mean_eps: 0.778960\n",
      "  4931/20000: episode: 179, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 9.748754, mae: 24.227279, mean_q: 49.613493, mean_eps: 0.778353\n",
      "  4945/20000: episode: 180, duration: 0.154s, episode steps:  14, steps per second:  91, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 4.312215, mae: 24.364629, mean_q: 50.212672, mean_eps: 0.777812\n",
      "  4963/20000: episode: 181, duration: 0.189s, episode steps:  18, steps per second:  95, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 5.552382, mae: 25.000728, mean_q: 51.392982, mean_eps: 0.777092\n",
      "  5002/20000: episode: 182, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.206403, mae: 25.135630, mean_q: 51.527090, mean_eps: 0.775810\n",
      "  5017/20000: episode: 183, duration: 0.171s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.168417, mae: 25.746320, mean_q: 53.195456, mean_eps: 0.774595\n",
      "  5029/20000: episode: 184, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 5.244665, mae: 25.618038, mean_q: 52.402801, mean_eps: 0.773988\n",
      "  5064/20000: episode: 185, duration: 0.373s, episode steps:  35, steps per second:  94, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 6.765257, mae: 25.723795, mean_q: 52.799250, mean_eps: 0.772930\n",
      "  5122/20000: episode: 186, duration: 0.626s, episode steps:  58, steps per second:  93, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.037097, mae: 26.035276, mean_q: 53.350350, mean_eps: 0.770837\n",
      "  5151/20000: episode: 187, duration: 0.306s, episode steps:  29, steps per second:  95, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.032398, mae: 26.049934, mean_q: 53.355720, mean_eps: 0.768880\n",
      "  5206/20000: episode: 188, duration: 0.603s, episode steps:  55, steps per second:  91, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 5.623025, mae: 26.385825, mean_q: 54.017820, mean_eps: 0.766990\n",
      "  5220/20000: episode: 189, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.436205, mae: 26.850704, mean_q: 54.567001, mean_eps: 0.765438\n",
      "  5251/20000: episode: 190, duration: 0.329s, episode steps:  31, steps per second:  94, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6.123392, mae: 27.397176, mean_q: 55.924335, mean_eps: 0.764425\n",
      "  5352/20000: episode: 191, duration: 1.059s, episode steps: 101, steps per second:  95, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.507696, mae: 27.097696, mean_q: 55.709082, mean_eps: 0.761455\n",
      "  5367/20000: episode: 192, duration: 0.165s, episode steps:  15, steps per second:  91, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3.934320, mae: 27.762509, mean_q: 57.007730, mean_eps: 0.758845\n",
      "  5454/20000: episode: 193, duration: 0.932s, episode steps:  87, steps per second:  93, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.425448, mae: 27.639392, mean_q: 56.890519, mean_eps: 0.756550\n",
      "  5469/20000: episode: 194, duration: 0.167s, episode steps:  15, steps per second:  90, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 4.277704, mae: 27.790530, mean_q: 57.251754, mean_eps: 0.754255\n",
      "  5488/20000: episode: 195, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 6.603084, mae: 27.459002, mean_q: 56.991913, mean_eps: 0.753490\n",
      "  5521/20000: episode: 196, duration: 0.387s, episode steps:  33, steps per second:  85, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.474132, mae: 28.287202, mean_q: 58.114325, mean_eps: 0.752320\n",
      "  5580/20000: episode: 197, duration: 0.615s, episode steps:  59, steps per second:  96, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 5.103635, mae: 28.802602, mean_q: 59.324679, mean_eps: 0.750250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5613/20000: episode: 198, duration: 0.367s, episode steps:  33, steps per second:  90, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.606 [0.000, 1.000],  loss: 7.672724, mae: 29.039780, mean_q: 59.603796, mean_eps: 0.748180\n",
      "  5637/20000: episode: 199, duration: 0.263s, episode steps:  24, steps per second:  91, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 4.999712, mae: 29.166821, mean_q: 59.990496, mean_eps: 0.746898\n",
      "  5652/20000: episode: 200, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 8.399810, mae: 29.490272, mean_q: 60.835766, mean_eps: 0.746020\n",
      "  5682/20000: episode: 201, duration: 0.330s, episode steps:  30, steps per second:  91, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.627362, mae: 29.580751, mean_q: 60.600857, mean_eps: 0.745007\n",
      "  5720/20000: episode: 202, duration: 0.421s, episode steps:  38, steps per second:  90, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 8.223923, mae: 29.904878, mean_q: 61.375363, mean_eps: 0.743478\n",
      "  5772/20000: episode: 203, duration: 0.567s, episode steps:  52, steps per second:  92, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 7.561394, mae: 30.589102, mean_q: 62.740986, mean_eps: 0.741452\n",
      "  5793/20000: episode: 204, duration: 0.239s, episode steps:  21, steps per second:  88, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7.958847, mae: 29.759512, mean_q: 61.302066, mean_eps: 0.739810\n",
      "  5846/20000: episode: 205, duration: 0.570s, episode steps:  53, steps per second:  93, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.691881, mae: 30.604078, mean_q: 62.712250, mean_eps: 0.738145\n",
      "  5894/20000: episode: 206, duration: 0.512s, episode steps:  48, steps per second:  94, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 6.270564, mae: 30.811245, mean_q: 63.365438, mean_eps: 0.735873\n",
      "  5929/20000: episode: 207, duration: 0.380s, episode steps:  35, steps per second:  92, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 9.572818, mae: 31.133026, mean_q: 63.908621, mean_eps: 0.734005\n",
      "  5947/20000: episode: 208, duration: 0.200s, episode steps:  18, steps per second:  90, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 10.295942, mae: 31.309783, mean_q: 64.739826, mean_eps: 0.732813\n",
      "  5964/20000: episode: 209, duration: 0.182s, episode steps:  17, steps per second:  93, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9.085044, mae: 31.086106, mean_q: 63.592516, mean_eps: 0.732025\n",
      "  6041/20000: episode: 210, duration: 0.836s, episode steps:  77, steps per second:  92, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 6.727785, mae: 31.622497, mean_q: 65.262542, mean_eps: 0.729910\n",
      "  6072/20000: episode: 211, duration: 0.357s, episode steps:  31, steps per second:  87, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 11.066721, mae: 32.163394, mean_q: 66.096565, mean_eps: 0.727480\n",
      "  6129/20000: episode: 212, duration: 0.649s, episode steps:  57, steps per second:  88, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 6.574937, mae: 32.240812, mean_q: 66.433135, mean_eps: 0.725500\n",
      "  6147/20000: episode: 213, duration: 0.205s, episode steps:  18, steps per second:  88, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 6.125200, mae: 32.187462, mean_q: 66.525325, mean_eps: 0.723813\n",
      "  6214/20000: episode: 214, duration: 0.733s, episode steps:  67, steps per second:  91, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 7.268543, mae: 32.371839, mean_q: 66.656338, mean_eps: 0.721900\n",
      "  6226/20000: episode: 215, duration: 0.145s, episode steps:  12, steps per second:  83, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 4.469766, mae: 33.829321, mean_q: 69.372703, mean_eps: 0.720122\n",
      "  6246/20000: episode: 216, duration: 0.250s, episode steps:  20, steps per second:  80, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.710456, mae: 33.776492, mean_q: 69.559698, mean_eps: 0.719403\n",
      "  6266/20000: episode: 217, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 7.484298, mae: 32.432450, mean_q: 66.785031, mean_eps: 0.718502\n",
      "  6337/20000: episode: 218, duration: 0.769s, episode steps:  71, steps per second:  92, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 7.030093, mae: 33.568235, mean_q: 68.987386, mean_eps: 0.716455\n",
      "  6378/20000: episode: 219, duration: 0.436s, episode steps:  41, steps per second:  94, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 11.106188, mae: 33.488256, mean_q: 69.004765, mean_eps: 0.713935\n",
      "  6409/20000: episode: 220, duration: 0.344s, episode steps:  31, steps per second:  90, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 8.493127, mae: 33.650597, mean_q: 69.540608, mean_eps: 0.712315\n",
      "  6542/20000: episode: 221, duration: 1.415s, episode steps: 133, steps per second:  94, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 9.654951, mae: 34.811697, mean_q: 71.653956, mean_eps: 0.708625\n",
      "  6618/20000: episode: 222, duration: 0.820s, episode steps:  76, steps per second:  93, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 8.962304, mae: 35.548276, mean_q: 73.342652, mean_eps: 0.703923\n",
      "  6643/20000: episode: 223, duration: 0.286s, episode steps:  25, steps per second:  87, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 9.382872, mae: 36.435772, mean_q: 74.817538, mean_eps: 0.701650\n",
      "  6667/20000: episode: 224, duration: 0.275s, episode steps:  24, steps per second:  87, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 7.549111, mae: 36.603249, mean_q: 75.024948, mean_eps: 0.700547\n",
      "  6708/20000: episode: 225, duration: 0.438s, episode steps:  41, steps per second:  94, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 9.110565, mae: 36.225440, mean_q: 74.500192, mean_eps: 0.699085\n",
      "  6808/20000: episode: 226, duration: 1.065s, episode steps: 100, steps per second:  94, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 12.666565, mae: 37.178038, mean_q: 76.222297, mean_eps: 0.695913\n",
      "  6854/20000: episode: 227, duration: 0.503s, episode steps:  46, steps per second:  91, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 18.112167, mae: 37.746072, mean_q: 77.239949, mean_eps: 0.692627\n",
      "  6935/20000: episode: 228, duration: 0.864s, episode steps:  81, steps per second:  94, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 13.597041, mae: 38.210219, mean_q: 78.345792, mean_eps: 0.689770\n",
      "  6994/20000: episode: 229, duration: 0.659s, episode steps:  59, steps per second:  89, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 11.201676, mae: 38.884025, mean_q: 79.988628, mean_eps: 0.686620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7035/20000: episode: 230, duration: 0.493s, episode steps:  41, steps per second:  83, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 15.562431, mae: 39.014076, mean_q: 79.827145, mean_eps: 0.684370\n",
      "  7118/20000: episode: 231, duration: 0.892s, episode steps:  83, steps per second:  93, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 14.181838, mae: 39.529860, mean_q: 80.962977, mean_eps: 0.681580\n",
      "  7152/20000: episode: 232, duration: 0.373s, episode steps:  34, steps per second:  91, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 21.405275, mae: 39.640001, mean_q: 80.962864, mean_eps: 0.678947\n",
      "  7181/20000: episode: 233, duration: 0.316s, episode steps:  29, steps per second:  92, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 11.687950, mae: 39.538839, mean_q: 81.452792, mean_eps: 0.677530\n",
      "  7256/20000: episode: 234, duration: 0.807s, episode steps:  75, steps per second:  93, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 14.948541, mae: 40.416947, mean_q: 82.947393, mean_eps: 0.675190\n",
      "  7296/20000: episode: 235, duration: 0.427s, episode steps:  40, steps per second:  94, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 12.722853, mae: 40.612577, mean_q: 83.442366, mean_eps: 0.672602\n",
      "  7315/20000: episode: 236, duration: 0.201s, episode steps:  19, steps per second:  95, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 8.839426, mae: 40.789438, mean_q: 84.316101, mean_eps: 0.671275\n",
      "  7339/20000: episode: 237, duration: 0.270s, episode steps:  24, steps per second:  89, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 9.015232, mae: 40.791827, mean_q: 83.921854, mean_eps: 0.670307\n",
      "  7398/20000: episode: 238, duration: 0.643s, episode steps:  59, steps per second:  92, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 11.335304, mae: 40.814283, mean_q: 84.166983, mean_eps: 0.668440\n",
      "  7435/20000: episode: 239, duration: 0.418s, episode steps:  37, steps per second:  88, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 12.766606, mae: 41.610748, mean_q: 85.882103, mean_eps: 0.666280\n",
      "  7562/20000: episode: 240, duration: 1.380s, episode steps: 127, steps per second:  92, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 13.111981, mae: 42.064027, mean_q: 86.117379, mean_eps: 0.662590\n",
      "  7596/20000: episode: 241, duration: 0.379s, episode steps:  34, steps per second:  90, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 15.672183, mae: 43.047294, mean_q: 88.409427, mean_eps: 0.658968\n",
      "  7739/20000: episode: 242, duration: 1.537s, episode steps: 143, steps per second:  93, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 15.314255, mae: 43.714372, mean_q: 89.908654, mean_eps: 0.654985\n",
      "  7773/20000: episode: 243, duration: 0.396s, episode steps:  34, steps per second:  86, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 17.058533, mae: 44.600048, mean_q: 91.425895, mean_eps: 0.651002\n",
      "  7917/20000: episode: 244, duration: 1.587s, episode steps: 144, steps per second:  91, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 15.265971, mae: 44.944019, mean_q: 92.250933, mean_eps: 0.646998\n",
      "  8017/20000: episode: 245, duration: 1.155s, episode steps: 100, steps per second:  87, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.731869, mae: 45.299696, mean_q: 93.046858, mean_eps: 0.641508\n",
      "  8059/20000: episode: 246, duration: 0.481s, episode steps:  42, steps per second:  87, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 19.940674, mae: 46.907277, mean_q: 95.953334, mean_eps: 0.638313\n",
      "  8224/20000: episode: 247, duration: 1.752s, episode steps: 165, steps per second:  94, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 20.430835, mae: 47.247984, mean_q: 96.918223, mean_eps: 0.633655\n",
      "  8333/20000: episode: 248, duration: 1.192s, episode steps: 109, steps per second:  91, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 21.311008, mae: 48.945207, mean_q: 100.071939, mean_eps: 0.627490\n",
      "  8429/20000: episode: 249, duration: 1.063s, episode steps:  96, steps per second:  90, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 20.218053, mae: 49.236131, mean_q: 101.172034, mean_eps: 0.622877\n",
      "  8493/20000: episode: 250, duration: 0.679s, episode steps:  64, steps per second:  94, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.480056, mae: 49.152264, mean_q: 101.294903, mean_eps: 0.619278\n",
      "  8643/20000: episode: 251, duration: 1.646s, episode steps: 150, steps per second:  91, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 18.378537, mae: 50.087898, mean_q: 102.636912, mean_eps: 0.614462\n",
      "  8692/20000: episode: 252, duration: 0.539s, episode steps:  49, steps per second:  91, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 25.770330, mae: 51.467138, mean_q: 105.521549, mean_eps: 0.609985\n",
      "  8739/20000: episode: 253, duration: 0.511s, episode steps:  47, steps per second:  92, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 28.246436, mae: 51.955802, mean_q: 106.382265, mean_eps: 0.607825\n",
      "  8781/20000: episode: 254, duration: 0.470s, episode steps:  42, steps per second:  89, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 19.863113, mae: 52.548713, mean_q: 107.411406, mean_eps: 0.605823\n",
      "  8905/20000: episode: 255, duration: 1.371s, episode steps: 124, steps per second:  90, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 25.779637, mae: 52.890637, mean_q: 108.273052, mean_eps: 0.602088\n",
      "  8944/20000: episode: 256, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 28.649449, mae: 53.337355, mean_q: 109.302425, mean_eps: 0.598420\n",
      "  9032/20000: episode: 257, duration: 0.967s, episode steps:  88, steps per second:  91, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 19.440648, mae: 53.927874, mean_q: 110.324273, mean_eps: 0.595562\n",
      "  9232/20000: episode: 258, duration: 2.103s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 21.642566, mae: 55.191326, mean_q: 113.178080, mean_eps: 0.589083\n",
      "  9416/20000: episode: 259, duration: 2.042s, episode steps: 184, steps per second:  90, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 20.834731, mae: 56.477240, mean_q: 115.797060, mean_eps: 0.580442\n",
      "  9574/20000: episode: 260, duration: 1.682s, episode steps: 158, steps per second:  94, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 20.831335, mae: 58.048268, mean_q: 118.913060, mean_eps: 0.572747\n",
      "  9774/20000: episode: 261, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 29.149703, mae: 58.718270, mean_q: 120.608194, mean_eps: 0.564692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9974/20000: episode: 262, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 30.107876, mae: 60.259772, mean_q: 123.173327, mean_eps: 0.555692\n",
      " 10085/20000: episode: 263, duration: 1.194s, episode steps: 111, steps per second:  93, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 30.226055, mae: 61.229475, mean_q: 125.461075, mean_eps: 0.548695\n",
      " 10135/20000: episode: 264, duration: 0.542s, episode steps:  50, steps per second:  92, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 34.520819, mae: 62.173015, mean_q: 127.481331, mean_eps: 0.545072\n",
      " 10276/20000: episode: 265, duration: 1.518s, episode steps: 141, steps per second:  93, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 26.974656, mae: 62.703698, mean_q: 128.601731, mean_eps: 0.540775\n",
      " 10476/20000: episode: 266, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 31.286783, mae: 64.223912, mean_q: 131.318168, mean_eps: 0.533102\n",
      " 10597/20000: episode: 267, duration: 1.304s, episode steps: 121, steps per second:  93, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 26.936575, mae: 64.923244, mean_q: 133.043260, mean_eps: 0.525880\n",
      " 10714/20000: episode: 268, duration: 1.284s, episode steps: 117, steps per second:  91, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 37.368094, mae: 65.577948, mean_q: 134.087874, mean_eps: 0.520525\n",
      " 10871/20000: episode: 269, duration: 1.653s, episode steps: 157, steps per second:  95, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 31.577003, mae: 66.980092, mean_q: 137.336288, mean_eps: 0.514360\n",
      " 10909/20000: episode: 270, duration: 0.407s, episode steps:  38, steps per second:  93, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 37.379837, mae: 67.562023, mean_q: 138.472590, mean_eps: 0.509972\n",
      " 11030/20000: episode: 271, duration: 1.301s, episode steps: 121, steps per second:  93, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 26.302564, mae: 67.643387, mean_q: 138.538608, mean_eps: 0.506395\n",
      " 11190/20000: episode: 272, duration: 1.700s, episode steps: 160, steps per second:  94, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 31.037542, mae: 69.006895, mean_q: 141.238868, mean_eps: 0.500072\n",
      " 11375/20000: episode: 273, duration: 1.965s, episode steps: 185, steps per second:  94, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 35.979416, mae: 69.830451, mean_q: 142.588605, mean_eps: 0.492310\n",
      " 11548/20000: episode: 274, duration: 1.862s, episode steps: 173, steps per second:  93, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 31.885324, mae: 70.552741, mean_q: 144.519640, mean_eps: 0.484255\n",
      " 11578/20000: episode: 275, duration: 0.336s, episode steps:  30, steps per second:  89, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 49.859255, mae: 70.693820, mean_q: 144.377135, mean_eps: 0.479687\n",
      " 11593/20000: episode: 276, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 32.286674, mae: 71.467001, mean_q: 145.767681, mean_eps: 0.478675\n",
      " 11793/20000: episode: 277, duration: 2.268s, episode steps: 200, steps per second:  88, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 36.954224, mae: 71.965637, mean_q: 147.299400, mean_eps: 0.473837\n",
      " 11928/20000: episode: 278, duration: 1.447s, episode steps: 135, steps per second:  93, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 45.483503, mae: 73.693592, mean_q: 150.545049, mean_eps: 0.466300\n",
      " 12012/20000: episode: 279, duration: 0.886s, episode steps:  84, steps per second:  95, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 30.989877, mae: 74.055379, mean_q: 152.211317, mean_eps: 0.461373\n",
      " 12159/20000: episode: 280, duration: 1.571s, episode steps: 147, steps per second:  94, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 34.938945, mae: 74.665135, mean_q: 152.957509, mean_eps: 0.456175\n",
      " 12359/20000: episode: 281, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 41.068215, mae: 75.644235, mean_q: 154.743296, mean_eps: 0.448367\n",
      " 12404/20000: episode: 282, duration: 0.494s, episode steps:  45, steps per second:  91, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 36.687322, mae: 76.806784, mean_q: 156.576049, mean_eps: 0.442855\n",
      " 12604/20000: episode: 283, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 34.711862, mae: 77.288307, mean_q: 158.167406, mean_eps: 0.437342\n",
      " 12802/20000: episode: 284, duration: 2.125s, episode steps: 198, steps per second:  93, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 47.052019, mae: 78.702694, mean_q: 160.498421, mean_eps: 0.428387\n",
      " 12928/20000: episode: 285, duration: 1.361s, episode steps: 126, steps per second:  93, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 39.973762, mae: 79.839142, mean_q: 163.122591, mean_eps: 0.421097\n",
      " 13128/20000: episode: 286, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 35.867680, mae: 80.034697, mean_q: 163.609223, mean_eps: 0.413762\n",
      " 13284/20000: episode: 287, duration: 1.735s, episode steps: 156, steps per second:  90, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 49.929143, mae: 80.950097, mean_q: 165.218789, mean_eps: 0.405753\n",
      " 13484/20000: episode: 288, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 56.607739, mae: 82.283481, mean_q: 167.736731, mean_eps: 0.397742\n",
      " 13684/20000: episode: 289, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 43.595766, mae: 82.802707, mean_q: 169.475984, mean_eps: 0.388742\n",
      " 13723/20000: episode: 290, duration: 0.425s, episode steps:  39, steps per second:  92, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 41.233957, mae: 82.606363, mean_q: 168.803645, mean_eps: 0.383365\n",
      " 13923/20000: episode: 291, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 54.347856, mae: 83.845495, mean_q: 171.020997, mean_eps: 0.377987\n",
      " 14123/20000: episode: 292, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 79.354162, mae: 84.468746, mean_q: 172.002479, mean_eps: 0.368987\n",
      " 14323/20000: episode: 293, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 40.666115, mae: 84.153078, mean_q: 171.624967, mean_eps: 0.359988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14523/20000: episode: 294, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 35.384257, mae: 85.070506, mean_q: 173.565745, mean_eps: 0.350987\n",
      " 14723/20000: episode: 295, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 47.378766, mae: 85.191408, mean_q: 173.758348, mean_eps: 0.341987\n",
      " 14923/20000: episode: 296, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 43.784523, mae: 85.289763, mean_q: 173.807012, mean_eps: 0.332987\n",
      " 15123/20000: episode: 297, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 49.772763, mae: 85.170283, mean_q: 173.647754, mean_eps: 0.323987\n",
      " 15323/20000: episode: 298, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 36.989121, mae: 85.681707, mean_q: 174.771494, mean_eps: 0.314987\n",
      " 15423/20000: episode: 299, duration: 1.052s, episode steps: 100, steps per second:  95, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 45.343629, mae: 86.171376, mean_q: 176.114715, mean_eps: 0.308237\n",
      " 15623/20000: episode: 300, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 54.028611, mae: 86.529583, mean_q: 176.369307, mean_eps: 0.301487\n",
      " 15823/20000: episode: 301, duration: 2.103s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 41.308353, mae: 87.656930, mean_q: 178.631032, mean_eps: 0.292487\n",
      " 16023/20000: episode: 302, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 52.375984, mae: 87.415773, mean_q: 178.108867, mean_eps: 0.283487\n",
      " 16223/20000: episode: 303, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 51.419829, mae: 87.464489, mean_q: 177.809581, mean_eps: 0.274487\n",
      " 16423/20000: episode: 304, duration: 2.293s, episode steps: 200, steps per second:  87, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 42.699175, mae: 88.584238, mean_q: 180.030781, mean_eps: 0.265487\n",
      " 16590/20000: episode: 305, duration: 1.775s, episode steps: 167, steps per second:  94, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 72.485346, mae: 88.722150, mean_q: 180.073933, mean_eps: 0.257230\n",
      " 16788/20000: episode: 306, duration: 2.088s, episode steps: 198, steps per second:  95, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 44.020752, mae: 87.896793, mean_q: 178.595302, mean_eps: 0.249018\n",
      " 16988/20000: episode: 307, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 47.618383, mae: 88.452463, mean_q: 180.031213, mean_eps: 0.240062\n",
      " 17188/20000: episode: 308, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 50.797577, mae: 87.042348, mean_q: 176.994821, mean_eps: 0.231062\n",
      " 17388/20000: episode: 309, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 47.799033, mae: 88.199624, mean_q: 179.191087, mean_eps: 0.222062\n",
      " 17588/20000: episode: 310, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 37.467586, mae: 88.648602, mean_q: 180.144670, mean_eps: 0.213062\n",
      " 17788/20000: episode: 311, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 35.773722, mae: 87.772483, mean_q: 178.460662, mean_eps: 0.204062\n",
      " 17988/20000: episode: 312, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 62.112197, mae: 87.810319, mean_q: 178.430295, mean_eps: 0.195062\n",
      " 18188/20000: episode: 313, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 55.469854, mae: 87.376612, mean_q: 176.772601, mean_eps: 0.186062\n",
      " 18388/20000: episode: 314, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 37.282014, mae: 86.517904, mean_q: 175.998277, mean_eps: 0.177062\n",
      " 18588/20000: episode: 315, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 65.883265, mae: 86.946014, mean_q: 176.093474, mean_eps: 0.168062\n",
      " 18788/20000: episode: 316, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 51.105600, mae: 86.554575, mean_q: 175.651538, mean_eps: 0.159062\n",
      " 18988/20000: episode: 317, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 60.409123, mae: 86.938933, mean_q: 176.938926, mean_eps: 0.150062\n",
      " 19188/20000: episode: 318, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 39.606425, mae: 86.488711, mean_q: 176.032836, mean_eps: 0.141062\n",
      " 19388/20000: episode: 319, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 47.336967, mae: 86.375228, mean_q: 175.415130, mean_eps: 0.132062\n",
      " 19588/20000: episode: 320, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 49.840514, mae: 86.522307, mean_q: 175.603012, mean_eps: 0.123062\n",
      " 19788/20000: episode: 321, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 41.914040, mae: 86.561175, mean_q: 175.595069, mean_eps: 0.114062\n",
      " 19988/20000: episode: 322, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 43.040733, mae: 86.042722, mean_q: 174.589871, mean_eps: 0.105062\n",
      "done, took 223.869 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2178cd2e0b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3921d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(f\"my_weights_cartpole.h5f\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22aad68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda69c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

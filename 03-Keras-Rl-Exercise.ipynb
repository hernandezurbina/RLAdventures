{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advisory-score",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-agency",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement your first keras-rl agent based on the **Acrobot** environment (https://gym.openai.com/envs/Acrobot-v1/) <br />\n",
    "The goal of this environment is to maneuver the robot arm upwards above the line with as little steps as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-conducting",
   "metadata": {},
   "source": [
    "**TASK: Import necessary libraries** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "**TASK: Create the environment** <br />\n",
    "The name is: *Acrobot-v1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: (6,)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == (6,) , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent** <br />\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want! <br />\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1, ) + num_observations))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation(\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acdf82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mixture",
   "metadata": {},
   "source": [
    "**TASK: Initialize the circular buffer**<br />\n",
    "Make sure you set the limit appropriately (50000 works well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e90589",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-grain",
   "metadata": {},
   "source": [
    "**TASK: Use the epsilon greedy action selection strategy with *decaying* epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e86cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr=\"eps\",\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=150000\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-straight",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=1000, *target_model_update*=1000, *batch_size*=32 and *gamma*=0.99 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=num_actions, memory=memory, \n",
    "               nb_steps_warmup=1000, target_model_update=1000, policy=policy,\n",
    "               gamma=0.99, batch_size=32\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-shooting",
   "metadata": {},
   "source": [
    "**TASK: Compile the model** <br />\n",
    "Feel free to explore the effects of different optimizers and learning rates.\n",
    "You can try Adam with a learning rate of 1e-3 as a first guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "**TASK: Fit the model** <br />\n",
    "150,000 steps should be a very good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adverse-determination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    500/150000: episode: 1, duration: 1.548s, episode steps: 500, steps per second: 323, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1000/150000: episode: 2, duration: 0.774s, episode steps: 500, steps per second: 646, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1500/150000: episode: 3, duration: 6.676s, episode steps: 500, steps per second:  75, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.007188, mae: 0.512267, mean_q: -0.701002, mean_eps: 0.992500\n",
      "   2000/150000: episode: 4, duration: 5.994s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.000415, mae: 0.502049, mean_q: -0.718353, mean_eps: 0.989503\n",
      "   2500/150000: episode: 5, duration: 5.957s, episode steps: 500, steps per second:  84, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.008334, mae: 1.256795, mean_q: -1.825103, mean_eps: 0.986503\n",
      "   3000/150000: episode: 6, duration: 6.069s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.000755, mae: 1.254379, mean_q: -1.850062, mean_eps: 0.983503\n",
      "   3500/150000: episode: 7, duration: 6.013s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.010758, mae: 1.992629, mean_q: -2.922840, mean_eps: 0.980503\n",
      "   4000/150000: episode: 8, duration: 6.031s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.003996, mae: 1.984792, mean_q: -2.931736, mean_eps: 0.977503\n",
      "   4500/150000: episode: 9, duration: 5.860s, episode steps: 500, steps per second:  85, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.018716, mae: 2.721047, mean_q: -3.997700, mean_eps: 0.974503\n",
      "   5000/150000: episode: 10, duration: 5.951s, episode steps: 500, steps per second:  84, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.008602, mae: 2.714121, mean_q: -4.011946, mean_eps: 0.971503\n",
      "   5500/150000: episode: 11, duration: 6.100s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.021932, mae: 3.470738, mean_q: -5.116178, mean_eps: 0.968503\n",
      "   6000/150000: episode: 12, duration: 6.027s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.017574, mae: 3.462865, mean_q: -5.113031, mean_eps: 0.965503\n",
      "   6500/150000: episode: 13, duration: 6.266s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.035193, mae: 4.154124, mean_q: -6.119267, mean_eps: 0.962503\n",
      "   7000/150000: episode: 14, duration: 7.257s, episode steps: 500, steps per second:  69, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.021437, mae: 4.148898, mean_q: -6.134826, mean_eps: 0.959503\n",
      "   7500/150000: episode: 15, duration: 7.083s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.043419, mae: 4.829506, mean_q: -7.118118, mean_eps: 0.956503\n",
      "   8000/150000: episode: 16, duration: 7.426s, episode steps: 500, steps per second:  67, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.041811, mae: 4.825801, mean_q: -7.121843, mean_eps: 0.953503\n",
      "   8500/150000: episode: 17, duration: 7.828s, episode steps: 500, steps per second:  64, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.044309, mae: 5.481812, mean_q: -8.087861, mean_eps: 0.950503\n",
      "   9000/150000: episode: 18, duration: 6.542s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.056945, mae: 5.479310, mean_q: -8.073233, mean_eps: 0.947503\n",
      "   9500/150000: episode: 19, duration: 6.808s, episode steps: 500, steps per second:  73, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.064327, mae: 6.089572, mean_q: -8.990414, mean_eps: 0.944503\n",
      "  10000/150000: episode: 20, duration: 8.352s, episode steps: 500, steps per second:  60, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 0.067406, mae: 6.083281, mean_q: -8.985720, mean_eps: 0.941503\n",
      "  10500/150000: episode: 21, duration: 6.532s, episode steps: 500, steps per second:  77, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000],  loss: 0.119501, mae: 6.809577, mean_q: -10.034978, mean_eps: 0.938503\n",
      "  11000/150000: episode: 22, duration: 6.916s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.105621, mae: 6.806211, mean_q: -10.043239, mean_eps: 0.935503\n",
      "  11500/150000: episode: 23, duration: 6.722s, episode steps: 500, steps per second:  74, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.126869, mae: 7.472856, mean_q: -11.024449, mean_eps: 0.932503\n",
      "  12000/150000: episode: 24, duration: 6.241s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.137886, mae: 7.470075, mean_q: -11.017754, mean_eps: 0.929503\n",
      "  12500/150000: episode: 25, duration: 7.433s, episode steps: 500, steps per second:  67, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.107667, mae: 8.115275, mean_q: -11.996715, mean_eps: 0.926503\n",
      "  13000/150000: episode: 26, duration: 6.993s, episode steps: 500, steps per second:  71, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000],  loss: 0.119191, mae: 8.116281, mean_q: -12.005221, mean_eps: 0.923503\n",
      "  13500/150000: episode: 27, duration: 6.703s, episode steps: 500, steps per second:  75, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.116771, mae: 8.718452, mean_q: -12.903111, mean_eps: 0.920503\n",
      "  14000/150000: episode: 28, duration: 6.619s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.121200, mae: 8.715057, mean_q: -12.904157, mean_eps: 0.917503\n",
      "  14500/150000: episode: 29, duration: 7.370s, episode steps: 500, steps per second:  68, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 0.195751, mae: 9.303872, mean_q: -13.745534, mean_eps: 0.914503\n",
      "  15000/150000: episode: 30, duration: 7.367s, episode steps: 500, steps per second:  68, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.170900, mae: 9.299743, mean_q: -13.759345, mean_eps: 0.911503\n",
      "  15500/150000: episode: 31, duration: 7.545s, episode steps: 500, steps per second:  66, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.120343, mae: 9.956026, mean_q: -14.750282, mean_eps: 0.908503\n",
      "  15783/150000: episode: 32, duration: 4.279s, episode steps: 283, steps per second:  66, episode reward: -282.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.171663, mae: 9.949203, mean_q: -14.717752, mean_eps: 0.906154\n",
      "  16283/150000: episode: 33, duration: 6.650s, episode steps: 500, steps per second:  75, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.181418, mae: 10.221091, mean_q: -15.117324, mean_eps: 0.903805\n",
      "  16783/150000: episode: 34, duration: 7.791s, episode steps: 500, steps per second:  64, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.225837, mae: 10.426541, mean_q: -15.421841, mean_eps: 0.900805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  17283/150000: episode: 35, duration: 6.979s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.217788, mae: 10.713266, mean_q: -15.849277, mean_eps: 0.897805\n",
      "  17783/150000: episode: 36, duration: 6.983s, episode steps: 500, steps per second:  72, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.199262, mae: 10.931666, mean_q: -16.176904, mean_eps: 0.894805\n",
      "  18283/150000: episode: 37, duration: 7.998s, episode steps: 500, steps per second:  63, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.247269, mae: 11.218981, mean_q: -16.576819, mean_eps: 0.891805\n",
      "  18783/150000: episode: 38, duration: 8.743s, episode steps: 500, steps per second:  57, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.188406, mae: 11.433496, mean_q: -16.934231, mean_eps: 0.888805\n",
      "  19283/150000: episode: 39, duration: 8.615s, episode steps: 500, steps per second:  58, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.170992, mae: 11.726709, mean_q: -17.367445, mean_eps: 0.885805\n",
      "  19783/150000: episode: 40, duration: 8.413s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.359053, mae: 11.939279, mean_q: -17.627182, mean_eps: 0.882805\n",
      "  20283/150000: episode: 41, duration: 8.673s, episode steps: 500, steps per second:  58, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.934 [0.000, 2.000],  loss: 0.298566, mae: 12.153918, mean_q: -17.964521, mean_eps: 0.879805\n",
      "  20783/150000: episode: 42, duration: 8.409s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 0.344103, mae: 12.304532, mean_q: -18.177001, mean_eps: 0.876805\n",
      "  21283/150000: episode: 43, duration: 8.530s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.302296, mae: 12.586609, mean_q: -18.615119, mean_eps: 0.873805\n",
      "  21783/150000: episode: 44, duration: 8.516s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.326992, mae: 12.798193, mean_q: -18.919562, mean_eps: 0.870805\n",
      "  22283/150000: episode: 45, duration: 8.535s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.982 [0.000, 2.000],  loss: 0.402943, mae: 13.083146, mean_q: -19.339693, mean_eps: 0.867805\n",
      "  22783/150000: episode: 46, duration: 8.470s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.360273, mae: 13.285466, mean_q: -19.643636, mean_eps: 0.864805\n",
      "  23283/150000: episode: 47, duration: 8.463s, episode steps: 500, steps per second:  59, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.369968, mae: 13.537729, mean_q: -20.022145, mean_eps: 0.861805\n",
      "  23646/150000: episode: 48, duration: 4.324s, episode steps: 363, steps per second:  84, episode reward: -362.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.333193, mae: 13.738473, mean_q: -20.320502, mean_eps: 0.859216\n",
      "  24146/150000: episode: 49, duration: 6.028s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.452444, mae: 13.880198, mean_q: -20.498047, mean_eps: 0.856627\n",
      "  24646/150000: episode: 50, duration: 6.284s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.335598, mae: 14.188933, mean_q: -20.981138, mean_eps: 0.853627\n",
      "  24899/150000: episode: 51, duration: 3.169s, episode steps: 253, steps per second:  80, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.446081, mae: 14.194982, mean_q: -20.956565, mean_eps: 0.851368\n",
      "  25399/150000: episode: 52, duration: 6.301s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.403327, mae: 14.484995, mean_q: -21.403272, mean_eps: 0.849109\n",
      "  25759/150000: episode: 53, duration: 4.360s, episode steps: 360, steps per second:  83, episode reward: -359.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.389809, mae: 14.561791, mean_q: -21.532051, mean_eps: 0.846529\n",
      "  26259/150000: episode: 54, duration: 6.088s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000],  loss: 0.346648, mae: 14.783923, mean_q: -21.870992, mean_eps: 0.843949\n",
      "  26759/150000: episode: 55, duration: 6.494s, episode steps: 500, steps per second:  77, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.624843, mae: 14.976823, mean_q: -22.084950, mean_eps: 0.840949\n",
      "  27259/150000: episode: 56, duration: 6.370s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.426345, mae: 15.115935, mean_q: -22.347723, mean_eps: 0.837949\n",
      "  27696/150000: episode: 57, duration: 5.568s, episode steps: 437, steps per second:  78, episode reward: -436.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.984 [0.000, 2.000],  loss: 0.407829, mae: 15.264002, mean_q: -22.571915, mean_eps: 0.835138\n",
      "  28196/150000: episode: 58, duration: 6.307s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.418900, mae: 15.451166, mean_q: -22.856849, mean_eps: 0.832327\n",
      "  28696/150000: episode: 59, duration: 6.298s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.946 [0.000, 2.000],  loss: 0.417914, mae: 15.768038, mean_q: -23.310227, mean_eps: 0.829327\n",
      "  29196/150000: episode: 60, duration: 6.315s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.470718, mae: 15.946578, mean_q: -23.572297, mean_eps: 0.826327\n",
      "  29696/150000: episode: 61, duration: 6.202s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.491063, mae: 16.178058, mean_q: -23.899448, mean_eps: 0.823327\n",
      "  30196/150000: episode: 62, duration: 6.232s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000],  loss: 0.446838, mae: 16.277745, mean_q: -24.052232, mean_eps: 0.820327\n",
      "  30684/150000: episode: 63, duration: 6.095s, episode steps: 488, steps per second:  80, episode reward: -487.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.918 [0.000, 2.000],  loss: 0.422173, mae: 16.370003, mean_q: -24.199814, mean_eps: 0.817363\n",
      "  31184/150000: episode: 64, duration: 6.264s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000],  loss: 0.457873, mae: 16.496551, mean_q: -24.372525, mean_eps: 0.814399\n",
      "  31585/150000: episode: 65, duration: 5.055s, episode steps: 401, steps per second:  79, episode reward: -400.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.377362, mae: 16.719805, mean_q: -24.750784, mean_eps: 0.811696\n",
      "  32085/150000: episode: 66, duration: 6.300s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.521644, mae: 16.735353, mean_q: -24.739691, mean_eps: 0.808993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32585/150000: episode: 67, duration: 6.254s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.353642, mae: 16.667734, mean_q: -24.686428, mean_eps: 0.805993\n",
      "  33085/150000: episode: 68, duration: 6.320s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.888 [0.000, 2.000],  loss: 0.513549, mae: 16.751382, mean_q: -24.759036, mean_eps: 0.802993\n",
      "  33410/150000: episode: 69, duration: 4.127s, episode steps: 325, steps per second:  79, episode reward: -324.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.532712, mae: 17.032548, mean_q: -25.169402, mean_eps: 0.800518\n",
      "  33910/150000: episode: 70, duration: 6.443s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.554668, mae: 16.994314, mean_q: -25.117097, mean_eps: 0.798043\n",
      "  34410/150000: episode: 71, duration: 6.416s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000],  loss: 0.569220, mae: 17.231815, mean_q: -25.451769, mean_eps: 0.795043\n",
      "  34910/150000: episode: 72, duration: 6.338s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.096 [0.000, 2.000],  loss: 0.581182, mae: 17.265343, mean_q: -25.488976, mean_eps: 0.792043\n",
      "  35410/150000: episode: 73, duration: 5.957s, episode steps: 500, steps per second:  84, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000],  loss: 0.547678, mae: 17.315200, mean_q: -25.598457, mean_eps: 0.789043\n",
      "  35910/150000: episode: 74, duration: 6.220s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.661662, mae: 17.278981, mean_q: -25.509677, mean_eps: 0.786043\n",
      "  36410/150000: episode: 75, duration: 6.347s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.509913, mae: 17.540390, mean_q: -25.941941, mean_eps: 0.783043\n",
      "  36910/150000: episode: 76, duration: 6.366s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.373688, mae: 17.590829, mean_q: -26.039635, mean_eps: 0.780043\n",
      "  37354/150000: episode: 77, duration: 5.665s, episode steps: 444, steps per second:  78, episode reward: -443.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.365643, mae: 17.845754, mean_q: -26.412033, mean_eps: 0.777211\n",
      "  37854/150000: episode: 78, duration: 6.339s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.483088, mae: 17.937729, mean_q: -26.533078, mean_eps: 0.774379\n",
      "  38354/150000: episode: 79, duration: 6.286s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000],  loss: 0.504864, mae: 18.222563, mean_q: -26.946765, mean_eps: 0.771379\n",
      "  38594/150000: episode: 80, duration: 3.074s, episode steps: 240, steps per second:  78, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.431798, mae: 18.334417, mean_q: -27.128271, mean_eps: 0.769159\n",
      "  39028/150000: episode: 81, duration: 5.428s, episode steps: 434, steps per second:  80, episode reward: -433.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.601356, mae: 18.375867, mean_q: -27.147645, mean_eps: 0.767137\n",
      "  39417/150000: episode: 82, duration: 5.078s, episode steps: 389, steps per second:  77, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.728287, mae: 18.730512, mean_q: -27.646211, mean_eps: 0.764668\n",
      "  39650/150000: episode: 83, duration: 2.989s, episode steps: 233, steps per second:  78, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.803616, mae: 18.724837, mean_q: -27.635744, mean_eps: 0.762802\n",
      "  40150/150000: episode: 84, duration: 6.327s, episode steps: 500, steps per second:  79, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.841199, mae: 18.759898, mean_q: -27.692872, mean_eps: 0.760603\n",
      "  40558/150000: episode: 85, duration: 5.171s, episode steps: 408, steps per second:  79, episode reward: -407.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.629730, mae: 18.926517, mean_q: -27.968601, mean_eps: 0.757879\n",
      "  40839/150000: episode: 86, duration: 3.579s, episode steps: 281, steps per second:  79, episode reward: -280.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.755845, mae: 18.821099, mean_q: -27.786173, mean_eps: 0.755812\n",
      "  41111/150000: episode: 87, duration: 3.551s, episode steps: 272, steps per second:  77, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.710460, mae: 19.043394, mean_q: -28.118244, mean_eps: 0.754153\n",
      "  41514/150000: episode: 88, duration: 5.239s, episode steps: 403, steps per second:  77, episode reward: -402.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.647114, mae: 19.270193, mean_q: -28.495058, mean_eps: 0.752128\n",
      "  41975/150000: episode: 89, duration: 6.097s, episode steps: 461, steps per second:  76, episode reward: -460.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.820193, mae: 19.231612, mean_q: -28.401300, mean_eps: 0.749536\n",
      "  42475/150000: episode: 90, duration: 6.864s, episode steps: 500, steps per second:  73, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.683811, mae: 19.603615, mean_q: -28.968738, mean_eps: 0.746653\n",
      "  42829/150000: episode: 91, duration: 5.517s, episode steps: 354, steps per second:  64, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.887 [0.000, 2.000],  loss: 0.733488, mae: 19.615446, mean_q: -28.994041, mean_eps: 0.744091\n",
      "  43328/150000: episode: 92, duration: 7.426s, episode steps: 499, steps per second:  67, episode reward: -498.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.749729, mae: 19.889755, mean_q: -29.370691, mean_eps: 0.741532\n",
      "  43828/150000: episode: 93, duration: 7.297s, episode steps: 500, steps per second:  69, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.633551, mae: 19.928504, mean_q: -29.457307, mean_eps: 0.738535\n",
      "  44311/150000: episode: 94, duration: 6.276s, episode steps: 483, steps per second:  77, episode reward: -482.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.718834, mae: 19.955144, mean_q: -29.468860, mean_eps: 0.735586\n",
      "  44790/150000: episode: 95, duration: 5.894s, episode steps: 479, steps per second:  81, episode reward: -478.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.040 [0.000, 2.000],  loss: 0.886817, mae: 20.009392, mean_q: -29.518644, mean_eps: 0.732700\n",
      "  45064/150000: episode: 96, duration: 3.450s, episode steps: 274, steps per second:  79, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.811226, mae: 20.054616, mean_q: -29.581352, mean_eps: 0.730441\n",
      "  45564/150000: episode: 97, duration: 6.385s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000],  loss: 0.760309, mae: 20.429964, mean_q: -30.151765, mean_eps: 0.728119\n",
      "  46009/150000: episode: 98, duration: 5.750s, episode steps: 445, steps per second:  77, episode reward: -444.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.605435, mae: 20.357813, mean_q: -30.065965, mean_eps: 0.725284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  46509/150000: episode: 99, duration: 6.548s, episode steps: 500, steps per second:  76, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.853960, mae: 20.430147, mean_q: -30.124311, mean_eps: 0.722449\n",
      "  46826/150000: episode: 100, duration: 4.097s, episode steps: 317, steps per second:  77, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.624664, mae: 20.409443, mean_q: -30.112555, mean_eps: 0.719998\n",
      "  47101/150000: episode: 101, duration: 3.533s, episode steps: 275, steps per second:  78, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.737468, mae: 20.536915, mean_q: -30.275816, mean_eps: 0.718222\n",
      "  47319/150000: episode: 102, duration: 2.786s, episode steps: 218, steps per second:  78, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.799904, mae: 20.587097, mean_q: -30.349878, mean_eps: 0.716743\n",
      "  47765/150000: episode: 103, duration: 5.796s, episode steps: 446, steps per second:  77, episode reward: -445.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.968189, mae: 20.547415, mean_q: -30.283986, mean_eps: 0.714751\n",
      "  48103/150000: episode: 104, duration: 4.376s, episode steps: 338, steps per second:  77, episode reward: -337.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.667965, mae: 20.666484, mean_q: -30.508264, mean_eps: 0.712399\n",
      "  48459/150000: episode: 105, duration: 4.616s, episode steps: 356, steps per second:  77, episode reward: -355.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.975 [0.000, 2.000],  loss: 0.715866, mae: 20.759894, mean_q: -30.615111, mean_eps: 0.710317\n",
      "  48930/150000: episode: 106, duration: 6.098s, episode steps: 471, steps per second:  77, episode reward: -470.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.858831, mae: 20.681190, mean_q: -30.478053, mean_eps: 0.707836\n",
      "  49237/150000: episode: 107, duration: 3.941s, episode steps: 307, steps per second:  78, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.743265, mae: 20.861152, mean_q: -30.776004, mean_eps: 0.705502\n",
      "  49696/150000: episode: 108, duration: 5.838s, episode steps: 459, steps per second:  79, episode reward: -458.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.815550, mae: 20.909742, mean_q: -30.839196, mean_eps: 0.703204\n",
      "  50011/150000: episode: 109, duration: 4.031s, episode steps: 315, steps per second:  78, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.582569, mae: 20.978634, mean_q: -30.987359, mean_eps: 0.700882\n",
      "  50406/150000: episode: 110, duration: 5.039s, episode steps: 395, steps per second:  78, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.888507, mae: 21.190264, mean_q: -31.226203, mean_eps: 0.698752\n",
      "  50759/150000: episode: 111, duration: 4.577s, episode steps: 353, steps per second:  77, episode reward: -352.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.842645, mae: 21.120655, mean_q: -31.126806, mean_eps: 0.696508\n",
      "  50946/150000: episode: 112, duration: 2.423s, episode steps: 187, steps per second:  77, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.661645, mae: 21.158871, mean_q: -31.228474, mean_eps: 0.694888\n",
      "  51126/150000: episode: 113, duration: 2.337s, episode steps: 180, steps per second:  77, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.730075, mae: 21.521344, mean_q: -31.746166, mean_eps: 0.693787\n",
      "  51296/150000: episode: 114, duration: 2.168s, episode steps: 170, steps per second:  78, episode reward: -169.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.118 [0.000, 2.000],  loss: 0.721461, mae: 21.564259, mean_q: -31.794405, mean_eps: 0.692737\n",
      "  51582/150000: episode: 115, duration: 3.692s, episode steps: 286, steps per second:  77, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.752937, mae: 21.593260, mean_q: -31.830634, mean_eps: 0.691369\n",
      "  51892/150000: episode: 116, duration: 3.970s, episode steps: 310, steps per second:  78, episode reward: -309.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.846444, mae: 21.615492, mean_q: -31.862436, mean_eps: 0.689581\n",
      "  52223/150000: episode: 117, duration: 4.300s, episode steps: 331, steps per second:  77, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.915872, mae: 21.737239, mean_q: -32.003378, mean_eps: 0.687658\n",
      "  52689/150000: episode: 118, duration: 5.969s, episode steps: 466, steps per second:  78, episode reward: -465.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.959 [0.000, 2.000],  loss: 0.905535, mae: 21.707234, mean_q: -31.943393, mean_eps: 0.685267\n",
      "  53003/150000: episode: 119, duration: 4.096s, episode steps: 314, steps per second:  77, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 1.127246, mae: 21.661383, mean_q: -31.885928, mean_eps: 0.682927\n",
      "  53385/150000: episode: 120, duration: 4.940s, episode steps: 382, steps per second:  77, episode reward: -381.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.775322, mae: 21.892532, mean_q: -32.238764, mean_eps: 0.680839\n",
      "  53740/150000: episode: 121, duration: 4.192s, episode steps: 355, steps per second:  85, episode reward: -354.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 1.047836, mae: 21.868469, mean_q: -32.173090, mean_eps: 0.678628\n",
      "  54007/150000: episode: 122, duration: 3.301s, episode steps: 267, steps per second:  81, episode reward: -266.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.692828, mae: 21.802224, mean_q: -32.118183, mean_eps: 0.676762\n",
      "  54360/150000: episode: 123, duration: 4.492s, episode steps: 353, steps per second:  79, episode reward: -352.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.811346, mae: 22.117003, mean_q: -32.563769, mean_eps: 0.674902\n",
      "  54801/150000: episode: 124, duration: 5.651s, episode steps: 441, steps per second:  78, episode reward: -440.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 1.067339, mae: 22.090862, mean_q: -32.502743, mean_eps: 0.672520\n",
      "  55091/150000: episode: 125, duration: 3.909s, episode steps: 290, steps per second:  74, episode reward: -289.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.059 [0.000, 2.000],  loss: 1.103973, mae: 22.135238, mean_q: -32.593805, mean_eps: 0.670327\n",
      "  55386/150000: episode: 126, duration: 3.967s, episode steps: 295, steps per second:  74, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.768950, mae: 22.301121, mean_q: -32.874555, mean_eps: 0.668572\n",
      "  55647/150000: episode: 127, duration: 3.489s, episode steps: 261, steps per second:  75, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.854965, mae: 22.203766, mean_q: -32.703484, mean_eps: 0.666904\n",
      "  55975/150000: episode: 128, duration: 4.292s, episode steps: 328, steps per second:  76, episode reward: -327.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.865138, mae: 22.265380, mean_q: -32.808414, mean_eps: 0.665137\n",
      "  56327/150000: episode: 129, duration: 4.517s, episode steps: 352, steps per second:  78, episode reward: -351.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.687131, mae: 22.358592, mean_q: -32.955134, mean_eps: 0.663097\n",
      "  56581/150000: episode: 130, duration: 3.317s, episode steps: 254, steps per second:  77, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 1.241730, mae: 22.292766, mean_q: -32.824483, mean_eps: 0.661279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  56979/150000: episode: 131, duration: 5.138s, episode steps: 398, steps per second:  77, episode reward: -397.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.020 [0.000, 2.000],  loss: 0.716201, mae: 22.245294, mean_q: -32.806619, mean_eps: 0.659323\n",
      "  57440/150000: episode: 132, duration: 5.918s, episode steps: 461, steps per second:  78, episode reward: -460.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.766471, mae: 22.398285, mean_q: -33.019986, mean_eps: 0.656746\n",
      "  57643/150000: episode: 133, duration: 2.641s, episode steps: 203, steps per second:  77, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.108 [0.000, 2.000],  loss: 0.971967, mae: 22.406157, mean_q: -33.004522, mean_eps: 0.654754\n",
      "  57950/150000: episode: 134, duration: 4.045s, episode steps: 307, steps per second:  76, episode reward: -306.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.997 [0.000, 2.000],  loss: 0.828145, mae: 22.368375, mean_q: -32.987844, mean_eps: 0.653224\n",
      "  58196/150000: episode: 135, duration: 3.228s, episode steps: 246, steps per second:  76, episode reward: -245.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.676924, mae: 22.514356, mean_q: -33.167073, mean_eps: 0.651565\n",
      "  58516/150000: episode: 136, duration: 4.212s, episode steps: 320, steps per second:  76, episode reward: -319.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.818093, mae: 22.615770, mean_q: -33.301054, mean_eps: 0.649867\n",
      "  58798/150000: episode: 137, duration: 3.687s, episode steps: 282, steps per second:  76, episode reward: -281.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.936 [0.000, 2.000],  loss: 0.601956, mae: 22.565320, mean_q: -33.251117, mean_eps: 0.648061\n",
      "  59176/150000: episode: 138, duration: 4.927s, episode steps: 378, steps per second:  77, episode reward: -377.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.957551, mae: 22.620264, mean_q: -33.291579, mean_eps: 0.646081\n",
      "  59450/150000: episode: 139, duration: 3.554s, episode steps: 274, steps per second:  77, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.054672, mae: 22.706786, mean_q: -33.382406, mean_eps: 0.644125\n",
      "  59621/150000: episode: 140, duration: 2.242s, episode steps: 171, steps per second:  76, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.836 [0.000, 2.000],  loss: 0.673793, mae: 22.614893, mean_q: -33.270340, mean_eps: 0.642790\n",
      "  59969/150000: episode: 141, duration: 4.511s, episode steps: 348, steps per second:  77, episode reward: -347.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.820235, mae: 22.623704, mean_q: -33.275558, mean_eps: 0.641233\n",
      "  60146/150000: episode: 142, duration: 2.278s, episode steps: 177, steps per second:  78, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 0.923484, mae: 22.818968, mean_q: -33.514010, mean_eps: 0.639658\n",
      "  60467/150000: episode: 143, duration: 4.171s, episode steps: 321, steps per second:  77, episode reward: -320.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.590058, mae: 22.861311, mean_q: -33.649607, mean_eps: 0.638164\n",
      "  60633/150000: episode: 144, duration: 2.111s, episode steps: 166, steps per second:  79, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.772364, mae: 22.886647, mean_q: -33.649412, mean_eps: 0.636703\n",
      "  60857/150000: episode: 145, duration: 2.858s, episode steps: 224, steps per second:  78, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.698907, mae: 22.857289, mean_q: -33.613458, mean_eps: 0.635533\n",
      "  61174/150000: episode: 146, duration: 4.077s, episode steps: 317, steps per second:  78, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.962133, mae: 22.850351, mean_q: -33.573399, mean_eps: 0.633910\n",
      "  61363/150000: episode: 147, duration: 2.438s, episode steps: 189, steps per second:  78, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.616966, mae: 22.875424, mean_q: -33.649202, mean_eps: 0.632392\n",
      "  61739/150000: episode: 148, duration: 4.979s, episode steps: 376, steps per second:  76, episode reward: -375.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.726249, mae: 22.785488, mean_q: -33.511261, mean_eps: 0.630697\n",
      "  61970/150000: episode: 149, duration: 3.235s, episode steps: 231, steps per second:  71, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.811768, mae: 22.737601, mean_q: -33.436292, mean_eps: 0.628876\n",
      "  62232/150000: episode: 150, duration: 3.677s, episode steps: 262, steps per second:  71, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.909434, mae: 23.009670, mean_q: -33.832263, mean_eps: 0.627397\n",
      "  62476/150000: episode: 151, duration: 3.251s, episode steps: 244, steps per second:  75, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.873 [0.000, 2.000],  loss: 0.877236, mae: 23.007911, mean_q: -33.836814, mean_eps: 0.625879\n",
      "  62859/150000: episode: 152, duration: 5.277s, episode steps: 383, steps per second:  73, episode reward: -382.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.891883, mae: 23.008661, mean_q: -33.861203, mean_eps: 0.623998\n",
      "  63049/150000: episode: 153, duration: 2.872s, episode steps: 190, steps per second:  66, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.595161, mae: 22.948587, mean_q: -33.794509, mean_eps: 0.622279\n",
      "  63273/150000: episode: 154, duration: 4.064s, episode steps: 224, steps per second:  55, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.897 [0.000, 2.000],  loss: 0.959349, mae: 23.159370, mean_q: -34.047385, mean_eps: 0.621037\n",
      "  63460/150000: episode: 155, duration: 3.315s, episode steps: 187, steps per second:  56, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.735484, mae: 23.156657, mean_q: -34.088641, mean_eps: 0.619804\n",
      "  63699/150000: episode: 156, duration: 4.289s, episode steps: 239, steps per second:  56, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.577498, mae: 23.200341, mean_q: -34.197240, mean_eps: 0.618526\n",
      "  63903/150000: episode: 157, duration: 3.615s, episode steps: 204, steps per second:  56, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.970956, mae: 23.199649, mean_q: -34.138131, mean_eps: 0.617197\n",
      "  64168/150000: episode: 158, duration: 4.605s, episode steps: 265, steps per second:  58, episode reward: -264.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 1.316945, mae: 23.259977, mean_q: -34.142318, mean_eps: 0.615790\n",
      "  64408/150000: episode: 159, duration: 4.202s, episode steps: 240, steps per second:  57, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.794608, mae: 23.174873, mean_q: -34.066586, mean_eps: 0.614275\n",
      "  64706/150000: episode: 160, duration: 4.099s, episode steps: 298, steps per second:  73, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.708235, mae: 23.250550, mean_q: -34.237509, mean_eps: 0.612661\n",
      "  64986/150000: episode: 161, duration: 4.792s, episode steps: 280, steps per second:  58, episode reward: -279.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.727209, mae: 23.181715, mean_q: -34.116757, mean_eps: 0.610927\n",
      "  65242/150000: episode: 162, duration: 4.146s, episode steps: 256, steps per second:  62, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.864968, mae: 23.063513, mean_q: -33.886800, mean_eps: 0.609319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  65483/150000: episode: 163, duration: 3.607s, episode steps: 241, steps per second:  67, episode reward: -240.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.913826, mae: 23.018549, mean_q: -33.807502, mean_eps: 0.607828\n",
      "  65659/150000: episode: 164, duration: 2.438s, episode steps: 176, steps per second:  72, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.669293, mae: 23.087313, mean_q: -33.921688, mean_eps: 0.606577\n",
      "  65943/150000: episode: 165, duration: 3.845s, episode steps: 284, steps per second:  74, episode reward: -283.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 0.813714, mae: 23.096375, mean_q: -33.928919, mean_eps: 0.605197\n",
      "  66116/150000: episode: 166, duration: 2.341s, episode steps: 173, steps per second:  74, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.998833, mae: 23.231192, mean_q: -34.070636, mean_eps: 0.603826\n",
      "  66565/150000: episode: 167, duration: 7.853s, episode steps: 449, steps per second:  57, episode reward: -448.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.229 [0.000, 2.000],  loss: 0.733389, mae: 23.375949, mean_q: -34.341644, mean_eps: 0.601960\n",
      "  66817/150000: episode: 168, duration: 4.557s, episode steps: 252, steps per second:  55, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.783583, mae: 23.335830, mean_q: -34.273006, mean_eps: 0.599857\n",
      "  67060/150000: episode: 169, duration: 4.384s, episode steps: 243, steps per second:  55, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.866602, mae: 23.285295, mean_q: -34.186265, mean_eps: 0.598372\n",
      "  67283/150000: episode: 170, duration: 3.906s, episode steps: 223, steps per second:  57, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.121 [0.000, 2.000],  loss: 0.586573, mae: 23.376893, mean_q: -34.332201, mean_eps: 0.596974\n",
      "  67539/150000: episode: 171, duration: 4.551s, episode steps: 256, steps per second:  56, episode reward: -255.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 1.060870, mae: 23.433658, mean_q: -34.376839, mean_eps: 0.595537\n",
      "  67938/150000: episode: 172, duration: 5.536s, episode steps: 399, steps per second:  72, episode reward: -398.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.833186, mae: 23.372218, mean_q: -34.350964, mean_eps: 0.593572\n",
      "  68114/150000: episode: 173, duration: 3.168s, episode steps: 176, steps per second:  56, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.675291, mae: 23.230363, mean_q: -34.108052, mean_eps: 0.591847\n",
      "  68433/150000: episode: 174, duration: 5.154s, episode steps: 319, steps per second:  62, episode reward: -318.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.831941, mae: 23.561127, mean_q: -34.606078, mean_eps: 0.590362\n",
      "  68639/150000: episode: 175, duration: 3.005s, episode steps: 206, steps per second:  69, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.852286, mae: 23.497502, mean_q: -34.496323, mean_eps: 0.588787\n",
      "  68936/150000: episode: 176, duration: 4.324s, episode steps: 297, steps per second:  69, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 1.048874, mae: 23.387575, mean_q: -34.302571, mean_eps: 0.587278\n",
      "  69250/150000: episode: 177, duration: 4.721s, episode steps: 314, steps per second:  67, episode reward: -313.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 1.020449, mae: 23.682059, mean_q: -34.703139, mean_eps: 0.585445\n",
      "  69485/150000: episode: 178, duration: 3.722s, episode steps: 235, steps per second:  63, episode reward: -234.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.906 [0.000, 2.000],  loss: 0.787190, mae: 23.695622, mean_q: -34.738861, mean_eps: 0.583798\n",
      "  69683/150000: episode: 179, duration: 2.928s, episode steps: 198, steps per second:  68, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.734870, mae: 23.688788, mean_q: -34.753248, mean_eps: 0.582499\n",
      "  69852/150000: episode: 180, duration: 3.035s, episode steps: 169, steps per second:  56, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.707189, mae: 23.640966, mean_q: -34.703639, mean_eps: 0.581398\n",
      "  69984/150000: episode: 181, duration: 2.356s, episode steps: 132, steps per second:  56, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 1.037604, mae: 23.598714, mean_q: -34.585865, mean_eps: 0.580495\n",
      "  70182/150000: episode: 182, duration: 2.938s, episode steps: 198, steps per second:  67, episode reward: -197.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 1.046625, mae: 23.712667, mean_q: -34.776579, mean_eps: 0.579505\n",
      "  70479/150000: episode: 183, duration: 5.160s, episode steps: 297, steps per second:  58, episode reward: -296.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.886510, mae: 23.706911, mean_q: -34.781890, mean_eps: 0.578020\n",
      "  70705/150000: episode: 184, duration: 3.999s, episode steps: 226, steps per second:  57, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.714453, mae: 23.716684, mean_q: -34.833987, mean_eps: 0.576451\n",
      "  70980/150000: episode: 185, duration: 4.868s, episode steps: 275, steps per second:  56, episode reward: -274.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.670722, mae: 23.613731, mean_q: -34.669022, mean_eps: 0.574948\n",
      "  71155/150000: episode: 186, duration: 3.064s, episode steps: 175, steps per second:  57, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.949677, mae: 23.703692, mean_q: -34.726332, mean_eps: 0.573598\n",
      "  71399/150000: episode: 187, duration: 4.340s, episode steps: 244, steps per second:  56, episode reward: -243.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.754793, mae: 23.852258, mean_q: -35.011263, mean_eps: 0.572341\n",
      "  71635/150000: episode: 188, duration: 4.237s, episode steps: 236, steps per second:  56, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.787281, mae: 23.770035, mean_q: -34.844090, mean_eps: 0.570901\n",
      "  71808/150000: episode: 189, duration: 3.190s, episode steps: 173, steps per second:  54, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.830790, mae: 23.667087, mean_q: -34.717378, mean_eps: 0.569674\n",
      "  72079/150000: episode: 190, duration: 4.759s, episode steps: 271, steps per second:  57, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.877055, mae: 23.742155, mean_q: -34.831071, mean_eps: 0.568342\n",
      "  72306/150000: episode: 191, duration: 3.160s, episode steps: 227, steps per second:  72, episode reward: -226.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 0.723413, mae: 23.801392, mean_q: -34.936550, mean_eps: 0.566848\n",
      "  72510/150000: episode: 192, duration: 3.009s, episode steps: 204, steps per second:  68, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.842640, mae: 23.808722, mean_q: -34.917046, mean_eps: 0.565555\n",
      "  72720/150000: episode: 193, duration: 2.806s, episode steps: 210, steps per second:  75, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.762015, mae: 23.706638, mean_q: -34.789707, mean_eps: 0.564313\n",
      "  72893/150000: episode: 194, duration: 2.402s, episode steps: 173, steps per second:  72, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.948361, mae: 23.864083, mean_q: -35.010584, mean_eps: 0.563164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  73140/150000: episode: 195, duration: 3.478s, episode steps: 247, steps per second:  71, episode reward: -246.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 0.690694, mae: 23.656436, mean_q: -34.658451, mean_eps: 0.561904\n",
      "  73369/150000: episode: 196, duration: 3.019s, episode steps: 229, steps per second:  76, episode reward: -228.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.786 [0.000, 2.000],  loss: 0.994547, mae: 23.702040, mean_q: -34.731466, mean_eps: 0.560476\n",
      "  73537/150000: episode: 197, duration: 2.473s, episode steps: 168, steps per second:  68, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.889603, mae: 23.630548, mean_q: -34.620173, mean_eps: 0.559285\n",
      "  73721/150000: episode: 198, duration: 2.619s, episode steps: 184, steps per second:  70, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.860892, mae: 23.677997, mean_q: -34.700624, mean_eps: 0.558229\n",
      "  73852/150000: episode: 199, duration: 1.896s, episode steps: 131, steps per second:  69, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.982490, mae: 23.626995, mean_q: -34.620300, mean_eps: 0.557284\n",
      "  74078/150000: episode: 200, duration: 2.960s, episode steps: 226, steps per second:  76, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.858914, mae: 23.832524, mean_q: -34.914833, mean_eps: 0.556213\n",
      "  74326/150000: episode: 201, duration: 3.936s, episode steps: 248, steps per second:  63, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.944 [0.000, 2.000],  loss: 0.704534, mae: 23.860491, mean_q: -35.016758, mean_eps: 0.554791\n",
      "  74502/150000: episode: 202, duration: 2.229s, episode steps: 176, steps per second:  79, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.812 [0.000, 2.000],  loss: 1.063255, mae: 23.961597, mean_q: -35.133878, mean_eps: 0.553519\n",
      "  74698/150000: episode: 203, duration: 3.033s, episode steps: 196, steps per second:  65, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.827 [0.000, 2.000],  loss: 0.908755, mae: 23.781221, mean_q: -34.873993, mean_eps: 0.552403\n",
      "  75064/150000: episode: 204, duration: 5.775s, episode steps: 366, steps per second:  63, episode reward: -365.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.763175, mae: 23.918938, mean_q: -35.102737, mean_eps: 0.550717\n",
      "  75254/150000: episode: 205, duration: 2.991s, episode steps: 190, steps per second:  64, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.734972, mae: 24.151856, mean_q: -35.460318, mean_eps: 0.549049\n",
      "  75497/150000: episode: 206, duration: 3.828s, episode steps: 243, steps per second:  63, episode reward: -242.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.758618, mae: 24.193001, mean_q: -35.520362, mean_eps: 0.547750\n",
      "  75729/150000: episode: 207, duration: 3.500s, episode steps: 232, steps per second:  66, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.941946, mae: 24.102252, mean_q: -35.368231, mean_eps: 0.546325\n",
      "  75957/150000: episode: 208, duration: 2.685s, episode steps: 228, steps per second:  85, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.720293, mae: 24.079465, mean_q: -35.367087, mean_eps: 0.544945\n",
      "  76197/150000: episode: 209, duration: 3.264s, episode steps: 240, steps per second:  74, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.856908, mae: 24.032463, mean_q: -35.189046, mean_eps: 0.543541\n",
      "  76390/150000: episode: 210, duration: 2.812s, episode steps: 193, steps per second:  69, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.792132, mae: 24.019529, mean_q: -35.196022, mean_eps: 0.542242\n",
      "  76676/150000: episode: 211, duration: 4.414s, episode steps: 286, steps per second:  65, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.917829, mae: 23.944742, mean_q: -35.100756, mean_eps: 0.540805\n",
      "  76879/150000: episode: 212, duration: 2.617s, episode steps: 203, steps per second:  78, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.978019, mae: 24.033186, mean_q: -35.224843, mean_eps: 0.539338\n",
      "  77047/150000: episode: 213, duration: 2.537s, episode steps: 168, steps per second:  66, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.842820, mae: 23.928975, mean_q: -35.045504, mean_eps: 0.538225\n",
      "  77253/150000: episode: 214, duration: 3.315s, episode steps: 206, steps per second:  62, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.102 [0.000, 2.000],  loss: 0.966049, mae: 24.223129, mean_q: -35.481016, mean_eps: 0.537103\n",
      "  77399/150000: episode: 215, duration: 2.498s, episode steps: 146, steps per second:  58, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.861052, mae: 24.275852, mean_q: -35.544501, mean_eps: 0.536047\n",
      "  77617/150000: episode: 216, duration: 3.510s, episode steps: 218, steps per second:  62, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.739704, mae: 24.272733, mean_q: -35.548322, mean_eps: 0.534955\n",
      "  77766/150000: episode: 217, duration: 2.299s, episode steps: 149, steps per second:  65, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 1.009331, mae: 24.006448, mean_q: -35.142879, mean_eps: 0.533854\n",
      "  77931/150000: episode: 218, duration: 2.720s, episode steps: 165, steps per second:  61, episode reward: -164.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.127 [0.000, 2.000],  loss: 0.713808, mae: 24.039569, mean_q: -35.273200, mean_eps: 0.532912\n",
      "  78074/150000: episode: 219, duration: 1.766s, episode steps: 143, steps per second:  81, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.802199, mae: 24.187268, mean_q: -35.449141, mean_eps: 0.531988\n",
      "  78260/150000: episode: 220, duration: 2.670s, episode steps: 186, steps per second:  70, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.720087, mae: 24.213037, mean_q: -35.481386, mean_eps: 0.531001\n",
      "  78410/150000: episode: 221, duration: 2.237s, episode steps: 150, steps per second:  67, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.750619, mae: 24.215003, mean_q: -35.464075, mean_eps: 0.529993\n",
      "  78611/150000: episode: 222, duration: 3.242s, episode steps: 201, steps per second:  62, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.828850, mae: 24.297911, mean_q: -35.579002, mean_eps: 0.528940\n",
      "  78819/150000: episode: 223, duration: 3.103s, episode steps: 208, steps per second:  67, episode reward: -207.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.807727, mae: 24.070114, mean_q: -35.253352, mean_eps: 0.527713\n",
      "  79088/150000: episode: 224, duration: 3.635s, episode steps: 269, steps per second:  74, episode reward: -268.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.063 [0.000, 2.000],  loss: 0.816769, mae: 24.100493, mean_q: -35.275691, mean_eps: 0.526282\n",
      "  79311/150000: episode: 225, duration: 3.077s, episode steps: 223, steps per second:  72, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 1.174107, mae: 24.230710, mean_q: -35.434727, mean_eps: 0.524806\n",
      "  79450/150000: episode: 226, duration: 2.108s, episode steps: 139, steps per second:  66, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.923780, mae: 24.090452, mean_q: -35.234645, mean_eps: 0.523720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79630/150000: episode: 227, duration: 2.321s, episode steps: 180, steps per second:  78, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.967 [0.000, 2.000],  loss: 0.722746, mae: 24.187197, mean_q: -35.398550, mean_eps: 0.522763\n",
      "  79786/150000: episode: 228, duration: 1.836s, episode steps: 156, steps per second:  85, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.872694, mae: 24.175469, mean_q: -35.380120, mean_eps: 0.521755\n",
      "  79894/150000: episode: 229, duration: 1.757s, episode steps: 108, steps per second:  61, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.643840, mae: 24.315198, mean_q: -35.583577, mean_eps: 0.520963\n",
      "  80074/150000: episode: 230, duration: 2.904s, episode steps: 180, steps per second:  62, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.757248, mae: 24.092609, mean_q: -35.265562, mean_eps: 0.520099\n",
      "  80276/150000: episode: 231, duration: 3.388s, episode steps: 202, steps per second:  60, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.761803, mae: 24.120817, mean_q: -35.325734, mean_eps: 0.518953\n",
      "  80409/150000: episode: 232, duration: 2.242s, episode steps: 133, steps per second:  59, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.779775, mae: 24.137751, mean_q: -35.335774, mean_eps: 0.517948\n",
      "  80637/150000: episode: 233, duration: 3.051s, episode steps: 228, steps per second:  75, episode reward: -227.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.987 [0.000, 2.000],  loss: 0.887665, mae: 24.165521, mean_q: -35.384726, mean_eps: 0.516865\n",
      "  80829/150000: episode: 234, duration: 2.500s, episode steps: 192, steps per second:  77, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.757088, mae: 24.168910, mean_q: -35.379312, mean_eps: 0.515605\n",
      "  80961/150000: episode: 235, duration: 1.868s, episode steps: 132, steps per second:  71, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.647470, mae: 24.215627, mean_q: -35.485466, mean_eps: 0.514633\n",
      "  81185/150000: episode: 236, duration: 3.091s, episode steps: 224, steps per second:  72, episode reward: -223.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 1.020679, mae: 23.925396, mean_q: -34.995273, mean_eps: 0.513565\n",
      "  81342/150000: episode: 237, duration: 2.316s, episode steps: 157, steps per second:  68, episode reward: -156.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.707112, mae: 24.061266, mean_q: -35.253288, mean_eps: 0.512422\n",
      "  81478/150000: episode: 238, duration: 1.682s, episode steps: 136, steps per second:  81, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.882 [0.000, 2.000],  loss: 0.810211, mae: 23.782381, mean_q: -34.831899, mean_eps: 0.511543\n",
      "  81628/150000: episode: 239, duration: 2.291s, episode steps: 150, steps per second:  65, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.638336, mae: 23.704802, mean_q: -34.717159, mean_eps: 0.510685\n",
      "  81832/150000: episode: 240, duration: 2.672s, episode steps: 204, steps per second:  76, episode reward: -203.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.618560, mae: 23.883079, mean_q: -35.023747, mean_eps: 0.509623\n",
      "  81999/150000: episode: 241, duration: 3.089s, episode steps: 167, steps per second:  54, episode reward: -166.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.677793, mae: 23.916816, mean_q: -35.053068, mean_eps: 0.508510\n",
      "  82139/150000: episode: 242, duration: 1.959s, episode steps: 140, steps per second:  71, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.879 [0.000, 2.000],  loss: 0.692982, mae: 23.909594, mean_q: -34.977303, mean_eps: 0.507589\n",
      "  82405/150000: episode: 243, duration: 3.842s, episode steps: 266, steps per second:  69, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.842 [0.000, 2.000],  loss: 0.814741, mae: 23.969547, mean_q: -35.065114, mean_eps: 0.506371\n",
      "  82546/150000: episode: 244, duration: 1.811s, episode steps: 141, steps per second:  78, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.965 [0.000, 2.000],  loss: 0.655672, mae: 23.837551, mean_q: -34.866437, mean_eps: 0.505150\n",
      "  82705/150000: episode: 245, duration: 2.112s, episode steps: 159, steps per second:  75, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.050 [0.000, 2.000],  loss: 0.854465, mae: 24.032512, mean_q: -35.163393, mean_eps: 0.504250\n",
      "  82889/150000: episode: 246, duration: 2.797s, episode steps: 184, steps per second:  66, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.767451, mae: 24.112869, mean_q: -35.308948, mean_eps: 0.503221\n",
      "  83114/150000: episode: 247, duration: 3.243s, episode steps: 225, steps per second:  69, episode reward: -224.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.013 [0.000, 2.000],  loss: 0.760056, mae: 23.986269, mean_q: -35.053177, mean_eps: 0.501994\n",
      "  83255/150000: episode: 248, duration: 1.845s, episode steps: 141, steps per second:  76, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 0.817836, mae: 24.072441, mean_q: -35.168610, mean_eps: 0.500896\n",
      "  83384/150000: episode: 249, duration: 1.670s, episode steps: 129, steps per second:  77, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.744414, mae: 23.935443, mean_q: -34.968865, mean_eps: 0.500086\n",
      "  83519/150000: episode: 250, duration: 1.806s, episode steps: 135, steps per second:  75, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.683730, mae: 23.999051, mean_q: -35.132885, mean_eps: 0.499294\n",
      "  83643/150000: episode: 251, duration: 1.781s, episode steps: 124, steps per second:  70, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.911 [0.000, 2.000],  loss: 0.745391, mae: 24.043908, mean_q: -35.157121, mean_eps: 0.498517\n",
      "  83833/150000: episode: 252, duration: 2.430s, episode steps: 190, steps per second:  78, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.782394, mae: 24.010615, mean_q: -35.083680, mean_eps: 0.497575\n",
      "  84013/150000: episode: 253, duration: 2.255s, episode steps: 180, steps per second:  80, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.826748, mae: 23.921161, mean_q: -34.968193, mean_eps: 0.496465\n",
      "  84161/150000: episode: 254, duration: 2.183s, episode steps: 148, steps per second:  68, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 1.132041, mae: 23.932053, mean_q: -34.931910, mean_eps: 0.495481\n",
      "  84316/150000: episode: 255, duration: 1.889s, episode steps: 155, steps per second:  82, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.787566, mae: 23.932984, mean_q: -34.953080, mean_eps: 0.494572\n",
      "  84525/150000: episode: 256, duration: 2.813s, episode steps: 209, steps per second:  74, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.871 [0.000, 2.000],  loss: 0.733933, mae: 24.051268, mean_q: -35.149540, mean_eps: 0.493480\n",
      "  84707/150000: episode: 257, duration: 2.439s, episode steps: 182, steps per second:  75, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.917938, mae: 23.849201, mean_q: -34.866434, mean_eps: 0.492307\n",
      "  84906/150000: episode: 258, duration: 2.761s, episode steps: 199, steps per second:  72, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.623686, mae: 24.101914, mean_q: -35.253944, mean_eps: 0.491164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85096/150000: episode: 259, duration: 2.514s, episode steps: 190, steps per second:  76, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.787117, mae: 24.028595, mean_q: -35.109994, mean_eps: 0.489997\n",
      "  85307/150000: episode: 260, duration: 2.796s, episode steps: 211, steps per second:  75, episode reward: -210.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.786704, mae: 24.112254, mean_q: -35.264618, mean_eps: 0.488794\n",
      "  85488/150000: episode: 261, duration: 2.380s, episode steps: 181, steps per second:  76, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.749400, mae: 23.968091, mean_q: -35.007933, mean_eps: 0.487618\n",
      "  85666/150000: episode: 262, duration: 2.350s, episode steps: 178, steps per second:  76, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.876920, mae: 24.127028, mean_q: -35.232311, mean_eps: 0.486541\n",
      "  85815/150000: episode: 263, duration: 1.956s, episode steps: 149, steps per second:  76, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.707626, mae: 24.118659, mean_q: -35.291291, mean_eps: 0.485560\n",
      "  85969/150000: episode: 264, duration: 2.086s, episode steps: 154, steps per second:  74, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.961 [0.000, 2.000],  loss: 0.647574, mae: 23.946066, mean_q: -34.989972, mean_eps: 0.484651\n",
      "  86141/150000: episode: 265, duration: 2.327s, episode steps: 172, steps per second:  74, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.826 [0.000, 2.000],  loss: 0.928834, mae: 24.245805, mean_q: -35.371760, mean_eps: 0.483673\n",
      "  86293/150000: episode: 266, duration: 1.995s, episode steps: 152, steps per second:  76, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.724930, mae: 24.286681, mean_q: -35.482930, mean_eps: 0.482701\n",
      "  86468/150000: episode: 267, duration: 2.206s, episode steps: 175, steps per second:  79, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.783581, mae: 24.171602, mean_q: -35.315705, mean_eps: 0.481720\n",
      "  86593/150000: episode: 268, duration: 1.536s, episode steps: 125, steps per second:  81, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.872 [0.000, 2.000],  loss: 0.697882, mae: 24.151085, mean_q: -35.279501, mean_eps: 0.480820\n",
      "  86770/150000: episode: 269, duration: 2.315s, episode steps: 177, steps per second:  76, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.876 [0.000, 2.000],  loss: 0.754508, mae: 24.261568, mean_q: -35.470817, mean_eps: 0.479914\n",
      "  86973/150000: episode: 270, duration: 2.737s, episode steps: 203, steps per second:  74, episode reward: -202.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 0.853384, mae: 24.048539, mean_q: -35.101924, mean_eps: 0.478774\n",
      "  87105/150000: episode: 271, duration: 1.771s, episode steps: 132, steps per second:  75, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.732094, mae: 24.389302, mean_q: -35.659665, mean_eps: 0.477769\n",
      "  87323/150000: episode: 272, duration: 2.889s, episode steps: 218, steps per second:  75, episode reward: -217.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.954 [0.000, 2.000],  loss: 0.753810, mae: 24.508100, mean_q: -35.862522, mean_eps: 0.476719\n",
      "  87465/150000: episode: 273, duration: 1.888s, episode steps: 142, steps per second:  75, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.859 [0.000, 2.000],  loss: 0.857406, mae: 24.323109, mean_q: -35.580897, mean_eps: 0.475639\n",
      "  87644/150000: episode: 274, duration: 2.364s, episode steps: 179, steps per second:  76, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.792751, mae: 24.319709, mean_q: -35.563706, mean_eps: 0.474676\n",
      "  87788/150000: episode: 275, duration: 1.897s, episode steps: 144, steps per second:  76, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.910 [0.000, 2.000],  loss: 0.741339, mae: 24.139385, mean_q: -35.319838, mean_eps: 0.473707\n",
      "  87984/150000: episode: 276, duration: 2.588s, episode steps: 196, steps per second:  76, episode reward: -195.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.969 [0.000, 2.000],  loss: 0.780781, mae: 24.351129, mean_q: -35.609273, mean_eps: 0.472687\n",
      "  88171/150000: episode: 277, duration: 2.472s, episode steps: 187, steps per second:  76, episode reward: -186.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.942953, mae: 24.435726, mean_q: -35.707954, mean_eps: 0.471538\n",
      "  88315/150000: episode: 278, duration: 1.882s, episode steps: 144, steps per second:  77, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.804724, mae: 24.533716, mean_q: -35.848437, mean_eps: 0.470545\n",
      "  88425/150000: episode: 279, duration: 1.445s, episode steps: 110, steps per second:  76, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.741298, mae: 24.386293, mean_q: -35.659266, mean_eps: 0.469783\n",
      "  88575/150000: episode: 280, duration: 2.005s, episode steps: 150, steps per second:  75, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 1.014546, mae: 24.317964, mean_q: -35.527385, mean_eps: 0.469003\n",
      "  88743/150000: episode: 281, duration: 2.253s, episode steps: 168, steps per second:  75, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.811489, mae: 24.324366, mean_q: -35.544368, mean_eps: 0.468049\n",
      "  88907/150000: episode: 282, duration: 2.140s, episode steps: 164, steps per second:  77, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.994 [0.000, 2.000],  loss: 0.768710, mae: 24.409353, mean_q: -35.694532, mean_eps: 0.467053\n",
      "  89107/150000: episode: 283, duration: 2.651s, episode steps: 200, steps per second:  75, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.896718, mae: 24.463673, mean_q: -35.772200, mean_eps: 0.465961\n",
      "  89240/150000: episode: 284, duration: 1.751s, episode steps: 133, steps per second:  76, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.761143, mae: 24.638904, mean_q: -36.028410, mean_eps: 0.464962\n",
      "  89353/150000: episode: 285, duration: 1.521s, episode steps: 113, steps per second:  74, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.881309, mae: 24.630119, mean_q: -35.993646, mean_eps: 0.464224\n",
      "  89471/150000: episode: 286, duration: 1.558s, episode steps: 118, steps per second:  76, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.731947, mae: 24.483331, mean_q: -35.758086, mean_eps: 0.463531\n",
      "  89620/150000: episode: 287, duration: 1.929s, episode steps: 149, steps per second:  77, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.757266, mae: 24.450328, mean_q: -35.730569, mean_eps: 0.462730\n",
      "  89809/150000: episode: 288, duration: 2.469s, episode steps: 189, steps per second:  77, episode reward: -188.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.721058, mae: 24.521310, mean_q: -35.870612, mean_eps: 0.461716\n",
      "  89993/150000: episode: 289, duration: 2.426s, episode steps: 184, steps per second:  76, episode reward: -183.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.962 [0.000, 2.000],  loss: 0.706573, mae: 24.334656, mean_q: -35.615127, mean_eps: 0.460597\n",
      "  90166/150000: episode: 290, duration: 2.272s, episode steps: 173, steps per second:  76, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.871020, mae: 24.737077, mean_q: -36.144559, mean_eps: 0.459526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  90328/150000: episode: 291, duration: 2.112s, episode steps: 162, steps per second:  77, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.712209, mae: 24.716010, mean_q: -36.177394, mean_eps: 0.458521\n",
      "  90462/150000: episode: 292, duration: 1.759s, episode steps: 134, steps per second:  76, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.231 [0.000, 2.000],  loss: 0.765644, mae: 24.773715, mean_q: -36.208499, mean_eps: 0.457633\n",
      "  90637/150000: episode: 293, duration: 2.313s, episode steps: 175, steps per second:  76, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.769108, mae: 24.517767, mean_q: -35.791574, mean_eps: 0.456706\n",
      "  90793/150000: episode: 294, duration: 2.043s, episode steps: 156, steps per second:  76, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.658119, mae: 24.600055, mean_q: -35.966807, mean_eps: 0.455713\n",
      "  90927/150000: episode: 295, duration: 1.740s, episode steps: 134, steps per second:  77, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.708145, mae: 24.810986, mean_q: -36.306731, mean_eps: 0.454843\n",
      "  91042/150000: episode: 296, duration: 1.527s, episode steps: 115, steps per second:  75, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.779022, mae: 24.531877, mean_q: -35.806285, mean_eps: 0.454096\n",
      "  91178/150000: episode: 297, duration: 1.751s, episode steps: 136, steps per second:  78, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.081 [0.000, 2.000],  loss: 0.716903, mae: 24.499938, mean_q: -35.778841, mean_eps: 0.453343\n",
      "  91333/150000: episode: 298, duration: 2.057s, episode steps: 155, steps per second:  75, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.077 [0.000, 2.000],  loss: 0.689414, mae: 24.586482, mean_q: -35.948885, mean_eps: 0.452470\n",
      "  91444/150000: episode: 299, duration: 1.480s, episode steps: 111, steps per second:  75, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.665794, mae: 24.476544, mean_q: -35.781942, mean_eps: 0.451672\n",
      "  91594/150000: episode: 300, duration: 1.940s, episode steps: 150, steps per second:  77, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.701311, mae: 24.553846, mean_q: -35.900940, mean_eps: 0.450889\n",
      "  91720/150000: episode: 301, duration: 1.629s, episode steps: 126, steps per second:  77, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.897517, mae: 24.415318, mean_q: -35.660118, mean_eps: 0.450061\n",
      "  91857/150000: episode: 302, duration: 1.827s, episode steps: 137, steps per second:  75, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.716326, mae: 24.713199, mean_q: -36.125367, mean_eps: 0.449272\n",
      "  91993/150000: episode: 303, duration: 1.769s, episode steps: 136, steps per second:  77, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.589954, mae: 24.571356, mean_q: -35.928482, mean_eps: 0.448453\n",
      "  92106/150000: episode: 304, duration: 1.470s, episode steps: 113, steps per second:  77, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.651100, mae: 24.582103, mean_q: -35.905728, mean_eps: 0.447706\n",
      "  92224/150000: episode: 305, duration: 1.581s, episode steps: 118, steps per second:  75, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.831 [0.000, 2.000],  loss: 0.831228, mae: 24.679016, mean_q: -36.047679, mean_eps: 0.447013\n",
      "  92376/150000: episode: 306, duration: 2.017s, episode steps: 152, steps per second:  75, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.768382, mae: 24.577943, mean_q: -35.894133, mean_eps: 0.446203\n",
      "  92492/150000: episode: 307, duration: 1.556s, episode steps: 116, steps per second:  75, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.734067, mae: 24.502113, mean_q: -35.756698, mean_eps: 0.445399\n",
      "  92685/150000: episode: 308, duration: 2.565s, episode steps: 193, steps per second:  75, episode reward: -192.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.710052, mae: 24.357774, mean_q: -35.556374, mean_eps: 0.444472\n",
      "  92848/150000: episode: 309, duration: 2.187s, episode steps: 163, steps per second:  75, episode reward: -162.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.626931, mae: 24.526060, mean_q: -35.840003, mean_eps: 0.443404\n",
      "  92964/150000: episode: 310, duration: 1.598s, episode steps: 116, steps per second:  73, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.876038, mae: 24.397296, mean_q: -35.639649, mean_eps: 0.442567\n",
      "  93126/150000: episode: 311, duration: 2.210s, episode steps: 162, steps per second:  73, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.130 [0.000, 2.000],  loss: 0.788635, mae: 24.629267, mean_q: -35.953622, mean_eps: 0.441733\n",
      "  93269/150000: episode: 312, duration: 1.874s, episode steps: 143, steps per second:  76, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.916 [0.000, 2.000],  loss: 0.705676, mae: 24.652585, mean_q: -36.040978, mean_eps: 0.440818\n",
      "  93425/150000: episode: 313, duration: 2.067s, episode steps: 156, steps per second:  75, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.006 [0.000, 2.000],  loss: 0.692997, mae: 24.261303, mean_q: -35.474203, mean_eps: 0.439921\n",
      "  93630/150000: episode: 314, duration: 2.755s, episode steps: 205, steps per second:  74, episode reward: -204.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.976 [0.000, 2.000],  loss: 0.669011, mae: 24.573545, mean_q: -35.954022, mean_eps: 0.438838\n",
      "  93745/150000: episode: 315, duration: 1.541s, episode steps: 115, steps per second:  75, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.657012, mae: 24.462254, mean_q: -35.772598, mean_eps: 0.437878\n",
      "  93858/150000: episode: 316, duration: 1.506s, episode steps: 113, steps per second:  75, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.692070, mae: 24.337967, mean_q: -35.550741, mean_eps: 0.437194\n",
      "  93962/150000: episode: 317, duration: 1.393s, episode steps: 104, steps per second:  75, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.793705, mae: 24.441356, mean_q: -35.689863, mean_eps: 0.436543\n",
      "  94100/150000: episode: 318, duration: 1.808s, episode steps: 138, steps per second:  76, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.942 [0.000, 2.000],  loss: 0.845622, mae: 24.591421, mean_q: -35.897846, mean_eps: 0.435817\n",
      "  94246/150000: episode: 319, duration: 1.985s, episode steps: 146, steps per second:  74, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.789823, mae: 24.432889, mean_q: -35.662924, mean_eps: 0.434965\n",
      "  94373/150000: episode: 320, duration: 1.686s, episode steps: 127, steps per second:  75, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.634005, mae: 24.654510, mean_q: -36.052365, mean_eps: 0.434146\n",
      "  94511/150000: episode: 321, duration: 1.848s, episode steps: 138, steps per second:  75, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.920 [0.000, 2.000],  loss: 0.652152, mae: 24.715089, mean_q: -36.131129, mean_eps: 0.433351\n",
      "  94680/150000: episode: 322, duration: 2.201s, episode steps: 169, steps per second:  77, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.870 [0.000, 2.000],  loss: 0.704136, mae: 24.491968, mean_q: -35.749750, mean_eps: 0.432430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94859/150000: episode: 323, duration: 2.343s, episode steps: 179, steps per second:  76, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.966 [0.000, 2.000],  loss: 0.736774, mae: 24.633215, mean_q: -35.964085, mean_eps: 0.431386\n",
      "  95010/150000: episode: 324, duration: 1.991s, episode steps: 151, steps per second:  76, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.907 [0.000, 2.000],  loss: 0.745039, mae: 24.216858, mean_q: -35.309681, mean_eps: 0.430396\n",
      "  95181/150000: episode: 325, duration: 2.268s, episode steps: 171, steps per second:  75, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.825 [0.000, 2.000],  loss: 0.744557, mae: 24.329805, mean_q: -35.500832, mean_eps: 0.429430\n",
      "  95295/150000: episode: 326, duration: 1.500s, episode steps: 114, steps per second:  76, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.814439, mae: 24.661751, mean_q: -35.957661, mean_eps: 0.428575\n",
      "  95433/150000: episode: 327, duration: 1.833s, episode steps: 138, steps per second:  75, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.744421, mae: 24.288099, mean_q: -35.411302, mean_eps: 0.427819\n",
      "  95541/150000: episode: 328, duration: 1.492s, episode steps: 108, steps per second:  72, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.701523, mae: 24.389922, mean_q: -35.567402, mean_eps: 0.427081\n",
      "  95675/150000: episode: 329, duration: 1.843s, episode steps: 134, steps per second:  73, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.662689, mae: 24.320706, mean_q: -35.486757, mean_eps: 0.426355\n",
      "  95877/150000: episode: 330, duration: 2.689s, episode steps: 202, steps per second:  75, episode reward: -201.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.955 [0.000, 2.000],  loss: 0.798199, mae: 24.620261, mean_q: -35.909125, mean_eps: 0.425347\n",
      "  96004/150000: episode: 331, duration: 1.680s, episode steps: 127, steps per second:  76, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.749500, mae: 24.545437, mean_q: -35.848115, mean_eps: 0.424360\n",
      "  96155/150000: episode: 332, duration: 2.040s, episode steps: 151, steps per second:  74, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.834 [0.000, 2.000],  loss: 0.756612, mae: 24.414762, mean_q: -35.597427, mean_eps: 0.423526\n",
      "  96304/150000: episode: 333, duration: 2.095s, episode steps: 149, steps per second:  71, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.960 [0.000, 2.000],  loss: 0.783364, mae: 24.348597, mean_q: -35.509690, mean_eps: 0.422626\n",
      "  96421/150000: episode: 334, duration: 1.877s, episode steps: 117, steps per second:  62, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.736664, mae: 23.992773, mean_q: -34.926992, mean_eps: 0.421828\n",
      "  96552/150000: episode: 335, duration: 1.528s, episode steps: 131, steps per second:  86, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.732506, mae: 24.234255, mean_q: -35.289019, mean_eps: 0.421084\n",
      "  96679/150000: episode: 336, duration: 1.582s, episode steps: 127, steps per second:  80, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.706927, mae: 24.343232, mean_q: -35.460277, mean_eps: 0.420310\n",
      "  96781/150000: episode: 337, duration: 1.268s, episode steps: 102, steps per second:  80, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.643081, mae: 24.187872, mean_q: -35.242702, mean_eps: 0.419623\n",
      "  96928/150000: episode: 338, duration: 1.764s, episode steps: 147, steps per second:  83, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.837 [0.000, 2.000],  loss: 0.664696, mae: 24.433902, mean_q: -35.650023, mean_eps: 0.418876\n",
      "  97088/150000: episode: 339, duration: 1.964s, episode steps: 160, steps per second:  81, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.087 [0.000, 2.000],  loss: 0.795213, mae: 24.481404, mean_q: -35.668506, mean_eps: 0.417955\n",
      "  97179/150000: episode: 340, duration: 1.164s, episode steps:  91, steps per second:  78, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.801255, mae: 24.197259, mean_q: -35.246044, mean_eps: 0.417202\n",
      "  97310/150000: episode: 341, duration: 1.683s, episode steps: 131, steps per second:  78, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.674154, mae: 24.352611, mean_q: -35.468334, mean_eps: 0.416536\n",
      "  97452/150000: episode: 342, duration: 1.825s, episode steps: 142, steps per second:  78, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.692081, mae: 24.202463, mean_q: -35.235633, mean_eps: 0.415717\n",
      "  97555/150000: episode: 343, duration: 1.313s, episode steps: 103, steps per second:  78, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.737314, mae: 24.219267, mean_q: -35.256825, mean_eps: 0.414982\n",
      "  97705/150000: episode: 344, duration: 1.952s, episode steps: 150, steps per second:  77, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.724814, mae: 24.281018, mean_q: -35.355395, mean_eps: 0.414223\n",
      "  97881/150000: episode: 345, duration: 2.226s, episode steps: 176, steps per second:  79, episode reward: -175.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.074 [0.000, 2.000],  loss: 0.712901, mae: 24.154956, mean_q: -35.177523, mean_eps: 0.413245\n",
      "  98029/150000: episode: 346, duration: 1.904s, episode steps: 148, steps per second:  78, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.680758, mae: 24.159712, mean_q: -35.192665, mean_eps: 0.412273\n",
      "  98152/150000: episode: 347, duration: 1.561s, episode steps: 123, steps per second:  79, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.927 [0.000, 2.000],  loss: 0.732102, mae: 24.342580, mean_q: -35.484558, mean_eps: 0.411460\n",
      "  98255/150000: episode: 348, duration: 1.320s, episode steps: 103, steps per second:  78, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.873283, mae: 23.978130, mean_q: -34.837800, mean_eps: 0.410782\n",
      "  98393/150000: episode: 349, duration: 1.706s, episode steps: 138, steps per second:  81, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.094 [0.000, 2.000],  loss: 0.756991, mae: 24.191169, mean_q: -35.243541, mean_eps: 0.410059\n",
      "  98513/150000: episode: 350, duration: 1.514s, episode steps: 120, steps per second:  79, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.692178, mae: 24.151294, mean_q: -35.173798, mean_eps: 0.409285\n",
      "  98632/150000: episode: 351, duration: 1.504s, episode steps: 119, steps per second:  79, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.774812, mae: 24.022266, mean_q: -34.951766, mean_eps: 0.408568\n",
      "  98761/150000: episode: 352, duration: 1.655s, episode steps: 129, steps per second:  78, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.673957, mae: 24.199580, mean_q: -35.240654, mean_eps: 0.407824\n",
      "  98911/150000: episode: 353, duration: 1.887s, episode steps: 150, steps per second:  79, episode reward: -149.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.659634, mae: 24.365932, mean_q: -35.538491, mean_eps: 0.406987\n",
      "  99020/150000: episode: 354, duration: 1.397s, episode steps: 109, steps per second:  78, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.046 [0.000, 2.000],  loss: 0.650205, mae: 24.320030, mean_q: -35.368998, mean_eps: 0.406210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  99147/150000: episode: 355, duration: 1.627s, episode steps: 127, steps per second:  78, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.784625, mae: 24.221216, mean_q: -35.230414, mean_eps: 0.405502\n",
      "  99265/150000: episode: 356, duration: 1.503s, episode steps: 118, steps per second:  78, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.751325, mae: 24.478100, mean_q: -35.632714, mean_eps: 0.404767\n",
      "  99387/150000: episode: 357, duration: 1.560s, episode steps: 122, steps per second:  78, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.729452, mae: 24.258942, mean_q: -35.340284, mean_eps: 0.404047\n",
      "  99515/150000: episode: 358, duration: 1.619s, episode steps: 128, steps per second:  79, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.729132, mae: 24.270661, mean_q: -35.329017, mean_eps: 0.403297\n",
      "  99681/150000: episode: 359, duration: 2.144s, episode steps: 166, steps per second:  77, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.742544, mae: 24.161226, mean_q: -35.143646, mean_eps: 0.402415\n",
      "  99815/150000: episode: 360, duration: 1.743s, episode steps: 134, steps per second:  77, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.670067, mae: 24.374035, mean_q: -35.503561, mean_eps: 0.401515\n",
      "  99986/150000: episode: 361, duration: 2.220s, episode steps: 171, steps per second:  77, episode reward: -170.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.725089, mae: 24.100850, mean_q: -35.101372, mean_eps: 0.400600\n",
      " 100115/150000: episode: 362, duration: 1.639s, episode steps: 129, steps per second:  79, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.825133, mae: 24.616419, mean_q: -35.819808, mean_eps: 0.399700\n",
      " 100236/150000: episode: 363, duration: 1.515s, episode steps: 121, steps per second:  80, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.083 [0.000, 2.000],  loss: 0.885305, mae: 24.324699, mean_q: -35.366657, mean_eps: 0.398950\n",
      " 100377/150000: episode: 364, duration: 1.812s, episode steps: 141, steps per second:  78, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.717388, mae: 24.381316, mean_q: -35.500013, mean_eps: 0.398164\n",
      " 100538/150000: episode: 365, duration: 2.079s, episode steps: 161, steps per second:  77, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.789834, mae: 24.454675, mean_q: -35.600161, mean_eps: 0.397258\n",
      " 100627/150000: episode: 366, duration: 1.147s, episode steps:  89, steps per second:  78, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.045 [0.000, 2.000],  loss: 0.805051, mae: 24.448783, mean_q: -35.546169, mean_eps: 0.396508\n",
      " 100781/150000: episode: 367, duration: 1.958s, episode steps: 154, steps per second:  79, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.266 [0.000, 2.000],  loss: 0.726517, mae: 24.295082, mean_q: -35.370487, mean_eps: 0.395779\n",
      " 100934/150000: episode: 368, duration: 1.934s, episode steps: 153, steps per second:  79, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.131 [0.000, 2.000],  loss: 0.744357, mae: 24.032932, mean_q: -34.994701, mean_eps: 0.394858\n",
      " 101066/150000: episode: 369, duration: 1.700s, episode steps: 132, steps per second:  78, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.694679, mae: 24.510277, mean_q: -35.712700, mean_eps: 0.394003\n",
      " 101177/150000: episode: 370, duration: 1.388s, episode steps: 111, steps per second:  80, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.771923, mae: 24.304259, mean_q: -35.384614, mean_eps: 0.393274\n",
      " 101309/150000: episode: 371, duration: 1.650s, episode steps: 132, steps per second:  80, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.144 [0.000, 2.000],  loss: 0.681713, mae: 24.593368, mean_q: -35.829792, mean_eps: 0.392545\n",
      " 101426/150000: episode: 372, duration: 1.502s, episode steps: 117, steps per second:  78, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.737625, mae: 24.346440, mean_q: -35.405943, mean_eps: 0.391798\n",
      " 101561/150000: episode: 373, duration: 1.695s, episode steps: 135, steps per second:  80, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.030 [0.000, 2.000],  loss: 0.639387, mae: 24.539009, mean_q: -35.746032, mean_eps: 0.391042\n",
      " 101674/150000: episode: 374, duration: 1.427s, episode steps: 113, steps per second:  79, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.638047, mae: 24.483804, mean_q: -35.633271, mean_eps: 0.390298\n",
      " 101820/150000: episode: 375, duration: 1.800s, episode steps: 146, steps per second:  81, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.640072, mae: 24.197703, mean_q: -35.225038, mean_eps: 0.389521\n",
      " 102001/150000: episode: 376, duration: 2.233s, episode steps: 181, steps per second:  81, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.591726, mae: 24.246701, mean_q: -35.328361, mean_eps: 0.388540\n",
      " 102129/150000: episode: 377, duration: 1.612s, episode steps: 128, steps per second:  79, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.752536, mae: 24.399729, mean_q: -35.497355, mean_eps: 0.387613\n",
      " 102288/150000: episode: 378, duration: 2.022s, episode steps: 159, steps per second:  79, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.706810, mae: 24.346789, mean_q: -35.474900, mean_eps: 0.386752\n",
      " 102406/150000: episode: 379, duration: 1.474s, episode steps: 118, steps per second:  80, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.667186, mae: 24.300565, mean_q: -35.403756, mean_eps: 0.385921\n",
      " 102507/150000: episode: 380, duration: 1.252s, episode steps: 101, steps per second:  81, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.941 [0.000, 2.000],  loss: 0.665747, mae: 24.030346, mean_q: -34.992169, mean_eps: 0.385264\n",
      " 102605/150000: episode: 381, duration: 1.237s, episode steps:  98, steps per second:  79, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.649167, mae: 23.953098, mean_q: -34.929037, mean_eps: 0.384667\n",
      " 102759/150000: episode: 382, duration: 1.923s, episode steps: 154, steps per second:  80, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.195 [0.000, 2.000],  loss: 0.599165, mae: 24.022041, mean_q: -34.969486, mean_eps: 0.383911\n",
      " 102899/150000: episode: 383, duration: 1.726s, episode steps: 140, steps per second:  81, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.576012, mae: 24.115896, mean_q: -35.142591, mean_eps: 0.383029\n",
      " 102989/150000: episode: 384, duration: 1.162s, episode steps:  90, steps per second:  77, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.989 [0.000, 2.000],  loss: 0.725281, mae: 24.280317, mean_q: -35.364450, mean_eps: 0.382339\n",
      " 103225/150000: episode: 385, duration: 2.921s, episode steps: 236, steps per second:  81, episode reward: -235.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.042 [0.000, 2.000],  loss: 0.819056, mae: 24.331383, mean_q: -35.352919, mean_eps: 0.381361\n",
      " 103335/150000: episode: 386, duration: 1.377s, episode steps: 110, steps per second:  80, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.964 [0.000, 2.000],  loss: 0.879232, mae: 23.949095, mean_q: -34.769397, mean_eps: 0.380323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 103444/150000: episode: 387, duration: 1.388s, episode steps: 109, steps per second:  79, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.817875, mae: 24.104471, mean_q: -35.029134, mean_eps: 0.379666\n",
      " 103547/150000: episode: 388, duration: 1.278s, episode steps: 103, steps per second:  81, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.610881, mae: 24.604855, mean_q: -35.880373, mean_eps: 0.379030\n",
      " 103661/150000: episode: 389, duration: 1.450s, episode steps: 114, steps per second:  79, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.754728, mae: 24.346901, mean_q: -35.429966, mean_eps: 0.378379\n",
      " 103783/150000: episode: 390, duration: 1.515s, episode steps: 122, steps per second:  81, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.820 [0.000, 2.000],  loss: 0.654116, mae: 24.405662, mean_q: -35.532761, mean_eps: 0.377671\n",
      " 103874/150000: episode: 391, duration: 1.166s, episode steps:  91, steps per second:  78, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.781911, mae: 24.040409, mean_q: -34.884886, mean_eps: 0.377032\n",
      " 103980/150000: episode: 392, duration: 1.353s, episode steps: 106, steps per second:  78, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.821 [0.000, 2.000],  loss: 0.600524, mae: 24.640134, mean_q: -35.899603, mean_eps: 0.376441\n",
      " 104091/150000: episode: 393, duration: 1.405s, episode steps: 111, steps per second:  79, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.892 [0.000, 2.000],  loss: 0.803539, mae: 24.413083, mean_q: -35.494173, mean_eps: 0.375790\n",
      " 104247/150000: episode: 394, duration: 1.975s, episode steps: 156, steps per second:  79, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 0.725527, mae: 24.464637, mean_q: -35.677236, mean_eps: 0.374989\n",
      " 104385/150000: episode: 395, duration: 1.720s, episode steps: 138, steps per second:  80, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.913 [0.000, 2.000],  loss: 0.659039, mae: 24.298463, mean_q: -35.401489, mean_eps: 0.374107\n",
      " 104491/150000: episode: 396, duration: 1.368s, episode steps: 106, steps per second:  77, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.755 [0.000, 2.000],  loss: 0.674264, mae: 24.095103, mean_q: -35.111993, mean_eps: 0.373375\n",
      " 104601/150000: episode: 397, duration: 1.373s, episode steps: 110, steps per second:  80, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.855 [0.000, 2.000],  loss: 0.709988, mae: 24.501180, mean_q: -35.686479, mean_eps: 0.372727\n",
      " 104731/150000: episode: 398, duration: 1.644s, episode steps: 130, steps per second:  79, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.938 [0.000, 2.000],  loss: 0.636822, mae: 24.168291, mean_q: -35.155627, mean_eps: 0.372007\n",
      " 105192/150000: episode: 399, duration: 5.468s, episode steps: 461, steps per second:  84, episode reward: -460.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.085 [0.000, 2.000],  loss: 0.719971, mae: 24.117262, mean_q: -35.087352, mean_eps: 0.370234\n",
      " 105344/150000: episode: 400, duration: 1.834s, episode steps: 152, steps per second:  83, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.774133, mae: 24.213479, mean_q: -35.202303, mean_eps: 0.368395\n",
      " 105559/150000: episode: 401, duration: 2.569s, episode steps: 215, steps per second:  84, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.725419, mae: 23.968215, mean_q: -34.808243, mean_eps: 0.367294\n",
      " 105705/150000: episode: 402, duration: 1.792s, episode steps: 146, steps per second:  81, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.676965, mae: 23.987490, mean_q: -34.850276, mean_eps: 0.366211\n",
      " 105839/150000: episode: 403, duration: 1.674s, episode steps: 134, steps per second:  80, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.771572, mae: 23.797299, mean_q: -34.547159, mean_eps: 0.365371\n",
      " 105964/150000: episode: 404, duration: 1.561s, episode steps: 125, steps per second:  80, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.824 [0.000, 2.000],  loss: 0.695710, mae: 23.811417, mean_q: -34.602639, mean_eps: 0.364594\n",
      " 106113/150000: episode: 405, duration: 1.880s, episode steps: 149, steps per second:  79, episode reward: -148.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.899 [0.000, 2.000],  loss: 0.851160, mae: 23.922625, mean_q: -34.748910, mean_eps: 0.363772\n",
      " 106255/150000: episode: 406, duration: 1.778s, episode steps: 142, steps per second:  80, episode reward: -141.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.880 [0.000, 2.000],  loss: 0.697732, mae: 23.842696, mean_q: -34.659660, mean_eps: 0.362899\n",
      " 106427/150000: episode: 407, duration: 2.158s, episode steps: 172, steps per second:  80, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.977 [0.000, 2.000],  loss: 0.643907, mae: 23.889260, mean_q: -34.684191, mean_eps: 0.361957\n",
      " 106554/150000: episode: 408, duration: 1.578s, episode steps: 127, steps per second:  80, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.692232, mae: 23.519824, mean_q: -34.151779, mean_eps: 0.361060\n",
      " 106683/150000: episode: 409, duration: 1.610s, episode steps: 129, steps per second:  80, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.736 [0.000, 2.000],  loss: 0.745173, mae: 23.976464, mean_q: -34.831837, mean_eps: 0.360292\n",
      " 106793/150000: episode: 410, duration: 1.402s, episode steps: 110, steps per second:  78, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.698704, mae: 23.842436, mean_q: -34.640977, mean_eps: 0.359575\n",
      " 106892/150000: episode: 411, duration: 1.244s, episode steps:  99, steps per second:  80, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.848 [0.000, 2.000],  loss: 0.703334, mae: 24.060574, mean_q: -34.988809, mean_eps: 0.358948\n",
      " 107036/150000: episode: 412, duration: 1.833s, episode steps: 144, steps per second:  79, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.686250, mae: 23.741494, mean_q: -34.472126, mean_eps: 0.358219\n",
      " 107174/150000: episode: 413, duration: 1.866s, episode steps: 138, steps per second:  74, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.708233, mae: 23.610428, mean_q: -34.270192, mean_eps: 0.357373\n",
      " 107321/150000: episode: 414, duration: 1.892s, episode steps: 147, steps per second:  78, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.650345, mae: 23.745362, mean_q: -34.476556, mean_eps: 0.356518\n",
      " 107496/150000: episode: 415, duration: 2.210s, episode steps: 175, steps per second:  79, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.766 [0.000, 2.000],  loss: 0.677802, mae: 23.627522, mean_q: -34.312606, mean_eps: 0.355552\n",
      " 107631/150000: episode: 416, duration: 1.727s, episode steps: 135, steps per second:  78, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.656255, mae: 23.609447, mean_q: -34.280300, mean_eps: 0.354622\n",
      " 107785/150000: episode: 417, duration: 2.022s, episode steps: 154, steps per second:  76, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.773 [0.000, 2.000],  loss: 0.648151, mae: 23.503671, mean_q: -34.123907, mean_eps: 0.353755\n",
      " 107896/150000: episode: 418, duration: 1.692s, episode steps: 111, steps per second:  66, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.937 [0.000, 2.000],  loss: 0.576383, mae: 23.491354, mean_q: -34.092390, mean_eps: 0.352960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 108050/150000: episode: 419, duration: 2.123s, episode steps: 154, steps per second:  73, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.646414, mae: 23.678283, mean_q: -34.400338, mean_eps: 0.352165\n",
      " 108230/150000: episode: 420, duration: 2.679s, episode steps: 180, steps per second:  67, episode reward: -179.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.878 [0.000, 2.000],  loss: 0.739951, mae: 23.311210, mean_q: -33.857364, mean_eps: 0.351163\n",
      " 108369/150000: episode: 421, duration: 1.721s, episode steps: 139, steps per second:  81, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.626727, mae: 23.442874, mean_q: -34.031088, mean_eps: 0.350206\n",
      " 108475/150000: episode: 422, duration: 1.558s, episode steps: 106, steps per second:  68, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.620843, mae: 23.093164, mean_q: -33.524558, mean_eps: 0.349471\n",
      " 108615/150000: episode: 423, duration: 1.734s, episode steps: 140, steps per second:  81, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.943 [0.000, 2.000],  loss: 0.740315, mae: 22.870467, mean_q: -33.176418, mean_eps: 0.348733\n",
      " 108724/150000: episode: 424, duration: 1.523s, episode steps: 109, steps per second:  72, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.621185, mae: 23.504573, mean_q: -34.160126, mean_eps: 0.347986\n",
      " 108896/150000: episode: 425, duration: 2.380s, episode steps: 172, steps per second:  72, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.052 [0.000, 2.000],  loss: 0.585750, mae: 23.272378, mean_q: -33.786258, mean_eps: 0.347143\n",
      " 109007/150000: episode: 426, duration: 1.511s, episode steps: 111, steps per second:  73, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.865 [0.000, 2.000],  loss: 0.623026, mae: 23.078294, mean_q: -33.472026, mean_eps: 0.346294\n",
      " 109131/150000: episode: 427, duration: 1.723s, episode steps: 124, steps per second:  72, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.016 [0.000, 2.000],  loss: 0.846277, mae: 23.154033, mean_q: -33.578550, mean_eps: 0.345589\n",
      " 109248/150000: episode: 428, duration: 1.662s, episode steps: 117, steps per second:  70, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.846 [0.000, 2.000],  loss: 0.629456, mae: 22.970853, mean_q: -33.340862, mean_eps: 0.344866\n",
      " 109347/150000: episode: 429, duration: 1.415s, episode steps:  99, steps per second:  70, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.777091, mae: 23.302848, mean_q: -33.832164, mean_eps: 0.344218\n",
      " 109513/150000: episode: 430, duration: 2.301s, episode steps: 166, steps per second:  72, episode reward: -165.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.648149, mae: 23.123945, mean_q: -33.613127, mean_eps: 0.343423\n",
      " 109631/150000: episode: 431, duration: 1.704s, episode steps: 118, steps per second:  69, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.727534, mae: 23.018270, mean_q: -33.435784, mean_eps: 0.342571\n",
      " 109744/150000: episode: 432, duration: 1.563s, episode steps: 113, steps per second:  72, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.751927, mae: 23.232420, mean_q: -33.767853, mean_eps: 0.341878\n",
      " 109858/150000: episode: 433, duration: 1.667s, episode steps: 114, steps per second:  68, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.607459, mae: 23.392854, mean_q: -34.023519, mean_eps: 0.341197\n",
      " 109964/150000: episode: 434, duration: 1.475s, episode steps: 106, steps per second:  72, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.698111, mae: 23.221964, mean_q: -33.715001, mean_eps: 0.340537\n",
      " 110062/150000: episode: 435, duration: 1.352s, episode steps:  98, steps per second:  72, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.898 [0.000, 2.000],  loss: 0.761087, mae: 23.161915, mean_q: -33.632270, mean_eps: 0.339925\n",
      " 110187/150000: episode: 436, duration: 1.702s, episode steps: 125, steps per second:  73, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.192 [0.000, 2.000],  loss: 0.675868, mae: 23.220965, mean_q: -33.716404, mean_eps: 0.339256\n",
      " 110316/150000: episode: 437, duration: 1.742s, episode steps: 129, steps per second:  74, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.669832, mae: 23.183199, mean_q: -33.666311, mean_eps: 0.338494\n",
      " 110434/150000: episode: 438, duration: 1.661s, episode steps: 118, steps per second:  71, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.068 [0.000, 2.000],  loss: 0.587523, mae: 23.459153, mean_q: -34.127176, mean_eps: 0.337753\n",
      " 110545/150000: episode: 439, duration: 1.585s, episode steps: 111, steps per second:  70, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.171 [0.000, 2.000],  loss: 0.586543, mae: 23.154749, mean_q: -33.627229, mean_eps: 0.337066\n",
      " 110660/150000: episode: 440, duration: 1.646s, episode steps: 115, steps per second:  70, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.639098, mae: 23.263483, mean_q: -33.799945, mean_eps: 0.336388\n",
      " 110801/150000: episode: 441, duration: 1.992s, episode steps: 141, steps per second:  71, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.626176, mae: 23.381242, mean_q: -33.974188, mean_eps: 0.335620\n",
      " 110954/150000: episode: 442, duration: 2.095s, episode steps: 153, steps per second:  73, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.591902, mae: 23.485596, mean_q: -34.138197, mean_eps: 0.334738\n",
      " 111046/150000: episode: 443, duration: 1.319s, episode steps:  92, steps per second:  70, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.935 [0.000, 2.000],  loss: 0.790022, mae: 23.091006, mean_q: -33.502910, mean_eps: 0.334003\n",
      " 111169/150000: episode: 444, duration: 1.703s, episode steps: 123, steps per second:  72, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.024 [0.000, 2.000],  loss: 0.777552, mae: 23.124695, mean_q: -33.583478, mean_eps: 0.333358\n",
      " 111268/150000: episode: 445, duration: 1.410s, episode steps:  99, steps per second:  70, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.949 [0.000, 2.000],  loss: 0.709575, mae: 23.225915, mean_q: -33.694925, mean_eps: 0.332692\n",
      " 111405/150000: episode: 446, duration: 1.912s, episode steps: 137, steps per second:  72, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.622355, mae: 23.364147, mean_q: -33.942305, mean_eps: 0.331984\n",
      " 111550/150000: episode: 447, duration: 2.021s, episode steps: 145, steps per second:  72, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.646640, mae: 23.140525, mean_q: -33.614422, mean_eps: 0.331138\n",
      " 111657/150000: episode: 448, duration: 1.454s, episode steps: 107, steps per second:  74, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.065 [0.000, 2.000],  loss: 0.559207, mae: 23.411072, mean_q: -34.050424, mean_eps: 0.330382\n",
      " 111741/150000: episode: 449, duration: 1.167s, episode steps:  84, steps per second:  72, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.614986, mae: 23.301963, mean_q: -33.884367, mean_eps: 0.329809\n",
      " 111867/150000: episode: 450, duration: 1.694s, episode steps: 126, steps per second:  74, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.119 [0.000, 2.000],  loss: 0.562692, mae: 23.196866, mean_q: -33.768837, mean_eps: 0.329179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 111976/150000: episode: 451, duration: 1.437s, episode steps: 109, steps per second:  76, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.574288, mae: 23.581548, mean_q: -34.317972, mean_eps: 0.328474\n",
      " 112116/150000: episode: 452, duration: 1.971s, episode steps: 140, steps per second:  71, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.993 [0.000, 2.000],  loss: 0.674833, mae: 23.534855, mean_q: -34.183983, mean_eps: 0.327727\n",
      " 112267/150000: episode: 453, duration: 2.124s, episode steps: 151, steps per second:  71, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.604503, mae: 23.321090, mean_q: -33.887898, mean_eps: 0.326854\n",
      " 112369/150000: episode: 454, duration: 1.377s, episode steps: 102, steps per second:  74, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 0.594648, mae: 23.242316, mean_q: -33.763731, mean_eps: 0.326095\n",
      " 112541/150000: episode: 455, duration: 2.523s, episode steps: 172, steps per second:  68, episode reward: -171.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.599491, mae: 23.351027, mean_q: -33.910899, mean_eps: 0.325273\n",
      " 112658/150000: episode: 456, duration: 1.683s, episode steps: 117, steps per second:  70, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.179 [0.000, 2.000],  loss: 0.623351, mae: 23.419913, mean_q: -34.056952, mean_eps: 0.324406\n",
      " 112814/150000: episode: 457, duration: 2.184s, episode steps: 156, steps per second:  71, episode reward: -155.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.549917, mae: 23.321092, mean_q: -33.902374, mean_eps: 0.323587\n",
      " 112958/150000: episode: 458, duration: 2.024s, episode steps: 144, steps per second:  71, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.160 [0.000, 2.000],  loss: 0.637712, mae: 23.199160, mean_q: -33.674560, mean_eps: 0.322687\n",
      " 113117/150000: episode: 459, duration: 2.234s, episode steps: 159, steps per second:  71, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.170 [0.000, 2.000],  loss: 0.603831, mae: 23.086512, mean_q: -33.510037, mean_eps: 0.321778\n",
      " 113251/150000: episode: 460, duration: 1.806s, episode steps: 134, steps per second:  74, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.097 [0.000, 2.000],  loss: 0.591492, mae: 23.248219, mean_q: -33.788078, mean_eps: 0.320899\n",
      " 113352/150000: episode: 461, duration: 1.352s, episode steps: 101, steps per second:  75, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.248 [0.000, 2.000],  loss: 0.589794, mae: 22.893224, mean_q: -33.249206, mean_eps: 0.320194\n",
      " 113490/150000: episode: 462, duration: 1.876s, episode steps: 138, steps per second:  74, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.532450, mae: 23.266707, mean_q: -33.830134, mean_eps: 0.319477\n",
      " 113617/150000: episode: 463, duration: 1.735s, episode steps: 127, steps per second:  73, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.031 [0.000, 2.000],  loss: 0.548360, mae: 23.182829, mean_q: -33.685569, mean_eps: 0.318682\n",
      " 113730/150000: episode: 464, duration: 1.567s, episode steps: 113, steps per second:  72, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.177 [0.000, 2.000],  loss: 0.608197, mae: 23.057716, mean_q: -33.487410, mean_eps: 0.317962\n",
      " 113892/150000: episode: 465, duration: 2.276s, episode steps: 162, steps per second:  71, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.142 [0.000, 2.000],  loss: 0.582178, mae: 23.333954, mean_q: -33.916956, mean_eps: 0.317137\n",
      " 114001/150000: episode: 466, duration: 1.593s, episode steps: 109, steps per second:  68, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.138 [0.000, 2.000],  loss: 0.497855, mae: 23.345677, mean_q: -33.986063, mean_eps: 0.316324\n",
      " 114096/150000: episode: 467, duration: 1.280s, episode steps:  95, steps per second:  74, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.594278, mae: 23.578567, mean_q: -34.274139, mean_eps: 0.315712\n",
      " 114177/150000: episode: 468, duration: 1.188s, episode steps:  81, steps per second:  68, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.062 [0.000, 2.000],  loss: 0.562245, mae: 22.951979, mean_q: -33.367064, mean_eps: 0.315184\n",
      " 114303/150000: episode: 469, duration: 1.770s, episode steps: 126, steps per second:  71, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.588157, mae: 23.347048, mean_q: -33.959137, mean_eps: 0.314563\n",
      " 114389/150000: episode: 470, duration: 1.212s, episode steps:  86, steps per second:  71, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.655060, mae: 23.185260, mean_q: -33.692514, mean_eps: 0.313927\n",
      " 114490/150000: episode: 471, duration: 1.370s, episode steps: 101, steps per second:  74, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.553242, mae: 23.059383, mean_q: -33.504725, mean_eps: 0.313366\n",
      " 114684/150000: episode: 472, duration: 2.639s, episode steps: 194, steps per second:  74, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.514691, mae: 23.099882, mean_q: -33.599882, mean_eps: 0.312481\n",
      " 114781/150000: episode: 473, duration: 1.347s, episode steps:  97, steps per second:  72, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.517508, mae: 23.566187, mean_q: -34.317097, mean_eps: 0.311608\n",
      " 114875/150000: episode: 474, duration: 1.320s, episode steps:  94, steps per second:  71, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.440495, mae: 23.261545, mean_q: -33.863556, mean_eps: 0.311035\n",
      " 114971/150000: episode: 475, duration: 1.342s, episode steps:  96, steps per second:  72, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.073 [0.000, 2.000],  loss: 0.470356, mae: 23.063955, mean_q: -33.503943, mean_eps: 0.310465\n",
      " 115071/150000: episode: 476, duration: 1.332s, episode steps: 100, steps per second:  75, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.180 [0.000, 2.000],  loss: 0.541379, mae: 23.531026, mean_q: -34.204140, mean_eps: 0.309877\n",
      " 115179/150000: episode: 477, duration: 1.426s, episode steps: 108, steps per second:  76, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.549451, mae: 23.554715, mean_q: -34.266198, mean_eps: 0.309253\n",
      " 115325/150000: episode: 478, duration: 1.911s, episode steps: 146, steps per second:  76, episode reward: -145.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.110 [0.000, 2.000],  loss: 0.605075, mae: 23.448523, mean_q: -34.099921, mean_eps: 0.308491\n",
      " 115446/150000: episode: 479, duration: 1.622s, episode steps: 121, steps per second:  75, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.017 [0.000, 2.000],  loss: 0.576409, mae: 23.375993, mean_q: -34.000120, mean_eps: 0.307690\n",
      " 115573/150000: episode: 480, duration: 1.656s, episode steps: 127, steps per second:  77, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.244 [0.000, 2.000],  loss: 0.586362, mae: 23.176840, mean_q: -33.714296, mean_eps: 0.306946\n",
      " 115746/150000: episode: 481, duration: 2.374s, episode steps: 173, steps per second:  73, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.517519, mae: 23.424992, mean_q: -34.079207, mean_eps: 0.306046\n",
      " 115841/150000: episode: 482, duration: 1.326s, episode steps:  95, steps per second:  72, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.947 [0.000, 2.000],  loss: 0.543840, mae: 23.164088, mean_q: -33.701833, mean_eps: 0.305242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 115971/150000: episode: 483, duration: 1.699s, episode steps: 130, steps per second:  77, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.946 [0.000, 2.000],  loss: 0.571952, mae: 22.840438, mean_q: -33.206687, mean_eps: 0.304567\n",
      " 116079/150000: episode: 484, duration: 1.464s, episode steps: 108, steps per second:  74, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.019 [0.000, 2.000],  loss: 0.638826, mae: 23.283588, mean_q: -33.826215, mean_eps: 0.303853\n",
      " 116208/150000: episode: 485, duration: 1.723s, episode steps: 129, steps per second:  75, episode reward: -128.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.643450, mae: 22.785659, mean_q: -33.059192, mean_eps: 0.303142\n",
      " 116323/150000: episode: 486, duration: 1.522s, episode steps: 115, steps per second:  76, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.616342, mae: 23.328186, mean_q: -33.908778, mean_eps: 0.302410\n",
      " 116429/150000: episode: 487, duration: 1.456s, episode steps: 106, steps per second:  73, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.858 [0.000, 2.000],  loss: 0.560054, mae: 23.185922, mean_q: -33.693162, mean_eps: 0.301747\n",
      " 116533/150000: episode: 488, duration: 1.361s, episode steps: 104, steps per second:  76, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.500808, mae: 23.149156, mean_q: -33.664153, mean_eps: 0.301117\n",
      " 116644/150000: episode: 489, duration: 1.512s, episode steps: 111, steps per second:  73, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.099 [0.000, 2.000],  loss: 0.501824, mae: 23.145324, mean_q: -33.685943, mean_eps: 0.300472\n",
      " 116804/150000: episode: 490, duration: 2.124s, episode steps: 160, steps per second:  75, episode reward: -159.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.573650, mae: 23.293056, mean_q: -33.904257, mean_eps: 0.299659\n",
      " 116935/150000: episode: 491, duration: 1.690s, episode steps: 131, steps per second:  78, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.008 [0.000, 2.000],  loss: 0.521257, mae: 23.092642, mean_q: -33.591528, mean_eps: 0.298786\n",
      " 117066/150000: episode: 492, duration: 1.735s, episode steps: 131, steps per second:  75, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.992 [0.000, 2.000],  loss: 0.651696, mae: 23.277064, mean_q: -33.812584, mean_eps: 0.298000\n",
      " 117181/150000: episode: 493, duration: 1.489s, episode steps: 115, steps per second:  77, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.667630, mae: 22.832007, mean_q: -33.136585, mean_eps: 0.297262\n",
      " 117301/150000: episode: 494, duration: 1.606s, episode steps: 120, steps per second:  75, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.075 [0.000, 2.000],  loss: 0.590809, mae: 23.049873, mean_q: -33.509650, mean_eps: 0.296557\n",
      " 117421/150000: episode: 495, duration: 1.634s, episode steps: 120, steps per second:  73, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.792 [0.000, 2.000],  loss: 0.547472, mae: 22.687448, mean_q: -32.975137, mean_eps: 0.295837\n",
      " 117534/150000: episode: 496, duration: 1.494s, episode steps: 113, steps per second:  76, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.572230, mae: 23.116190, mean_q: -33.604109, mean_eps: 0.295138\n",
      " 117693/150000: episode: 497, duration: 2.153s, episode steps: 159, steps per second:  74, episode reward: -158.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.698 [0.000, 2.000],  loss: 0.554539, mae: 22.845522, mean_q: -33.177197, mean_eps: 0.294322\n",
      " 117829/150000: episode: 498, duration: 1.847s, episode steps: 136, steps per second:  74, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.007 [0.000, 2.000],  loss: 0.534900, mae: 23.030471, mean_q: -33.493067, mean_eps: 0.293437\n",
      " 117963/150000: episode: 499, duration: 1.812s, episode steps: 134, steps per second:  74, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.791 [0.000, 2.000],  loss: 0.607219, mae: 22.960750, mean_q: -33.352669, mean_eps: 0.292627\n",
      " 118116/150000: episode: 500, duration: 2.156s, episode steps: 153, steps per second:  71, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.685567, mae: 23.157378, mean_q: -33.616521, mean_eps: 0.291766\n",
      " 118247/150000: episode: 501, duration: 1.763s, episode steps: 131, steps per second:  74, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.543000, mae: 23.061911, mean_q: -33.470101, mean_eps: 0.290914\n",
      " 118422/150000: episode: 502, duration: 2.406s, episode steps: 175, steps per second:  73, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.914 [0.000, 2.000],  loss: 0.555154, mae: 23.108506, mean_q: -33.584053, mean_eps: 0.289996\n",
      " 118531/150000: episode: 503, duration: 1.480s, episode steps: 109, steps per second:  74, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.798 [0.000, 2.000],  loss: 0.574645, mae: 22.999365, mean_q: -33.423035, mean_eps: 0.289144\n",
      " 118685/150000: episode: 504, duration: 2.126s, episode steps: 154, steps per second:  72, episode reward: -153.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.539 [0.000, 2.000],  loss: 0.495684, mae: 23.353504, mean_q: -33.928582, mean_eps: 0.288355\n",
      " 118837/150000: episode: 505, duration: 2.031s, episode steps: 152, steps per second:  75, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.711 [0.000, 2.000],  loss: 0.536292, mae: 23.094830, mean_q: -33.583524, mean_eps: 0.287437\n",
      " 118968/150000: episode: 506, duration: 1.748s, episode steps: 131, steps per second:  75, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.512084, mae: 22.935655, mean_q: -33.315536, mean_eps: 0.286588\n",
      " 119083/150000: episode: 507, duration: 1.517s, episode steps: 115, steps per second:  76, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.737919, mae: 22.699157, mean_q: -32.909403, mean_eps: 0.285850\n",
      " 119234/150000: episode: 508, duration: 2.201s, episode steps: 151, steps per second:  69, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.768 [0.000, 2.000],  loss: 0.644762, mae: 23.103886, mean_q: -33.556298, mean_eps: 0.285052\n",
      " 119378/150000: episode: 509, duration: 1.744s, episode steps: 144, steps per second:  83, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.806 [0.000, 2.000],  loss: 0.683318, mae: 22.811031, mean_q: -33.100165, mean_eps: 0.284167\n",
      " 119539/150000: episode: 510, duration: 1.988s, episode steps: 161, steps per second:  81, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.652 [0.000, 2.000],  loss: 0.639350, mae: 22.931924, mean_q: -33.265291, mean_eps: 0.283252\n",
      " 119673/150000: episode: 511, duration: 1.906s, episode steps: 134, steps per second:  70, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.761 [0.000, 2.000],  loss: 0.554610, mae: 22.948575, mean_q: -33.271797, mean_eps: 0.282367\n",
      " 119841/150000: episode: 512, duration: 2.311s, episode steps: 168, steps per second:  73, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.804 [0.000, 2.000],  loss: 0.541841, mae: 23.067987, mean_q: -33.526056, mean_eps: 0.281461\n",
      " 119978/150000: episode: 513, duration: 1.905s, episode steps: 137, steps per second:  72, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.752 [0.000, 2.000],  loss: 0.588298, mae: 22.764218, mean_q: -33.056068, mean_eps: 0.280546\n",
      " 120116/150000: episode: 514, duration: 1.874s, episode steps: 138, steps per second:  74, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.684511, mae: 22.999144, mean_q: -33.373008, mean_eps: 0.279721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120211/150000: episode: 515, duration: 1.244s, episode steps:  95, steps per second:  76, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.895 [0.000, 2.000],  loss: 0.654922, mae: 22.895721, mean_q: -33.183960, mean_eps: 0.279022\n",
      " 120316/150000: episode: 516, duration: 1.444s, episode steps: 105, steps per second:  73, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.971 [0.000, 2.000],  loss: 0.720821, mae: 22.983417, mean_q: -33.328052, mean_eps: 0.278422\n",
      " 120421/150000: episode: 517, duration: 1.443s, episode steps: 105, steps per second:  73, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.701195, mae: 22.792458, mean_q: -33.083661, mean_eps: 0.277792\n",
      " 120527/150000: episode: 518, duration: 1.478s, episode steps: 106, steps per second:  72, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.774 [0.000, 2.000],  loss: 0.617150, mae: 23.026789, mean_q: -33.438758, mean_eps: 0.277159\n",
      " 120639/150000: episode: 519, duration: 1.581s, episode steps: 112, steps per second:  71, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.608975, mae: 22.956675, mean_q: -33.306933, mean_eps: 0.276505\n",
      " 120771/150000: episode: 520, duration: 1.752s, episode steps: 132, steps per second:  75, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.570523, mae: 23.013449, mean_q: -33.458172, mean_eps: 0.275773\n",
      " 120904/150000: episode: 521, duration: 1.853s, episode steps: 133, steps per second:  72, episode reward: -132.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.586581, mae: 22.948535, mean_q: -33.384540, mean_eps: 0.274978\n",
      " 121030/150000: episode: 522, duration: 1.746s, episode steps: 126, steps per second:  72, episode reward: -125.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.849 [0.000, 2.000],  loss: 0.617490, mae: 23.091535, mean_q: -33.514944, mean_eps: 0.274201\n",
      " 121139/150000: episode: 523, duration: 1.544s, episode steps: 109, steps per second:  71, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.714995, mae: 22.852222, mean_q: -33.133435, mean_eps: 0.273496\n",
      " 121294/150000: episode: 524, duration: 2.116s, episode steps: 155, steps per second:  73, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.948 [0.000, 2.000],  loss: 0.593577, mae: 23.115090, mean_q: -33.588402, mean_eps: 0.272704\n",
      " 121402/150000: episode: 525, duration: 1.814s, episode steps: 108, steps per second:  60, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.606929, mae: 22.501396, mean_q: -32.626505, mean_eps: 0.271915\n",
      " 121497/150000: episode: 526, duration: 1.135s, episode steps:  95, steps per second:  84, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.614923, mae: 22.940080, mean_q: -33.314691, mean_eps: 0.271306\n",
      " 121601/150000: episode: 527, duration: 1.296s, episode steps: 104, steps per second:  80, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.601863, mae: 22.876879, mean_q: -33.272005, mean_eps: 0.270709\n",
      " 121715/150000: episode: 528, duration: 1.579s, episode steps: 114, steps per second:  72, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.921 [0.000, 2.000],  loss: 0.593650, mae: 22.818244, mean_q: -33.143812, mean_eps: 0.270055\n",
      " 121828/150000: episode: 529, duration: 1.670s, episode steps: 113, steps per second:  68, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.973 [0.000, 2.000],  loss: 0.575111, mae: 23.086255, mean_q: -33.575264, mean_eps: 0.269374\n",
      " 121914/150000: episode: 530, duration: 1.121s, episode steps:  86, steps per second:  77, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.553620, mae: 23.144244, mean_q: -33.655106, mean_eps: 0.268777\n",
      " 122017/150000: episode: 531, duration: 1.454s, episode steps: 103, steps per second:  71, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.609319, mae: 23.002985, mean_q: -33.428412, mean_eps: 0.268210\n",
      " 122124/150000: episode: 532, duration: 1.601s, episode steps: 107, steps per second:  67, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.047 [0.000, 2.000],  loss: 0.788704, mae: 22.931849, mean_q: -33.275720, mean_eps: 0.267580\n",
      " 122264/150000: episode: 533, duration: 1.730s, episode steps: 140, steps per second:  81, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.685949, mae: 23.050215, mean_q: -33.487759, mean_eps: 0.266839\n",
      " 122464/150000: episode: 534, duration: 2.472s, episode steps: 200, steps per second:  81, episode reward: -199.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.280 [0.000, 2.000],  loss: 0.681071, mae: 22.891779, mean_q: -33.240176, mean_eps: 0.265819\n",
      " 122612/150000: episode: 535, duration: 1.953s, episode steps: 148, steps per second:  76, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.980 [0.000, 2.000],  loss: 0.685145, mae: 22.984449, mean_q: -33.343034, mean_eps: 0.264775\n",
      " 122718/150000: episode: 536, duration: 1.430s, episode steps: 106, steps per second:  74, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.634671, mae: 22.937818, mean_q: -33.280555, mean_eps: 0.264013\n",
      " 122834/150000: episode: 537, duration: 1.437s, episode steps: 116, steps per second:  81, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.656447, mae: 22.894759, mean_q: -33.219064, mean_eps: 0.263347\n",
      " 123016/150000: episode: 538, duration: 2.225s, episode steps: 182, steps per second:  82, episode reward: -181.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.945 [0.000, 2.000],  loss: 0.647537, mae: 22.874576, mean_q: -33.169636, mean_eps: 0.262453\n",
      " 123125/150000: episode: 539, duration: 1.597s, episode steps: 109, steps per second:  68, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.901083, mae: 22.879010, mean_q: -33.139884, mean_eps: 0.261580\n",
      " 123256/150000: episode: 540, duration: 1.742s, episode steps: 131, steps per second:  75, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.015 [0.000, 2.000],  loss: 0.809554, mae: 22.883306, mean_q: -33.220014, mean_eps: 0.260860\n",
      " 123387/150000: episode: 541, duration: 1.771s, episode steps: 131, steps per second:  74, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.153 [0.000, 2.000],  loss: 0.807977, mae: 23.094443, mean_q: -33.486466, mean_eps: 0.260074\n",
      " 123484/150000: episode: 542, duration: 1.136s, episode steps:  97, steps per second:  85, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.801842, mae: 22.698593, mean_q: -32.862360, mean_eps: 0.259390\n",
      " 123625/150000: episode: 543, duration: 1.760s, episode steps: 141, steps per second:  80, episode reward: -140.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.742756, mae: 23.029255, mean_q: -33.412373, mean_eps: 0.258676\n",
      " 123739/150000: episode: 544, duration: 1.374s, episode steps: 114, steps per second:  83, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.219 [0.000, 2.000],  loss: 0.686922, mae: 22.935558, mean_q: -33.321689, mean_eps: 0.257911\n",
      " 123884/150000: episode: 545, duration: 1.734s, episode steps: 145, steps per second:  84, episode reward: -144.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.145 [0.000, 2.000],  loss: 0.695224, mae: 22.843671, mean_q: -33.145009, mean_eps: 0.257134\n",
      " 123978/150000: episode: 546, duration: 1.165s, episode steps:  94, steps per second:  81, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.615330, mae: 23.050361, mean_q: -33.478404, mean_eps: 0.256417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 124068/150000: episode: 547, duration: 1.160s, episode steps:  90, steps per second:  78, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.233 [0.000, 2.000],  loss: 0.699292, mae: 22.742907, mean_q: -32.955969, mean_eps: 0.255865\n",
      " 124262/150000: episode: 548, duration: 2.427s, episode steps: 194, steps per second:  80, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.278 [0.000, 2.000],  loss: 0.721473, mae: 22.885147, mean_q: -33.200352, mean_eps: 0.255013\n",
      " 124397/150000: episode: 549, duration: 1.731s, episode steps: 135, steps per second:  78, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.259 [0.000, 2.000],  loss: 0.726418, mae: 22.654337, mean_q: -32.870471, mean_eps: 0.254026\n",
      " 124510/150000: episode: 550, duration: 1.417s, episode steps: 113, steps per second:  80, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.221 [0.000, 2.000],  loss: 0.671162, mae: 22.721072, mean_q: -32.953066, mean_eps: 0.253282\n",
      " 124731/150000: episode: 551, duration: 2.791s, episode steps: 221, steps per second:  79, episode reward: -220.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.312 [0.000, 2.000],  loss: 0.727944, mae: 22.794030, mean_q: -33.081447, mean_eps: 0.252280\n",
      " 124835/150000: episode: 552, duration: 1.324s, episode steps: 104, steps per second:  79, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 0.691651, mae: 22.679964, mean_q: -32.889446, mean_eps: 0.251305\n",
      " 124967/150000: episode: 553, duration: 1.692s, episode steps: 132, steps per second:  78, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.333 [0.000, 2.000],  loss: 0.637868, mae: 22.895067, mean_q: -33.221499, mean_eps: 0.250597\n",
      " 125087/150000: episode: 554, duration: 1.532s, episode steps: 120, steps per second:  78, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.755706, mae: 22.904170, mean_q: -33.219015, mean_eps: 0.249841\n",
      " 125249/150000: episode: 555, duration: 2.020s, episode steps: 162, steps per second:  80, episode reward: -161.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.272 [0.000, 2.000],  loss: 0.629091, mae: 22.651261, mean_q: -32.878335, mean_eps: 0.248995\n",
      " 125376/150000: episode: 556, duration: 1.604s, episode steps: 127, steps per second:  79, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.449 [0.000, 2.000],  loss: 0.617404, mae: 22.954234, mean_q: -33.320874, mean_eps: 0.248128\n",
      " 125527/150000: episode: 557, duration: 1.879s, episode steps: 151, steps per second:  80, episode reward: -150.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.397 [0.000, 2.000],  loss: 0.658343, mae: 22.590934, mean_q: -32.763671, mean_eps: 0.247294\n",
      " 125667/150000: episode: 558, duration: 1.784s, episode steps: 140, steps per second:  78, episode reward: -139.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.464 [0.000, 2.000],  loss: 0.698168, mae: 22.568168, mean_q: -32.725363, mean_eps: 0.246421\n",
      " 125815/150000: episode: 559, duration: 1.869s, episode steps: 148, steps per second:  79, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.162 [0.000, 2.000],  loss: 0.619908, mae: 22.808102, mean_q: -33.114740, mean_eps: 0.245557\n",
      " 125953/150000: episode: 560, duration: 1.752s, episode steps: 138, steps per second:  79, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.167 [0.000, 2.000],  loss: 0.616325, mae: 22.797746, mean_q: -33.084382, mean_eps: 0.244699\n",
      " 126096/150000: episode: 561, duration: 1.842s, episode steps: 143, steps per second:  78, episode reward: -142.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.329 [0.000, 2.000],  loss: 0.661790, mae: 23.127542, mean_q: -33.565218, mean_eps: 0.243856\n",
      " 126196/150000: episode: 562, duration: 1.267s, episode steps: 100, steps per second:  79, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.734018, mae: 22.833579, mean_q: -33.129912, mean_eps: 0.243127\n",
      " 126284/150000: episode: 563, duration: 1.146s, episode steps:  88, steps per second:  77, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.659101, mae: 22.885564, mean_q: -33.176739, mean_eps: 0.242563\n",
      " 126400/150000: episode: 564, duration: 1.468s, episode steps: 116, steps per second:  79, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.659991, mae: 22.841202, mean_q: -33.145608, mean_eps: 0.241951\n",
      " 126513/150000: episode: 565, duration: 1.401s, episode steps: 113, steps per second:  81, episode reward: -112.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.018 [0.000, 2.000],  loss: 0.575529, mae: 22.873304, mean_q: -33.205703, mean_eps: 0.241264\n",
      " 126610/150000: episode: 566, duration: 1.296s, episode steps:  97, steps per second:  75, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.258 [0.000, 2.000],  loss: 0.609668, mae: 22.830200, mean_q: -33.120467, mean_eps: 0.240634\n",
      " 126712/150000: episode: 567, duration: 1.390s, episode steps: 102, steps per second:  73, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.049 [0.000, 2.000],  loss: 0.643167, mae: 22.887913, mean_q: -33.272472, mean_eps: 0.240037\n",
      " 126804/150000: episode: 568, duration: 1.202s, episode steps:  92, steps per second:  77, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.120 [0.000, 2.000],  loss: 0.622845, mae: 22.880015, mean_q: -33.207186, mean_eps: 0.239455\n",
      " 126904/150000: episode: 569, duration: 1.255s, episode steps: 100, steps per second:  80, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.240 [0.000, 2.000],  loss: 0.663502, mae: 22.537543, mean_q: -32.657625, mean_eps: 0.238879\n",
      " 127013/150000: episode: 570, duration: 1.378s, episode steps: 109, steps per second:  79, episode reward: -108.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.248 [0.000, 2.000],  loss: 0.611783, mae: 22.768467, mean_q: -33.045590, mean_eps: 0.238252\n",
      " 127088/150000: episode: 571, duration: 0.935s, episode steps:  75, steps per second:  80, episode reward: -74.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.840 [0.000, 2.000],  loss: 0.749750, mae: 22.834795, mean_q: -33.136360, mean_eps: 0.237700\n",
      " 127223/150000: episode: 572, duration: 1.662s, episode steps: 135, steps per second:  81, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.104 [0.000, 2.000],  loss: 0.775890, mae: 22.667855, mean_q: -32.901343, mean_eps: 0.237070\n",
      " 127324/150000: episode: 573, duration: 1.280s, episode steps: 101, steps per second:  79, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.129 [0.000, 2.000],  loss: 0.694795, mae: 22.694319, mean_q: -32.962553, mean_eps: 0.236362\n",
      " 127455/150000: episode: 574, duration: 1.675s, episode steps: 131, steps per second:  78, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.076 [0.000, 2.000],  loss: 0.623059, mae: 22.669488, mean_q: -32.935124, mean_eps: 0.235666\n",
      " 127567/150000: episode: 575, duration: 1.498s, episode steps: 112, steps per second:  75, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.098 [0.000, 2.000],  loss: 0.631508, mae: 22.509179, mean_q: -32.669503, mean_eps: 0.234937\n",
      " 127663/150000: episode: 576, duration: 1.213s, episode steps:  96, steps per second:  79, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.135 [0.000, 2.000],  loss: 0.673945, mae: 22.424019, mean_q: -32.484050, mean_eps: 0.234313\n",
      " 127768/150000: episode: 577, duration: 1.324s, episode steps: 105, steps per second:  79, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.679596, mae: 22.633860, mean_q: -32.863757, mean_eps: 0.233710\n",
      " 127915/150000: episode: 578, duration: 2.072s, episode steps: 147, steps per second:  71, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.116 [0.000, 2.000],  loss: 0.652475, mae: 22.507536, mean_q: -32.689758, mean_eps: 0.232954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 127994/150000: episode: 579, duration: 1.105s, episode steps:  79, steps per second:  71, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.924 [0.000, 2.000],  loss: 0.659322, mae: 22.595931, mean_q: -32.874423, mean_eps: 0.232276\n",
      " 128114/150000: episode: 580, duration: 1.851s, episode steps: 120, steps per second:  65, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.917 [0.000, 2.000],  loss: 0.781931, mae: 22.726148, mean_q: -32.981393, mean_eps: 0.231679\n",
      " 128198/150000: episode: 581, duration: 1.095s, episode steps:  84, steps per second:  77, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.697991, mae: 22.493397, mean_q: -32.619364, mean_eps: 0.231067\n",
      " 128304/150000: episode: 582, duration: 1.309s, episode steps: 106, steps per second:  81, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.650960, mae: 22.656001, mean_q: -32.885591, mean_eps: 0.230497\n",
      " 128420/150000: episode: 583, duration: 1.536s, episode steps: 116, steps per second:  76, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.874030, mae: 22.383136, mean_q: -32.417432, mean_eps: 0.229831\n",
      " 128525/150000: episode: 584, duration: 1.306s, episode steps: 105, steps per second:  80, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.905 [0.000, 2.000],  loss: 0.713886, mae: 22.461242, mean_q: -32.560991, mean_eps: 0.229168\n",
      " 128660/150000: episode: 585, duration: 1.731s, episode steps: 135, steps per second:  78, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.623784, mae: 22.419473, mean_q: -32.513066, mean_eps: 0.228448\n",
      " 128747/150000: episode: 586, duration: 1.357s, episode steps:  87, steps per second:  64, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.828 [0.000, 2.000],  loss: 0.638954, mae: 22.405939, mean_q: -32.488776, mean_eps: 0.227782\n",
      " 128837/150000: episode: 587, duration: 1.218s, episode steps:  90, steps per second:  74, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.634906, mae: 22.672379, mean_q: -32.945195, mean_eps: 0.227251\n",
      " 128920/150000: episode: 588, duration: 1.119s, episode steps:  83, steps per second:  74, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.653215, mae: 22.511124, mean_q: -32.635052, mean_eps: 0.226732\n",
      " 129054/150000: episode: 589, duration: 1.653s, episode steps: 134, steps per second:  81, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.985 [0.000, 2.000],  loss: 0.712734, mae: 22.432541, mean_q: -32.495961, mean_eps: 0.226081\n",
      " 129170/150000: episode: 590, duration: 1.413s, episode steps: 116, steps per second:  82, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.776 [0.000, 2.000],  loss: 0.713486, mae: 22.579877, mean_q: -32.724368, mean_eps: 0.225331\n",
      " 129318/150000: episode: 591, duration: 1.817s, episode steps: 148, steps per second:  81, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.667910, mae: 22.540927, mean_q: -32.684980, mean_eps: 0.224539\n",
      " 129419/150000: episode: 592, duration: 1.247s, episode steps: 101, steps per second:  81, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.602370, mae: 22.856218, mean_q: -33.159319, mean_eps: 0.223792\n",
      " 129539/150000: episode: 593, duration: 1.624s, episode steps: 120, steps per second:  74, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.592907, mae: 22.511149, mean_q: -32.629109, mean_eps: 0.223129\n",
      " 129643/150000: episode: 594, duration: 1.355s, episode steps: 104, steps per second:  77, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.541259, mae: 22.624736, mean_q: -32.834935, mean_eps: 0.222457\n",
      " 129727/150000: episode: 595, duration: 1.072s, episode steps:  84, steps per second:  78, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.604671, mae: 22.658623, mean_q: -32.798363, mean_eps: 0.221893\n",
      " 129818/150000: episode: 596, duration: 1.168s, episode steps:  91, steps per second:  78, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.890 [0.000, 2.000],  loss: 0.638777, mae: 22.544706, mean_q: -32.647917, mean_eps: 0.221368\n",
      " 129928/150000: episode: 597, duration: 1.437s, episode steps: 110, steps per second:  77, episode reward: -109.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.755 [0.000, 2.000],  loss: 0.601769, mae: 22.443039, mean_q: -32.511827, mean_eps: 0.220765\n",
      " 130040/150000: episode: 598, duration: 1.432s, episode steps: 112, steps per second:  78, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.866 [0.000, 2.000],  loss: 0.749272, mae: 22.878856, mean_q: -33.113525, mean_eps: 0.220099\n",
      " 130145/150000: episode: 599, duration: 1.333s, episode steps: 105, steps per second:  79, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.781 [0.000, 2.000],  loss: 0.691744, mae: 22.678550, mean_q: -32.880801, mean_eps: 0.219448\n",
      " 130267/150000: episode: 600, duration: 1.594s, episode steps: 122, steps per second:  77, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.692558, mae: 22.884323, mean_q: -33.162410, mean_eps: 0.218767\n",
      " 130378/150000: episode: 601, duration: 1.562s, episode steps: 111, steps per second:  71, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.721 [0.000, 2.000],  loss: 0.632622, mae: 22.876708, mean_q: -33.195343, mean_eps: 0.218068\n",
      " 130503/150000: episode: 602, duration: 1.560s, episode steps: 125, steps per second:  80, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.597723, mae: 22.765334, mean_q: -33.019156, mean_eps: 0.217360\n",
      " 130634/150000: episode: 603, duration: 1.585s, episode steps: 131, steps per second:  83, episode reward: -130.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.802 [0.000, 2.000],  loss: 0.623869, mae: 22.907517, mean_q: -33.216268, mean_eps: 0.216592\n",
      " 130762/150000: episode: 604, duration: 1.548s, episode steps: 128, steps per second:  83, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.695 [0.000, 2.000],  loss: 0.637382, mae: 22.871530, mean_q: -33.185350, mean_eps: 0.215815\n",
      " 130878/150000: episode: 605, duration: 1.476s, episode steps: 116, steps per second:  79, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.767 [0.000, 2.000],  loss: 0.616842, mae: 22.489723, mean_q: -32.592001, mean_eps: 0.215083\n",
      " 131073/150000: episode: 606, duration: 2.497s, episode steps: 195, steps per second:  78, episode reward: -194.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.595 [0.000, 2.000],  loss: 0.646527, mae: 22.603590, mean_q: -32.734237, mean_eps: 0.214150\n",
      " 131234/150000: episode: 607, duration: 2.052s, episode steps: 161, steps per second:  78, episode reward: -160.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.696 [0.000, 2.000],  loss: 0.681190, mae: 22.581188, mean_q: -32.674081, mean_eps: 0.213082\n",
      " 131355/150000: episode: 608, duration: 1.669s, episode steps: 121, steps per second:  73, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.678 [0.000, 2.000],  loss: 0.662661, mae: 22.580954, mean_q: -32.685881, mean_eps: 0.212236\n",
      " 131459/150000: episode: 609, duration: 1.363s, episode steps: 104, steps per second:  76, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.817 [0.000, 2.000],  loss: 0.657232, mae: 22.553275, mean_q: -32.626065, mean_eps: 0.211561\n",
      " 131574/150000: episode: 610, duration: 1.460s, episode steps: 115, steps per second:  79, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.730 [0.000, 2.000],  loss: 0.688465, mae: 22.460810, mean_q: -32.497388, mean_eps: 0.210904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 131675/150000: episode: 611, duration: 1.296s, episode steps: 101, steps per second:  78, episode reward: -100.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.693 [0.000, 2.000],  loss: 0.650319, mae: 22.571280, mean_q: -32.685423, mean_eps: 0.210256\n",
      " 131827/150000: episode: 612, duration: 1.893s, episode steps: 152, steps per second:  80, episode reward: -151.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.783 [0.000, 2.000],  loss: 0.568133, mae: 22.542762, mean_q: -32.698610, mean_eps: 0.209497\n",
      " 132008/150000: episode: 613, duration: 2.256s, episode steps: 181, steps per second:  80, episode reward: -180.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.746 [0.000, 2.000],  loss: 0.697759, mae: 22.561601, mean_q: -32.700209, mean_eps: 0.208498\n",
      " 132152/150000: episode: 614, duration: 1.672s, episode steps: 144, steps per second:  86, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.725325, mae: 22.376476, mean_q: -32.362304, mean_eps: 0.207523\n",
      " 132284/150000: episode: 615, duration: 1.514s, episode steps: 132, steps per second:  87, episode reward: -131.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.580393, mae: 22.434697, mean_q: -32.483338, mean_eps: 0.206695\n",
      " 132386/150000: episode: 616, duration: 1.222s, episode steps: 102, steps per second:  83, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.628732, mae: 22.439077, mean_q: -32.503940, mean_eps: 0.205993\n",
      " 132491/150000: episode: 617, duration: 1.282s, episode steps: 105, steps per second:  82, episode reward: -104.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.790 [0.000, 2.000],  loss: 0.533551, mae: 22.453544, mean_q: -32.538029, mean_eps: 0.205372\n",
      " 132597/150000: episode: 618, duration: 1.270s, episode steps: 106, steps per second:  83, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.575098, mae: 22.602245, mean_q: -32.770232, mean_eps: 0.204739\n",
      " 132761/150000: episode: 619, duration: 1.978s, episode steps: 164, steps per second:  83, episode reward: -163.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.579 [0.000, 2.000],  loss: 0.673179, mae: 22.612922, mean_q: -32.777170, mean_eps: 0.203929\n",
      " 132869/150000: episode: 620, duration: 1.387s, episode steps: 108, steps per second:  78, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.815 [0.000, 2.000],  loss: 0.584167, mae: 22.787283, mean_q: -33.057716, mean_eps: 0.203113\n",
      " 132990/150000: episode: 621, duration: 1.514s, episode steps: 121, steps per second:  80, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.041 [0.000, 2.000],  loss: 0.604035, mae: 22.548921, mean_q: -32.737658, mean_eps: 0.202426\n",
      " 133086/150000: episode: 622, duration: 1.180s, episode steps:  96, steps per second:  81, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.896 [0.000, 2.000],  loss: 0.712470, mae: 22.783816, mean_q: -33.023667, mean_eps: 0.201775\n",
      " 133167/150000: episode: 623, duration: 0.986s, episode steps:  81, steps per second:  82, episode reward: -80.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.658640, mae: 22.652186, mean_q: -32.886944, mean_eps: 0.201244\n",
      " 133306/150000: episode: 624, duration: 1.746s, episode steps: 139, steps per second:  80, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.686280, mae: 22.708315, mean_q: -32.936977, mean_eps: 0.200584\n",
      " 133403/150000: episode: 625, duration: 1.241s, episode steps:  97, steps per second:  78, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.661900, mae: 22.732925, mean_q: -32.964078, mean_eps: 0.199876\n",
      " 133523/150000: episode: 626, duration: 1.527s, episode steps: 120, steps per second:  79, episode reward: -119.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.933 [0.000, 2.000],  loss: 0.659157, mae: 22.526717, mean_q: -32.664014, mean_eps: 0.199225\n",
      " 133634/150000: episode: 627, duration: 1.409s, episode steps: 111, steps per second:  79, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.838 [0.000, 2.000],  loss: 0.660022, mae: 22.460121, mean_q: -32.563694, mean_eps: 0.198532\n",
      " 133742/150000: episode: 628, duration: 1.387s, episode steps: 108, steps per second:  78, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.926 [0.000, 2.000],  loss: 0.631301, mae: 22.259671, mean_q: -32.275961, mean_eps: 0.197875\n",
      " 133920/150000: episode: 629, duration: 2.201s, episode steps: 178, steps per second:  81, episode reward: -177.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.564267, mae: 22.442305, mean_q: -32.578728, mean_eps: 0.197017\n",
      " 134027/150000: episode: 630, duration: 1.348s, episode steps: 107, steps per second:  79, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.670529, mae: 22.372011, mean_q: -32.475642, mean_eps: 0.196162\n",
      " 134134/150000: episode: 631, duration: 1.383s, episode steps: 107, steps per second:  77, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.925 [0.000, 2.000],  loss: 0.655155, mae: 22.871149, mean_q: -33.147385, mean_eps: 0.195520\n",
      " 134225/150000: episode: 632, duration: 1.161s, episode steps:  91, steps per second:  78, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.033 [0.000, 2.000],  loss: 0.582296, mae: 22.773399, mean_q: -33.104029, mean_eps: 0.194926\n",
      " 134319/150000: episode: 633, duration: 1.201s, episode steps:  94, steps per second:  78, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.117 [0.000, 2.000],  loss: 0.586551, mae: 22.541395, mean_q: -32.760349, mean_eps: 0.194371\n",
      " 134405/150000: episode: 634, duration: 1.085s, episode steps:  86, steps per second:  79, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 0.616065, mae: 22.801449, mean_q: -33.090634, mean_eps: 0.193831\n",
      " 134511/150000: episode: 635, duration: 1.378s, episode steps: 106, steps per second:  77, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.189 [0.000, 2.000],  loss: 0.611146, mae: 22.638811, mean_q: -32.825420, mean_eps: 0.193255\n",
      " 134587/150000: episode: 636, duration: 1.029s, episode steps:  76, steps per second:  74, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.026 [0.000, 2.000],  loss: 0.717474, mae: 22.704350, mean_q: -32.958807, mean_eps: 0.192709\n",
      " 134680/150000: episode: 637, duration: 1.207s, episode steps:  93, steps per second:  77, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.650162, mae: 22.446918, mean_q: -32.588991, mean_eps: 0.192202\n",
      " 134796/150000: episode: 638, duration: 1.506s, episode steps: 116, steps per second:  77, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.581858, mae: 22.692227, mean_q: -32.965249, mean_eps: 0.191575\n",
      " 134935/150000: episode: 639, duration: 1.737s, episode steps: 139, steps per second:  80, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.652843, mae: 22.843352, mean_q: -33.198605, mean_eps: 0.190810\n",
      " 135060/150000: episode: 640, duration: 1.905s, episode steps: 125, steps per second:  66, episode reward: -124.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.712310, mae: 22.392077, mean_q: -32.465390, mean_eps: 0.190018\n",
      " 135147/150000: episode: 641, duration: 1.319s, episode steps:  87, steps per second:  66, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.931 [0.000, 2.000],  loss: 0.690739, mae: 22.659833, mean_q: -32.803552, mean_eps: 0.189382\n",
      " 135261/150000: episode: 642, duration: 1.673s, episode steps: 114, steps per second:  68, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.044 [0.000, 2.000],  loss: 0.639635, mae: 22.218409, mean_q: -32.215400, mean_eps: 0.188779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 135389/150000: episode: 643, duration: 1.855s, episode steps: 128, steps per second:  69, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.624339, mae: 22.497711, mean_q: -32.645699, mean_eps: 0.188053\n",
      " 135519/150000: episode: 644, duration: 1.928s, episode steps: 130, steps per second:  67, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.069 [0.000, 2.000],  loss: 0.577864, mae: 22.559071, mean_q: -32.709139, mean_eps: 0.187279\n",
      " 135603/150000: episode: 645, duration: 1.278s, episode steps:  84, steps per second:  66, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.628098, mae: 22.434583, mean_q: -32.504304, mean_eps: 0.186637\n",
      " 135687/150000: episode: 646, duration: 1.253s, episode steps:  84, steps per second:  67, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.036 [0.000, 2.000],  loss: 0.669871, mae: 22.710018, mean_q: -32.886307, mean_eps: 0.186133\n",
      " 135814/150000: episode: 647, duration: 1.886s, episode steps: 127, steps per second:  67, episode reward: -126.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.663956, mae: 22.291553, mean_q: -32.319300, mean_eps: 0.185500\n",
      " 135906/150000: episode: 648, duration: 1.407s, episode steps:  92, steps per second:  65, episode reward: -91.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.228 [0.000, 2.000],  loss: 0.615247, mae: 22.236279, mean_q: -32.247984, mean_eps: 0.184843\n",
      " 136042/150000: episode: 649, duration: 2.158s, episode steps: 136, steps per second:  63, episode reward: -135.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.037 [0.000, 2.000],  loss: 0.695649, mae: 22.316971, mean_q: -32.286137, mean_eps: 0.184159\n",
      " 136141/150000: episode: 650, duration: 1.229s, episode steps:  99, steps per second:  81, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.781022, mae: 22.424773, mean_q: -32.408835, mean_eps: 0.183454\n",
      " 136269/150000: episode: 651, duration: 1.775s, episode steps: 128, steps per second:  72, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.203 [0.000, 2.000],  loss: 0.719199, mae: 22.463282, mean_q: -32.484635, mean_eps: 0.182773\n",
      " 136391/150000: episode: 652, duration: 1.788s, episode steps: 122, steps per second:  68, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.344 [0.000, 2.000],  loss: 0.720572, mae: 22.802930, mean_q: -33.019594, mean_eps: 0.182023\n",
      " 136487/150000: episode: 653, duration: 1.408s, episode steps:  96, steps per second:  68, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.990 [0.000, 2.000],  loss: 0.642949, mae: 22.586941, mean_q: -32.765006, mean_eps: 0.181369\n",
      " 136608/150000: episode: 654, duration: 1.801s, episode steps: 121, steps per second:  67, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.256 [0.000, 2.000],  loss: 0.627201, mae: 22.219039, mean_q: -32.177881, mean_eps: 0.180718\n",
      " 136696/150000: episode: 655, duration: 1.275s, episode steps:  88, steps per second:  69, episode reward: -87.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.057 [0.000, 2.000],  loss: 0.696875, mae: 22.072677, mean_q: -31.978055, mean_eps: 0.180091\n",
      " 136802/150000: episode: 656, duration: 1.566s, episode steps: 106, steps per second:  68, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.635929, mae: 22.231413, mean_q: -32.229343, mean_eps: 0.179509\n",
      " 136902/150000: episode: 657, duration: 1.467s, episode steps: 100, steps per second:  68, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.670185, mae: 22.330733, mean_q: -32.348949, mean_eps: 0.178891\n",
      " 137040/150000: episode: 658, duration: 2.049s, episode steps: 138, steps per second:  67, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.109 [0.000, 2.000],  loss: 0.756430, mae: 22.549086, mean_q: -32.626585, mean_eps: 0.178177\n",
      " 137146/150000: episode: 659, duration: 1.560s, episode steps: 106, steps per second:  68, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.236 [0.000, 2.000],  loss: 0.778763, mae: 22.290904, mean_q: -32.234631, mean_eps: 0.177445\n",
      " 137283/150000: episode: 660, duration: 2.018s, episode steps: 137, steps per second:  68, episode reward: -136.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.161 [0.000, 2.000],  loss: 0.710911, mae: 22.031484, mean_q: -31.867429, mean_eps: 0.176716\n",
      " 137399/150000: episode: 661, duration: 1.694s, episode steps: 116, steps per second:  68, episode reward: -115.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.233 [0.000, 2.000],  loss: 0.695176, mae: 22.351130, mean_q: -32.352913, mean_eps: 0.175957\n",
      " 137503/150000: episode: 662, duration: 1.369s, episode steps: 104, steps per second:  76, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.624701, mae: 22.173301, mean_q: -32.104043, mean_eps: 0.175297\n",
      " 137626/150000: episode: 663, duration: 1.531s, episode steps: 123, steps per second:  80, episode reward: -122.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.154 [0.000, 2.000],  loss: 0.684314, mae: 22.258503, mean_q: -32.219562, mean_eps: 0.174616\n",
      " 137781/150000: episode: 664, duration: 1.913s, episode steps: 155, steps per second:  81, episode reward: -154.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.252 [0.000, 2.000],  loss: 0.743006, mae: 22.217927, mean_q: -32.162342, mean_eps: 0.173782\n",
      " 137898/150000: episode: 665, duration: 1.435s, episode steps: 117, steps per second:  82, episode reward: -116.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.034 [0.000, 2.000],  loss: 0.659834, mae: 22.473428, mean_q: -32.538360, mean_eps: 0.172966\n",
      " 138005/150000: episode: 666, duration: 1.355s, episode steps: 107, steps per second:  79, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.084 [0.000, 2.000],  loss: 0.704928, mae: 22.212054, mean_q: -32.151484, mean_eps: 0.172294\n",
      " 138124/150000: episode: 667, duration: 1.494s, episode steps: 119, steps per second:  80, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.277 [0.000, 2.000],  loss: 0.792830, mae: 22.171583, mean_q: -32.043029, mean_eps: 0.171616\n",
      " 138323/150000: episode: 668, duration: 2.660s, episode steps: 199, steps per second:  75, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.402 [0.000, 2.000],  loss: 0.788462, mae: 22.159736, mean_q: -32.002253, mean_eps: 0.170662\n",
      " 138435/150000: episode: 669, duration: 1.381s, episode steps: 112, steps per second:  81, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.009 [0.000, 2.000],  loss: 0.662266, mae: 22.332469, mean_q: -32.362451, mean_eps: 0.169729\n",
      " 138528/150000: episode: 670, duration: 1.136s, episode steps:  93, steps per second:  82, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.151 [0.000, 2.000],  loss: 0.693883, mae: 22.126578, mean_q: -32.045861, mean_eps: 0.169114\n",
      " 138639/150000: episode: 671, duration: 1.425s, episode steps: 111, steps per second:  78, episode reward: -110.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.414 [0.000, 2.000],  loss: 0.634723, mae: 22.060776, mean_q: -31.935150, mean_eps: 0.168502\n",
      " 138763/150000: episode: 672, duration: 1.550s, episode steps: 124, steps per second:  80, episode reward: -123.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.347 [0.000, 2.000],  loss: 0.649597, mae: 22.074660, mean_q: -31.893504, mean_eps: 0.167797\n",
      " 138882/150000: episode: 673, duration: 1.551s, episode steps: 119, steps per second:  77, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.210 [0.000, 2.000],  loss: 0.608078, mae: 22.187137, mean_q: -32.109570, mean_eps: 0.167068\n",
      " 138982/150000: episode: 674, duration: 1.264s, episode steps: 100, steps per second:  79, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.290 [0.000, 2.000],  loss: 0.650326, mae: 21.870663, mean_q: -31.629191, mean_eps: 0.166411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 139112/150000: episode: 675, duration: 1.646s, episode steps: 130, steps per second:  79, episode reward: -129.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.231 [0.000, 2.000],  loss: 0.734915, mae: 22.168101, mean_q: -32.093348, mean_eps: 0.165721\n",
      " 139208/150000: episode: 676, duration: 1.192s, episode steps:  96, steps per second:  81, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.668885, mae: 22.243820, mean_q: -32.293613, mean_eps: 0.165043\n",
      " 139347/150000: episode: 677, duration: 1.773s, episode steps: 139, steps per second:  78, episode reward: -138.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.665309, mae: 22.408902, mean_q: -32.471414, mean_eps: 0.164338\n",
      " 139520/150000: episode: 678, duration: 2.274s, episode steps: 173, steps per second:  76, episode reward: -172.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.643340, mae: 22.464211, mean_q: -32.516848, mean_eps: 0.163402\n",
      " 139609/150000: episode: 679, duration: 1.079s, episode steps:  89, steps per second:  83, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.622161, mae: 22.129481, mean_q: -32.043372, mean_eps: 0.162616\n",
      " 139753/150000: episode: 680, duration: 1.744s, episode steps: 144, steps per second:  83, episode reward: -143.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.236 [0.000, 2.000],  loss: 0.606173, mae: 22.088451, mean_q: -31.955793, mean_eps: 0.161917\n",
      " 139847/150000: episode: 681, duration: 1.172s, episode steps:  94, steps per second:  80, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.579733, mae: 22.076784, mean_q: -31.977224, mean_eps: 0.161203\n",
      " 139959/150000: episode: 682, duration: 1.381s, episode steps: 112, steps per second:  81, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.125 [0.000, 2.000],  loss: 0.609379, mae: 22.345288, mean_q: -32.358315, mean_eps: 0.160585\n",
      " 140050/150000: episode: 683, duration: 1.142s, episode steps:  91, steps per second:  80, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.055 [0.000, 2.000],  loss: 0.841076, mae: 22.416222, mean_q: -32.425354, mean_eps: 0.159976\n",
      " 140145/150000: episode: 684, duration: 1.176s, episode steps:  95, steps per second:  81, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.863 [0.000, 2.000],  loss: 0.803729, mae: 22.385451, mean_q: -32.403826, mean_eps: 0.159418\n",
      " 140236/150000: episode: 685, duration: 1.158s, episode steps:  91, steps per second:  79, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.833684, mae: 22.241028, mean_q: -32.188144, mean_eps: 0.158860\n",
      " 140322/150000: episode: 686, duration: 1.083s, episode steps:  86, steps per second:  79, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.919 [0.000, 2.000],  loss: 0.684593, mae: 22.566571, mean_q: -32.667913, mean_eps: 0.158329\n",
      " 140425/150000: episode: 687, duration: 1.667s, episode steps: 103, steps per second:  62, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.136 [0.000, 2.000],  loss: 0.619670, mae: 21.977778, mean_q: -31.828402, mean_eps: 0.157762\n",
      " 140525/150000: episode: 688, duration: 1.280s, episode steps: 100, steps per second:  78, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.140 [0.000, 2.000],  loss: 0.771126, mae: 22.308668, mean_q: -32.296084, mean_eps: 0.157153\n",
      " 140599/150000: episode: 689, duration: 0.988s, episode steps:  74, steps per second:  75, episode reward: -73.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.687539, mae: 22.085149, mean_q: -31.992462, mean_eps: 0.156631\n",
      " 140707/150000: episode: 690, duration: 1.376s, episode steps: 108, steps per second:  78, episode reward: -107.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.852 [0.000, 2.000],  loss: 0.615174, mae: 22.188911, mean_q: -32.145634, mean_eps: 0.156085\n",
      " 140805/150000: episode: 691, duration: 1.181s, episode steps:  98, steps per second:  83, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.637176, mae: 22.016270, mean_q: -31.864018, mean_eps: 0.155467\n",
      " 140905/150000: episode: 692, duration: 1.192s, episode steps: 100, steps per second:  84, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.599361, mae: 22.344797, mean_q: -32.421472, mean_eps: 0.154873\n",
      " 141007/150000: episode: 693, duration: 1.238s, episode steps: 102, steps per second:  82, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.547102, mae: 22.216078, mean_q: -32.172993, mean_eps: 0.154267\n",
      " 141102/150000: episode: 694, duration: 1.276s, episode steps:  95, steps per second:  74, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.761399, mae: 22.359342, mean_q: -32.420953, mean_eps: 0.153676\n",
      " 141178/150000: episode: 695, duration: 0.938s, episode steps:  76, steps per second:  81, episode reward: -75.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.733567, mae: 22.448208, mean_q: -32.566784, mean_eps: 0.153163\n",
      " 141269/150000: episode: 696, duration: 1.195s, episode steps:  91, steps per second:  76, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.132 [0.000, 2.000],  loss: 0.658435, mae: 22.553878, mean_q: -32.639926, mean_eps: 0.152662\n",
      " 141387/150000: episode: 697, duration: 1.496s, episode steps: 118, steps per second:  79, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.958 [0.000, 2.000],  loss: 0.785023, mae: 22.304069, mean_q: -32.292505, mean_eps: 0.152035\n",
      " 141489/150000: episode: 698, duration: 1.436s, episode steps: 102, steps per second:  71, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.754618, mae: 22.923410, mean_q: -33.255302, mean_eps: 0.151375\n",
      " 141585/150000: episode: 699, duration: 1.265s, episode steps:  96, steps per second:  76, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.727770, mae: 22.275271, mean_q: -32.233469, mean_eps: 0.150781\n",
      " 141680/150000: episode: 700, duration: 1.387s, episode steps:  95, steps per second:  69, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.147 [0.000, 2.000],  loss: 0.680243, mae: 22.398848, mean_q: -32.450283, mean_eps: 0.150208\n",
      " 141773/150000: episode: 701, duration: 1.053s, episode steps:  93, steps per second:  88, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.717154, mae: 22.318572, mean_q: -32.343313, mean_eps: 0.149644\n",
      " 141875/150000: episode: 702, duration: 1.257s, episode steps: 102, steps per second:  81, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.758537, mae: 22.610697, mean_q: -32.749114, mean_eps: 0.149059\n",
      " 141954/150000: episode: 703, duration: 1.014s, episode steps:  79, steps per second:  78, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.541515, mae: 22.691176, mean_q: -32.950579, mean_eps: 0.148516\n",
      " 142044/150000: episode: 704, duration: 1.106s, episode steps:  90, steps per second:  81, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.736995, mae: 22.453411, mean_q: -32.486217, mean_eps: 0.148009\n",
      " 142150/150000: episode: 705, duration: 1.304s, episode steps: 106, steps per second:  81, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.934 [0.000, 2.000],  loss: 0.828464, mae: 22.173628, mean_q: -32.033151, mean_eps: 0.147421\n",
      " 142272/150000: episode: 706, duration: 1.479s, episode steps: 122, steps per second:  82, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.902 [0.000, 2.000],  loss: 0.636052, mae: 22.466708, mean_q: -32.548085, mean_eps: 0.146737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 142390/150000: episode: 707, duration: 1.452s, episode steps: 118, steps per second:  81, episode reward: -117.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.780 [0.000, 2.000],  loss: 0.736946, mae: 22.330822, mean_q: -32.335937, mean_eps: 0.146017\n",
      " 142489/150000: episode: 708, duration: 1.268s, episode steps:  99, steps per second:  78, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.909 [0.000, 2.000],  loss: 0.651389, mae: 22.475500, mean_q: -32.568982, mean_eps: 0.145366\n",
      " 142573/150000: episode: 709, duration: 1.042s, episode steps:  84, steps per second:  81, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.940 [0.000, 2.000],  loss: 0.722776, mae: 22.071967, mean_q: -31.973662, mean_eps: 0.144817\n",
      " 142673/150000: episode: 710, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.740 [0.000, 2.000],  loss: 0.636538, mae: 22.221164, mean_q: -32.208127, mean_eps: 0.144265\n",
      " 142795/150000: episode: 711, duration: 1.543s, episode steps: 122, steps per second:  79, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.795 [0.000, 2.000],  loss: 0.670978, mae: 22.051002, mean_q: -31.952902, mean_eps: 0.143599\n",
      " 142917/150000: episode: 712, duration: 1.567s, episode steps: 122, steps per second:  78, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.762 [0.000, 2.000],  loss: 0.624723, mae: 22.132228, mean_q: -32.094114, mean_eps: 0.142867\n",
      " 143006/150000: episode: 713, duration: 1.131s, episode steps:  89, steps per second:  79, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.888 [0.000, 2.000],  loss: 0.682249, mae: 22.504063, mean_q: -32.614227, mean_eps: 0.142234\n",
      " 143109/150000: episode: 714, duration: 1.312s, episode steps: 103, steps per second:  78, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.806334, mae: 22.624741, mean_q: -32.731588, mean_eps: 0.141658\n",
      " 143213/150000: episode: 715, duration: 1.292s, episode steps: 104, steps per second:  81, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.769 [0.000, 2.000],  loss: 0.741129, mae: 22.396942, mean_q: -32.357989, mean_eps: 0.141037\n",
      " 143381/150000: episode: 716, duration: 2.122s, episode steps: 168, steps per second:  79, episode reward: -167.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.542 [0.000, 2.000],  loss: 0.700334, mae: 22.451792, mean_q: -32.469980, mean_eps: 0.140221\n",
      " 143529/150000: episode: 717, duration: 1.884s, episode steps: 148, steps per second:  79, episode reward: -147.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.615 [0.000, 2.000],  loss: 0.699551, mae: 22.383626, mean_q: -32.377221, mean_eps: 0.139273\n",
      " 143622/150000: episode: 718, duration: 1.182s, episode steps:  93, steps per second:  79, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.720 [0.000, 2.000],  loss: 0.581774, mae: 22.564092, mean_q: -32.666336, mean_eps: 0.138550\n",
      " 143724/150000: episode: 719, duration: 1.305s, episode steps: 102, steps per second:  78, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.706 [0.000, 2.000],  loss: 0.679125, mae: 22.436013, mean_q: -32.442552, mean_eps: 0.137965\n",
      " 143862/150000: episode: 720, duration: 1.776s, episode steps: 138, steps per second:  78, episode reward: -137.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.790 [0.000, 2.000],  loss: 0.683551, mae: 22.685015, mean_q: -32.777069, mean_eps: 0.137245\n",
      " 143952/150000: episode: 721, duration: 1.144s, episode steps:  90, steps per second:  79, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.844 [0.000, 2.000],  loss: 0.579364, mae: 22.245356, mean_q: -32.177862, mean_eps: 0.136561\n",
      " 144038/150000: episode: 722, duration: 1.097s, episode steps:  86, steps per second:  78, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.744 [0.000, 2.000],  loss: 0.795228, mae: 22.257247, mean_q: -32.081839, mean_eps: 0.136033\n",
      " 144136/150000: episode: 723, duration: 1.249s, episode steps:  98, steps per second:  78, episode reward: -97.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.745 [0.000, 2.000],  loss: 0.784597, mae: 22.601376, mean_q: -32.624383, mean_eps: 0.135481\n",
      " 144250/150000: episode: 724, duration: 1.473s, episode steps: 114, steps per second:  77, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.693 [0.000, 2.000],  loss: 0.743900, mae: 22.096713, mean_q: -31.963071, mean_eps: 0.134845\n",
      " 144364/150000: episode: 725, duration: 1.446s, episode steps: 114, steps per second:  79, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.061 [0.000, 2.000],  loss: 0.650237, mae: 22.363563, mean_q: -32.350088, mean_eps: 0.134161\n",
      " 144457/150000: episode: 726, duration: 1.197s, episode steps:  93, steps per second:  78, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.753 [0.000, 2.000],  loss: 0.624212, mae: 22.543169, mean_q: -32.599617, mean_eps: 0.133540\n",
      " 144547/150000: episode: 727, duration: 1.212s, episode steps:  90, steps per second:  74, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.678 [0.000, 2.000],  loss: 0.682726, mae: 22.383789, mean_q: -32.374146, mean_eps: 0.132991\n",
      " 144675/150000: episode: 728, duration: 1.952s, episode steps: 128, steps per second:  66, episode reward: -127.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.688 [0.000, 2.000],  loss: 0.651931, mae: 22.498243, mean_q: -32.542894, mean_eps: 0.132337\n",
      " 144777/150000: episode: 729, duration: 1.301s, episode steps: 102, steps per second:  78, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.716 [0.000, 2.000],  loss: 0.631709, mae: 22.215686, mean_q: -32.099419, mean_eps: 0.131647\n",
      " 144862/150000: episode: 730, duration: 1.055s, episode steps:  85, steps per second:  81, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.847 [0.000, 2.000],  loss: 0.614223, mae: 22.434773, mean_q: -32.468181, mean_eps: 0.131086\n",
      " 144966/150000: episode: 731, duration: 1.420s, episode steps: 104, steps per second:  73, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.779 [0.000, 2.000],  loss: 0.685547, mae: 22.664865, mean_q: -32.781309, mean_eps: 0.130519\n",
      " 145043/150000: episode: 732, duration: 0.951s, episode steps:  77, steps per second:  81, episode reward: -76.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.827294, mae: 22.306752, mean_q: -32.273355, mean_eps: 0.129976\n",
      " 145122/150000: episode: 733, duration: 1.063s, episode steps:  79, steps per second:  74, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.861 [0.000, 2.000],  loss: 0.749679, mae: 22.640108, mean_q: -32.698711, mean_eps: 0.129508\n",
      " 145229/150000: episode: 734, duration: 1.391s, episode steps: 107, steps per second:  77, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 0.607 [0.000, 2.000],  loss: 0.736932, mae: 22.444399, mean_q: -32.383352, mean_eps: 0.128950\n",
      " 145350/150000: episode: 735, duration: 1.685s, episode steps: 121, steps per second:  72, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.686 [0.000, 2.000],  loss: 0.736901, mae: 22.468255, mean_q: -32.488276, mean_eps: 0.128266\n",
      " 145452/150000: episode: 736, duration: 1.364s, episode steps: 102, steps per second:  75, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.765 [0.000, 2.000],  loss: 0.703759, mae: 22.807448, mean_q: -32.973133, mean_eps: 0.127597\n",
      " 145542/150000: episode: 737, duration: 1.190s, episode steps:  90, steps per second:  76, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.889 [0.000, 2.000],  loss: 0.615135, mae: 22.646951, mean_q: -32.761605, mean_eps: 0.127021\n",
      " 145620/150000: episode: 738, duration: 1.265s, episode steps:  78, steps per second:  62, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 0.756 [0.000, 2.000],  loss: 0.662502, mae: 22.492581, mean_q: -32.544650, mean_eps: 0.126517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 145742/150000: episode: 739, duration: 1.840s, episode steps: 122, steps per second:  66, episode reward: -121.000, mean reward: -0.992 [-1.000,  0.000], mean action: 0.590 [0.000, 2.000],  loss: 0.629177, mae: 22.734914, mean_q: -32.894172, mean_eps: 0.125917\n",
      " 145832/150000: episode: 740, duration: 1.044s, episode steps:  90, steps per second:  86, episode reward: -89.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.956 [0.000, 2.000],  loss: 0.610255, mae: 22.711421, mean_q: -32.903394, mean_eps: 0.125281\n",
      " 145927/150000: episode: 741, duration: 1.123s, episode steps:  95, steps per second:  85, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 0.590614, mae: 22.528282, mean_q: -32.642552, mean_eps: 0.124726\n",
      " 146062/150000: episode: 742, duration: 1.648s, episode steps: 135, steps per second:  82, episode reward: -134.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.615 [0.000, 2.000],  loss: 0.734329, mae: 22.349256, mean_q: -32.304621, mean_eps: 0.124036\n",
      " 146133/150000: episode: 743, duration: 0.871s, episode steps:  71, steps per second:  81, episode reward: -70.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.028 [0.000, 2.000],  loss: 0.741426, mae: 22.407973, mean_q: -32.415988, mean_eps: 0.123418\n",
      " 146219/150000: episode: 744, duration: 1.036s, episode steps:  86, steps per second:  83, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.763862, mae: 22.262265, mean_q: -32.194858, mean_eps: 0.122947\n",
      " 146318/150000: episode: 745, duration: 1.239s, episode steps:  99, steps per second:  80, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.737845, mae: 22.326744, mean_q: -32.297299, mean_eps: 0.122392\n",
      " 146402/150000: episode: 746, duration: 1.049s, episode steps:  84, steps per second:  80, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.669836, mae: 22.014669, mean_q: -31.846339, mean_eps: 0.121843\n",
      " 146502/150000: episode: 747, duration: 1.343s, episode steps: 100, steps per second:  74, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.070 [0.000, 2.000],  loss: 0.549436, mae: 22.642392, mean_q: -32.807013, mean_eps: 0.121291\n",
      " 146605/150000: episode: 748, duration: 1.168s, episode steps: 103, steps per second:  88, episode reward: -102.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.922 [0.000, 2.000],  loss: 0.672426, mae: 22.396212, mean_q: -32.404145, mean_eps: 0.120682\n",
      " 146699/150000: episode: 749, duration: 1.088s, episode steps:  94, steps per second:  86, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.832574, mae: 22.401009, mean_q: -32.445908, mean_eps: 0.120091\n",
      " 146811/150000: episode: 750, duration: 1.262s, episode steps: 112, steps per second:  89, episode reward: -111.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.027 [0.000, 2.000],  loss: 0.658443, mae: 22.209047, mean_q: -32.161558, mean_eps: 0.119473\n",
      " 146896/150000: episode: 751, duration: 1.317s, episode steps:  85, steps per second:  65, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.012 [0.000, 2.000],  loss: 0.673847, mae: 22.454567, mean_q: -32.558531, mean_eps: 0.118882\n",
      " 147010/150000: episode: 752, duration: 2.043s, episode steps: 114, steps per second:  56, episode reward: -113.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.053 [0.000, 2.000],  loss: 0.659303, mae: 22.391392, mean_q: -32.372665, mean_eps: 0.118285\n",
      " 147080/150000: episode: 753, duration: 0.967s, episode steps:  70, steps per second:  72, episode reward: -69.000, mean reward: -0.986 [-1.000,  0.000], mean action: 1.086 [0.000, 2.000],  loss: 0.770756, mae: 22.495407, mean_q: -32.560640, mean_eps: 0.117733\n",
      " 147158/150000: episode: 754, duration: 0.892s, episode steps:  78, steps per second:  87, episode reward: -77.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.051 [0.000, 2.000],  loss: 0.740592, mae: 22.435812, mean_q: -32.534670, mean_eps: 0.117289\n",
      " 147255/150000: episode: 755, duration: 1.243s, episode steps:  97, steps per second:  78, episode reward: -96.000, mean reward: -0.990 [-1.000,  0.000], mean action: 0.928 [0.000, 2.000],  loss: 0.762708, mae: 22.445572, mean_q: -32.503851, mean_eps: 0.116764\n",
      " 147342/150000: episode: 756, duration: 1.206s, episode steps:  87, steps per second:  72, episode reward: -86.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.126 [0.000, 2.000],  loss: 0.631000, mae: 22.409913, mean_q: -32.415469, mean_eps: 0.116212\n",
      " 147426/150000: episode: 757, duration: 0.977s, episode steps:  84, steps per second:  86, episode reward: -83.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.143 [0.000, 2.000],  loss: 0.643080, mae: 22.379136, mean_q: -32.430586, mean_eps: 0.115699\n",
      " 147508/150000: episode: 758, duration: 1.093s, episode steps:  82, steps per second:  75, episode reward: -81.000, mean reward: -0.988 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.695168, mae: 22.800396, mean_q: -33.046798, mean_eps: 0.115201\n",
      " 147627/150000: episode: 759, duration: 1.736s, episode steps: 119, steps per second:  69, episode reward: -118.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.101 [0.000, 2.000],  loss: 0.729049, mae: 22.351852, mean_q: -32.342871, mean_eps: 0.114598\n",
      " 147731/150000: episode: 760, duration: 1.248s, episode steps: 104, steps per second:  83, episode reward: -103.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.115 [0.000, 2.000],  loss: 0.672881, mae: 22.503148, mean_q: -32.594014, mean_eps: 0.113929\n",
      " 147825/150000: episode: 761, duration: 1.254s, episode steps:  94, steps per second:  75, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.011 [0.000, 2.000],  loss: 0.741195, mae: 22.254203, mean_q: -32.221437, mean_eps: 0.113335\n",
      " 147889/150000: episode: 762, duration: 0.736s, episode steps:  64, steps per second:  87, episode reward: -63.000, mean reward: -0.984 [-1.000,  0.000], mean action: 1.172 [0.000, 2.000],  loss: 0.569153, mae: 22.338171, mean_q: -32.368897, mean_eps: 0.112861\n",
      " 147995/150000: episode: 763, duration: 1.484s, episode steps: 106, steps per second:  71, episode reward: -105.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.066 [0.000, 2.000],  loss: 0.639793, mae: 21.972396, mean_q: -31.805995, mean_eps: 0.112351\n",
      " 148089/150000: episode: 764, duration: 1.229s, episode steps:  94, steps per second:  76, episode reward: -93.000, mean reward: -0.989 [-1.000,  0.000], mean action: 0.957 [0.000, 2.000],  loss: 0.844040, mae: 22.062782, mean_q: -31.823205, mean_eps: 0.111751\n",
      " 148185/150000: episode: 765, duration: 1.117s, episode steps:  96, steps per second:  86, episode reward: -95.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.724506, mae: 22.343208, mean_q: -32.340183, mean_eps: 0.111181\n",
      " 148264/150000: episode: 766, duration: 0.887s, episode steps:  79, steps per second:  89, episode reward: -78.000, mean reward: -0.987 [-1.000,  0.000], mean action: 1.165 [0.000, 2.000],  loss: 0.677928, mae: 22.452163, mean_q: -32.532440, mean_eps: 0.110656\n",
      " 148357/150000: episode: 767, duration: 1.236s, episode steps:  93, steps per second:  75, episode reward: -92.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.290 [0.000, 2.000],  loss: 0.726056, mae: 21.860611, mean_q: -31.564810, mean_eps: 0.110140\n",
      " 148440/150000: episode: 768, duration: 0.951s, episode steps:  83, steps per second:  87, episode reward: -82.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.060 [0.000, 2.000],  loss: 0.618213, mae: 21.987325, mean_q: -31.864013, mean_eps: 0.109612\n",
      " 148539/150000: episode: 769, duration: 1.234s, episode steps:  99, steps per second:  80, episode reward: -98.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.091 [0.000, 2.000],  loss: 0.715845, mae: 21.971539, mean_q: -31.781344, mean_eps: 0.109066\n",
      " 148639/150000: episode: 770, duration: 1.281s, episode steps: 100, steps per second:  78, episode reward: -99.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.080 [0.000, 2.000],  loss: 0.729008, mae: 22.281852, mean_q: -32.287798, mean_eps: 0.108469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 148725/150000: episode: 771, duration: 0.955s, episode steps:  86, steps per second:  90, episode reward: -85.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.198 [0.000, 2.000],  loss: 0.683938, mae: 22.256199, mean_q: -32.188518, mean_eps: 0.107911\n",
      " 148810/150000: episode: 772, duration: 0.947s, episode steps:  85, steps per second:  90, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.082 [0.000, 2.000],  loss: 0.612877, mae: 22.238542, mean_q: -32.176607, mean_eps: 0.107398\n",
      " 148901/150000: episode: 773, duration: 1.041s, episode steps:  91, steps per second:  87, episode reward: -90.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.619375, mae: 22.170071, mean_q: -32.067989, mean_eps: 0.106870\n",
      " 148969/150000: episode: 774, duration: 0.787s, episode steps:  68, steps per second:  86, episode reward: -67.000, mean reward: -0.985 [-1.000,  0.000], mean action: 1.103 [0.000, 2.000],  loss: 0.719771, mae: 22.269679, mean_q: -32.215967, mean_eps: 0.106393\n",
      " 149084/150000: episode: 775, duration: 1.611s, episode steps: 115, steps per second:  71, episode reward: -114.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.113 [0.000, 2.000],  loss: 0.675553, mae: 22.304042, mean_q: -32.227079, mean_eps: 0.105844\n",
      " 149186/150000: episode: 776, duration: 1.329s, episode steps: 102, steps per second:  77, episode reward: -101.000, mean reward: -0.990 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.713613, mae: 21.935290, mean_q: -31.670298, mean_eps: 0.105193\n",
      " 149271/150000: episode: 777, duration: 1.152s, episode steps:  85, steps per second:  74, episode reward: -84.000, mean reward: -0.988 [-1.000,  0.000], mean action: 1.235 [0.000, 2.000],  loss: 0.765027, mae: 22.161093, mean_q: -32.016888, mean_eps: 0.104632\n",
      " 149392/150000: episode: 778, duration: 1.368s, episode steps: 121, steps per second:  88, episode reward: -120.000, mean reward: -0.992 [-1.000,  0.000], mean action: 1.463 [0.000, 2.000],  loss: 0.630877, mae: 22.201829, mean_q: -32.080703, mean_eps: 0.104014\n",
      " 149526/150000: episode: 779, duration: 1.977s, episode steps: 134, steps per second:  68, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.470 [0.000, 2.000],  loss: 0.606837, mae: 22.303888, mean_q: -32.290965, mean_eps: 0.103249\n",
      " 149621/150000: episode: 780, duration: 1.411s, episode steps:  95, steps per second:  67, episode reward: -94.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.389 [0.000, 2.000],  loss: 0.657752, mae: 22.164731, mean_q: -32.063649, mean_eps: 0.102562\n",
      " 149710/150000: episode: 781, duration: 1.016s, episode steps:  89, steps per second:  88, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.056 [0.000, 2.000],  loss: 0.586164, mae: 21.989266, mean_q: -31.833878, mean_eps: 0.102010\n",
      " 149817/150000: episode: 782, duration: 1.537s, episode steps: 107, steps per second:  70, episode reward: -106.000, mean reward: -0.991 [-1.000,  0.000], mean action: 1.355 [0.000, 2.000],  loss: 0.630304, mae: 22.139007, mean_q: -32.055296, mean_eps: 0.101422\n",
      " 149906/150000: episode: 783, duration: 1.064s, episode steps:  89, steps per second:  84, episode reward: -88.000, mean reward: -0.989 [-1.000,  0.000], mean action: 1.348 [0.000, 2.000],  loss: 0.601258, mae: 22.312644, mean_q: -32.319581, mean_eps: 0.100834\n",
      "done, took 2023.835 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18403f787f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "framed-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -500.000, steps: 500\n",
      "Episode 2: reward: -114.000, steps: 115\n",
      "Episode 3: reward: -500.000, steps: 500\n",
      "Episode 4: reward: -500.000, steps: 500\n",
      "Episode 5: reward: -500.000, steps: 500\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3044ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

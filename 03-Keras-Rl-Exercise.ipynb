{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advisory-score",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Copyright by Pierian Data Inc.</em></center>\n",
    "<center><em>For more information, visit us at <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-novel",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-agency",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement your first keras-rl agent based on the **Acrobot** environment (https://gym.openai.com/envs/Acrobot-v1/) <br />\n",
    "The goal of this environment is to maneuver the robot arm upwards above the line with as little steps as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-conducting",
   "metadata": {},
   "source": [
    "**TASK: Import necessary libraries** <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-tenant",
   "metadata": {},
   "source": [
    "**TASK: Create the environment** <br />\n",
    "The name is: *Acrobot-v1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "israeli-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: 3\n",
      "Observation Space: (6,)\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape\n",
    "print(f\"Action Space: {env.action_space.n}\")\n",
    "print(f\"Observation Space: {num_observations}\")\n",
    "\n",
    "assert num_actions == 3 and num_observations == (6,) , \"Wrong environment!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-tuesday",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent** <br />\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want! <br />\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1, ) + num_observations))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Dense(num_actions))\n",
    "model.add(Activation(\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061349a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mixture",
   "metadata": {},
   "source": [
    "**TASK: Initialize the circular buffer**<br />\n",
    "Make sure you set the limit appropriately (50000 works well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "short-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860e4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-grain",
   "metadata": {},
   "source": [
    "**TASK: Use the epsilon greedy action selection strategy with *decaying* epsilon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polished-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2248a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr=\"eps\",\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=50000\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-straight",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=1000, *target_model_update*=1000, *batch_size*=32 and *gamma*=0.99 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "terminal-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=num_actions, memory=memory, \n",
    "               nb_steps_warmup=1000, target_model_update=1000, policy=policy,\n",
    "               gamma=0.99, batch_size=32\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-shooting",
   "metadata": {},
   "source": [
    "**TASK: Compile the model** <br />\n",
    "Feel free to explore the effects of different optimizers and learning rates.\n",
    "You can try Adam with a learning rate of 1e-3 as a first guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "damaged-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-belgium",
   "metadata": {},
   "source": [
    "**TASK: Fit the model** <br />\n",
    "150,000 steps should be a very good starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adverse-determination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500/50000: episode: 1, duration: 1.266s, episode steps: 500, steps per second: 395, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1000/50000: episode: 2, duration: 0.713s, episode steps: 500, steps per second: 701, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VictorHernandez-Urbi\\anaconda3\\envs\\env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1500/50000: episode: 3, duration: 6.053s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.998 [0.000, 2.000],  loss: 0.013619, mae: 0.640838, mean_q: -0.881434, mean_eps: 0.977500\n",
      "  2000/50000: episode: 4, duration: 5.934s, episode steps: 500, steps per second:  84, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.000578, mae: 0.639652, mean_q: -0.923714, mean_eps: 0.968509\n",
      "  2500/50000: episode: 5, duration: 5.773s, episode steps: 500, steps per second:  87, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.008 [0.000, 2.000],  loss: 0.007981, mae: 1.396117, mean_q: -2.037532, mean_eps: 0.959509\n",
      "  3000/50000: episode: 6, duration: 5.676s, episode steps: 500, steps per second:  88, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.001321, mae: 1.391162, mean_q: -2.052871, mean_eps: 0.950509\n",
      "  3500/50000: episode: 7, duration: 5.864s, episode steps: 500, steps per second:  85, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.013199, mae: 2.152375, mean_q: -3.163664, mean_eps: 0.941509\n",
      "  4000/50000: episode: 8, duration: 6.119s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.004228, mae: 2.145409, mean_q: -3.177659, mean_eps: 0.932509\n",
      "  4500/50000: episode: 9, duration: 6.094s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000],  loss: 0.015248, mae: 2.964922, mean_q: -4.378034, mean_eps: 0.923509\n",
      "  5000/50000: episode: 10, duration: 6.061s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.010189, mae: 2.960748, mean_q: -4.381626, mean_eps: 0.914509\n",
      "  5500/50000: episode: 11, duration: 6.090s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.019291, mae: 3.732157, mean_q: -5.519575, mean_eps: 0.905509\n",
      "  6000/50000: episode: 12, duration: 6.122s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.016258, mae: 3.727502, mean_q: -5.518587, mean_eps: 0.896509\n",
      "  6500/50000: episode: 13, duration: 6.148s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.992 [0.000, 2.000],  loss: 0.040121, mae: 4.467467, mean_q: -6.592988, mean_eps: 0.887509\n",
      "  7000/50000: episode: 14, duration: 6.151s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.004 [0.000, 2.000],  loss: 0.029088, mae: 4.460731, mean_q: -6.600250, mean_eps: 0.878509\n",
      "  7500/50000: episode: 15, duration: 6.091s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.043647, mae: 5.199036, mean_q: -7.682176, mean_eps: 0.869509\n",
      "  8000/50000: episode: 16, duration: 6.064s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.046007, mae: 5.199465, mean_q: -7.688712, mean_eps: 0.860509\n",
      "  8500/50000: episode: 17, duration: 6.062s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000],  loss: 0.063365, mae: 5.969823, mean_q: -8.822159, mean_eps: 0.851509\n",
      "  9000/50000: episode: 18, duration: 6.044s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.057190, mae: 5.964312, mean_q: -8.830481, mean_eps: 0.842509\n",
      "  9500/50000: episode: 19, duration: 6.280s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.072498, mae: 6.538676, mean_q: -9.667760, mean_eps: 0.833509\n",
      " 10000/50000: episode: 20, duration: 6.153s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.085660, mae: 6.540935, mean_q: -9.661903, mean_eps: 0.824509\n",
      " 10500/50000: episode: 21, duration: 6.167s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000],  loss: 0.097701, mae: 7.216791, mean_q: -10.673452, mean_eps: 0.815509\n",
      " 11000/50000: episode: 22, duration: 6.079s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.089627, mae: 7.215885, mean_q: -10.689282, mean_eps: 0.806509\n",
      " 11500/50000: episode: 23, duration: 6.149s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.978 [0.000, 2.000],  loss: 0.114726, mae: 7.845340, mean_q: -11.605108, mean_eps: 0.797509\n",
      " 12000/50000: episode: 24, duration: 5.667s, episode steps: 500, steps per second:  88, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000],  loss: 0.119913, mae: 7.838220, mean_q: -11.598355, mean_eps: 0.788509\n",
      " 12500/50000: episode: 25, duration: 5.767s, episode steps: 500, steps per second:  87, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000],  loss: 0.157154, mae: 8.559625, mean_q: -12.653401, mean_eps: 0.779509\n",
      " 13000/50000: episode: 26, duration: 6.164s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000],  loss: 0.181031, mae: 8.550924, mean_q: -12.653693, mean_eps: 0.770509\n",
      " 13500/50000: episode: 27, duration: 6.090s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 0.162198, mae: 8.803350, mean_q: -13.014094, mean_eps: 0.761509\n",
      " 13903/50000: episode: 28, duration: 4.882s, episode steps: 403, steps per second:  83, episode reward: -402.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.995 [0.000, 2.000],  loss: 0.158765, mae: 8.802684, mean_q: -13.036442, mean_eps: 0.753382\n",
      " 14403/50000: episode: 29, duration: 6.128s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.149383, mae: 9.153637, mean_q: -13.554213, mean_eps: 0.745255\n",
      " 14903/50000: episode: 30, duration: 6.112s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000],  loss: 0.168685, mae: 9.238900, mean_q: -13.687007, mean_eps: 0.736255\n",
      " 15403/50000: episode: 31, duration: 6.080s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.154104, mae: 9.643914, mean_q: -14.281554, mean_eps: 0.727255\n",
      " 15903/50000: episode: 32, duration: 6.109s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.248946, mae: 9.732320, mean_q: -14.389203, mean_eps: 0.718255\n",
      " 16310/50000: episode: 33, duration: 4.902s, episode steps: 407, steps per second:  83, episode reward: -406.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.124337, mae: 10.119996, mean_q: -15.000757, mean_eps: 0.710092\n",
      " 16810/50000: episode: 34, duration: 6.159s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.002 [0.000, 2.000],  loss: 0.207098, mae: 10.237662, mean_q: -15.149366, mean_eps: 0.701929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17310/50000: episode: 35, duration: 6.061s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.231627, mae: 10.548152, mean_q: -15.606153, mean_eps: 0.692929\n",
      " 17810/50000: episode: 36, duration: 6.094s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 0.224074, mae: 10.736222, mean_q: -15.890663, mean_eps: 0.683929\n",
      " 18205/50000: episode: 37, duration: 4.838s, episode steps: 395, steps per second:  82, episode reward: -394.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.043 [0.000, 2.000],  loss: 0.269359, mae: 10.930934, mean_q: -16.152099, mean_eps: 0.675874\n",
      " 18705/50000: episode: 38, duration: 6.136s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.022 [0.000, 2.000],  loss: 0.200168, mae: 11.107694, mean_q: -16.460044, mean_eps: 0.667819\n",
      " 19205/50000: episode: 39, duration: 6.103s, episode steps: 500, steps per second:  82, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.306178, mae: 11.329775, mean_q: -16.745416, mean_eps: 0.658819\n",
      " 19705/50000: episode: 40, duration: 5.998s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000],  loss: 0.267647, mae: 11.615578, mean_q: -17.196590, mean_eps: 0.649819\n",
      " 20039/50000: episode: 41, duration: 4.081s, episode steps: 334, steps per second:  82, episode reward: -333.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.039 [0.000, 2.000],  loss: 0.240364, mae: 11.667549, mean_q: -17.277711, mean_eps: 0.642313\n",
      " 20539/50000: episode: 42, duration: 6.159s, episode steps: 500, steps per second:  81, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.246845, mae: 12.046353, mean_q: -17.846466, mean_eps: 0.634807\n",
      " 20893/50000: episode: 43, duration: 4.600s, episode steps: 354, steps per second:  77, episode reward: -353.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.071 [0.000, 2.000],  loss: 0.262390, mae: 12.021397, mean_q: -17.807532, mean_eps: 0.627121\n",
      " 21184/50000: episode: 44, duration: 3.879s, episode steps: 291, steps per second:  75, episode reward: -290.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.038 [0.000, 2.000],  loss: 0.312822, mae: 12.300789, mean_q: -18.193334, mean_eps: 0.621316\n",
      " 21499/50000: episode: 45, duration: 3.763s, episode steps: 315, steps per second:  84, episode reward: -314.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.156 [0.000, 2.000],  loss: 0.420545, mae: 12.442357, mean_q: -18.387283, mean_eps: 0.615862\n",
      " 21775/50000: episode: 46, duration: 3.080s, episode steps: 276, steps per second:  90, episode reward: -275.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.134 [0.000, 2.000],  loss: 0.351813, mae: 12.443699, mean_q: -18.404820, mean_eps: 0.610543\n",
      " 22275/50000: episode: 47, duration: 5.727s, episode steps: 500, steps per second:  87, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.307717, mae: 12.697555, mean_q: -18.797412, mean_eps: 0.603559\n",
      " 22664/50000: episode: 48, duration: 4.566s, episode steps: 389, steps per second:  85, episode reward: -388.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.183 [0.000, 2.000],  loss: 0.257261, mae: 12.907943, mean_q: -19.119507, mean_eps: 0.595558\n",
      " 23146/50000: episode: 49, duration: 6.589s, episode steps: 482, steps per second:  73, episode reward: -481.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.988 [0.000, 2.000],  loss: 0.399530, mae: 13.049175, mean_q: -19.296954, mean_eps: 0.587719\n",
      " 23561/50000: episode: 50, duration: 5.235s, episode steps: 415, steps per second:  79, episode reward: -414.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.067 [0.000, 2.000],  loss: 0.335907, mae: 13.405518, mean_q: -19.849878, mean_eps: 0.579646\n",
      " 24023/50000: episode: 51, duration: 6.001s, episode steps: 462, steps per second:  77, episode reward: -461.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.362562, mae: 13.428551, mean_q: -19.876704, mean_eps: 0.571753\n",
      " 24523/50000: episode: 52, duration: 6.410s, episode steps: 500, steps per second:  78, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000],  loss: 0.406155, mae: 13.842668, mean_q: -20.488700, mean_eps: 0.563095\n",
      " 24781/50000: episode: 53, duration: 3.249s, episode steps: 258, steps per second:  79, episode reward: -257.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.023 [0.000, 2.000],  loss: 0.293503, mae: 13.860303, mean_q: -20.549156, mean_eps: 0.556273\n",
      " 25281/50000: episode: 54, duration: 6.522s, episode steps: 500, steps per second:  77, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 0.397615, mae: 14.100322, mean_q: -20.873478, mean_eps: 0.549451\n",
      " 25653/50000: episode: 55, duration: 4.583s, episode steps: 372, steps per second:  81, episode reward: -371.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.078 [0.000, 2.000],  loss: 0.543551, mae: 14.285961, mean_q: -21.113488, mean_eps: 0.541603\n",
      " 25951/50000: episode: 56, duration: 4.125s, episode steps: 298, steps per second:  72, episode reward: -297.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.950 [0.000, 2.000],  loss: 0.346262, mae: 14.274873, mean_q: -21.137126, mean_eps: 0.535573\n",
      " 26228/50000: episode: 57, duration: 3.490s, episode steps: 277, steps per second:  79, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.874 [0.000, 2.000],  loss: 0.386662, mae: 14.536590, mean_q: -21.546388, mean_eps: 0.530398\n",
      " 26460/50000: episode: 58, duration: 2.818s, episode steps: 232, steps per second:  82, episode reward: -231.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.991 [0.000, 2.000],  loss: 0.388115, mae: 14.606165, mean_q: -21.634613, mean_eps: 0.525817\n",
      " 26845/50000: episode: 59, duration: 4.903s, episode steps: 385, steps per second:  79, episode reward: -384.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.470902, mae: 14.591364, mean_q: -21.598766, mean_eps: 0.520264\n",
      " 27345/50000: episode: 60, duration: 6.279s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000],  loss: 0.377598, mae: 14.824815, mean_q: -21.946636, mean_eps: 0.512299\n",
      " 27616/50000: episode: 61, duration: 3.270s, episode steps: 271, steps per second:  83, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.978 [0.000, 2.000],  loss: 0.516255, mae: 14.953437, mean_q: -22.100676, mean_eps: 0.505360\n",
      " 28007/50000: episode: 62, duration: 4.606s, episode steps: 391, steps per second:  85, episode reward: -390.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.010 [0.000, 2.000],  loss: 0.464002, mae: 14.965233, mean_q: -22.160287, mean_eps: 0.499402\n",
      " 28383/50000: episode: 63, duration: 4.373s, episode steps: 376, steps per second:  86, episode reward: -375.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.979 [0.000, 2.000],  loss: 0.415241, mae: 15.376576, mean_q: -22.756592, mean_eps: 0.492499\n",
      " 28645/50000: episode: 64, duration: 3.136s, episode steps: 262, steps per second:  84, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.137 [0.000, 2.000],  loss: 0.363630, mae: 15.359573, mean_q: -22.747191, mean_eps: 0.486757\n",
      " 29145/50000: episode: 65, duration: 6.029s, episode steps: 500, steps per second:  83, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.956 [0.000, 2.000],  loss: 0.639479, mae: 15.481315, mean_q: -22.874721, mean_eps: 0.479899\n",
      " 29399/50000: episode: 66, duration: 3.104s, episode steps: 254, steps per second:  82, episode reward: -253.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.079 [0.000, 2.000],  loss: 0.521163, mae: 15.819015, mean_q: -23.397728, mean_eps: 0.473113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29899/50000: episode: 67, duration: 6.246s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.938 [0.000, 2.000],  loss: 0.565819, mae: 15.846997, mean_q: -23.415014, mean_eps: 0.466327\n",
      " 30311/50000: episode: 68, duration: 5.571s, episode steps: 412, steps per second:  74, episode reward: -411.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.005 [0.000, 2.000],  loss: 0.485658, mae: 16.143714, mean_q: -23.886650, mean_eps: 0.458119\n",
      " 30692/50000: episode: 69, duration: 5.142s, episode steps: 381, steps per second:  74, episode reward: -380.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.021 [0.000, 2.000],  loss: 0.451323, mae: 16.230098, mean_q: -24.025420, mean_eps: 0.450982\n",
      " 30898/50000: episode: 70, duration: 2.503s, episode steps: 206, steps per second:  82, episode reward: -205.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.530631, mae: 16.225981, mean_q: -23.996630, mean_eps: 0.445699\n",
      " 31282/50000: episode: 71, duration: 4.816s, episode steps: 384, steps per second:  80, episode reward: -383.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.901 [0.000, 2.000],  loss: 0.654961, mae: 16.620624, mean_q: -24.539420, mean_eps: 0.440389\n",
      " 31472/50000: episode: 72, duration: 2.167s, episode steps: 190, steps per second:  88, episode reward: -189.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.184 [0.000, 2.000],  loss: 0.489682, mae: 16.753274, mean_q: -24.785131, mean_eps: 0.435223\n",
      " 31658/50000: episode: 73, duration: 2.104s, episode steps: 186, steps per second:  88, episode reward: -185.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.048 [0.000, 2.000],  loss: 0.479068, mae: 16.772045, mean_q: -24.828852, mean_eps: 0.431839\n",
      " 32158/50000: episode: 74, duration: 6.235s, episode steps: 500, steps per second:  80, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000],  loss: 0.579477, mae: 16.871738, mean_q: -24.955037, mean_eps: 0.425665\n",
      " 32420/50000: episode: 75, duration: 3.436s, episode steps: 262, steps per second:  76, episode reward: -261.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.893 [0.000, 2.000],  loss: 0.574854, mae: 17.146456, mean_q: -25.358825, mean_eps: 0.418807\n",
      " 32684/50000: episode: 76, duration: 3.370s, episode steps: 264, steps per second:  78, episode reward: -263.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.163 [0.000, 2.000],  loss: 0.443991, mae: 17.183903, mean_q: -25.459186, mean_eps: 0.414073\n",
      " 32894/50000: episode: 77, duration: 2.887s, episode steps: 210, steps per second:  73, episode reward: -209.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.481757, mae: 17.138798, mean_q: -25.377352, mean_eps: 0.409807\n",
      " 33280/50000: episode: 78, duration: 5.463s, episode steps: 386, steps per second:  71, episode reward: -385.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.093 [0.000, 2.000],  loss: 0.514863, mae: 17.470670, mean_q: -25.850574, mean_eps: 0.404443\n",
      " 33566/50000: episode: 79, duration: 3.915s, episode steps: 286, steps per second:  73, episode reward: -285.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.035 [0.000, 2.000],  loss: 0.604284, mae: 17.579768, mean_q: -25.992767, mean_eps: 0.398395\n",
      " 33806/50000: episode: 80, duration: 3.161s, episode steps: 240, steps per second:  76, episode reward: -239.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.004 [0.000, 2.000],  loss: 0.453826, mae: 17.607781, mean_q: -26.064228, mean_eps: 0.393661\n",
      " 34072/50000: episode: 81, duration: 3.317s, episode steps: 266, steps per second:  80, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.128 [0.000, 2.000],  loss: 0.609191, mae: 17.670715, mean_q: -26.107561, mean_eps: 0.389107\n",
      " 34338/50000: episode: 82, duration: 3.412s, episode steps: 266, steps per second:  78, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.124 [0.000, 2.000],  loss: 0.769363, mae: 17.890134, mean_q: -26.430777, mean_eps: 0.384319\n",
      " 34623/50000: episode: 83, duration: 3.756s, episode steps: 285, steps per second:  76, episode reward: -284.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.032 [0.000, 2.000],  loss: 0.552913, mae: 17.875087, mean_q: -26.448382, mean_eps: 0.379360\n",
      " 35047/50000: episode: 84, duration: 5.894s, episode steps: 424, steps per second:  72, episode reward: -423.000, mean reward: -0.998 [-1.000,  0.000], mean action: 0.983 [0.000, 2.000],  loss: 0.521249, mae: 17.924285, mean_q: -26.527563, mean_eps: 0.372979\n",
      " 35350/50000: episode: 85, duration: 4.049s, episode steps: 303, steps per second:  75, episode reward: -302.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.201 [0.000, 2.000],  loss: 0.666499, mae: 18.236048, mean_q: -26.936124, mean_eps: 0.366436\n",
      " 35709/50000: episode: 86, duration: 4.905s, episode steps: 359, steps per second:  73, episode reward: -358.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.524160, mae: 18.203507, mean_q: -26.925384, mean_eps: 0.360478\n",
      " 36073/50000: episode: 87, duration: 4.853s, episode steps: 364, steps per second:  75, episode reward: -363.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.058 [0.000, 2.000],  loss: 0.636524, mae: 18.229952, mean_q: -26.964761, mean_eps: 0.353971\n",
      " 36390/50000: episode: 88, duration: 4.012s, episode steps: 317, steps per second:  79, episode reward: -316.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.526626, mae: 18.516543, mean_q: -27.364606, mean_eps: 0.347842\n",
      " 36641/50000: episode: 89, duration: 3.091s, episode steps: 251, steps per second:  81, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.904 [0.000, 2.000],  loss: 0.610057, mae: 18.510579, mean_q: -27.378135, mean_eps: 0.342730\n",
      " 37141/50000: episode: 90, duration: 6.777s, episode steps: 500, steps per second:  74, episode reward: -500.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.046 [0.000, 2.000],  loss: 0.837512, mae: 18.564213, mean_q: -27.404753, mean_eps: 0.335971\n",
      " 37335/50000: episode: 91, duration: 2.866s, episode steps: 194, steps per second:  68, episode reward: -193.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.088 [0.000, 2.000],  loss: 1.024698, mae: 18.782833, mean_q: -27.706194, mean_eps: 0.329725\n",
      " 37527/50000: episode: 92, duration: 2.585s, episode steps: 192, steps per second:  74, episode reward: -191.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.141 [0.000, 2.000],  loss: 0.868140, mae: 18.837083, mean_q: -27.817345, mean_eps: 0.326251\n",
      " 37704/50000: episode: 93, duration: 2.315s, episode steps: 177, steps per second:  76, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.972 [0.000, 2.000],  loss: 0.855826, mae: 18.807224, mean_q: -27.766360, mean_eps: 0.322930\n",
      " 38016/50000: episode: 94, duration: 3.837s, episode steps: 312, steps per second:  81, episode reward: -311.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.176 [0.000, 2.000],  loss: 0.715092, mae: 18.769526, mean_q: -27.717236, mean_eps: 0.318529\n",
      " 38185/50000: episode: 95, duration: 2.310s, episode steps: 169, steps per second:  73, episode reward: -168.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.763 [0.000, 2.000],  loss: 0.518883, mae: 18.982089, mean_q: -28.038828, mean_eps: 0.314200\n",
      " 38397/50000: episode: 96, duration: 2.791s, episode steps: 212, steps per second:  76, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.778 [0.000, 2.000],  loss: 0.452696, mae: 18.964986, mean_q: -28.042747, mean_eps: 0.310771\n",
      " 38650/50000: episode: 97, duration: 3.065s, episode steps: 253, steps per second:  83, episode reward: -252.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.640 [0.000, 2.000],  loss: 0.383814, mae: 18.939020, mean_q: -28.005287, mean_eps: 0.306586\n",
      " 38922/50000: episode: 98, duration: 3.646s, episode steps: 272, steps per second:  75, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.835 [0.000, 2.000],  loss: 0.714962, mae: 18.973067, mean_q: -28.017775, mean_eps: 0.301861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39217/50000: episode: 99, duration: 3.984s, episode steps: 295, steps per second:  74, episode reward: -294.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.814 [0.000, 2.000],  loss: 0.796642, mae: 18.937146, mean_q: -27.946136, mean_eps: 0.296758\n",
      " 39478/50000: episode: 100, duration: 3.483s, episode steps: 261, steps per second:  75, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.705 [0.000, 2.000],  loss: 0.661888, mae: 19.045137, mean_q: -28.159995, mean_eps: 0.291754\n",
      " 39675/50000: episode: 101, duration: 2.620s, episode steps: 197, steps per second:  75, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.929 [0.000, 2.000],  loss: 0.635070, mae: 19.051954, mean_q: -28.158097, mean_eps: 0.287632\n",
      " 39894/50000: episode: 102, duration: 3.247s, episode steps: 219, steps per second:  67, episode reward: -218.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.968 [0.000, 2.000],  loss: 0.851955, mae: 19.016666, mean_q: -28.068147, mean_eps: 0.283888\n",
      " 40160/50000: episode: 103, duration: 3.432s, episode steps: 266, steps per second:  78, episode reward: -265.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.932 [0.000, 2.000],  loss: 0.593657, mae: 19.272129, mean_q: -28.507849, mean_eps: 0.279523\n",
      " 40386/50000: episode: 104, duration: 3.115s, episode steps: 226, steps per second:  73, episode reward: -225.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.850 [0.000, 2.000],  loss: 0.723446, mae: 19.428456, mean_q: -28.703030, mean_eps: 0.275095\n",
      " 40739/50000: episode: 105, duration: 4.533s, episode steps: 353, steps per second:  78, episode reward: -352.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.000 [0.000, 2.000],  loss: 0.850768, mae: 19.378745, mean_q: -28.610563, mean_eps: 0.269884\n",
      " 41010/50000: episode: 106, duration: 3.657s, episode steps: 271, steps per second:  74, episode reward: -270.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.845 [0.000, 2.000],  loss: 0.704624, mae: 19.334099, mean_q: -28.549496, mean_eps: 0.264268\n",
      " 41249/50000: episode: 107, duration: 3.363s, episode steps: 239, steps per second:  71, episode reward: -238.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.912 [0.000, 2.000],  loss: 0.709934, mae: 19.791527, mean_q: -29.257540, mean_eps: 0.259678\n",
      " 41510/50000: episode: 108, duration: 3.772s, episode steps: 261, steps per second:  69, episode reward: -260.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.816 [0.000, 2.000],  loss: 0.858163, mae: 19.765096, mean_q: -29.172065, mean_eps: 0.255178\n",
      " 41730/50000: episode: 109, duration: 3.085s, episode steps: 220, steps per second:  71, episode reward: -219.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.818 [0.000, 2.000],  loss: 0.754268, mae: 19.726510, mean_q: -29.099582, mean_eps: 0.250849\n",
      " 41921/50000: episode: 110, duration: 2.576s, episode steps: 191, steps per second:  74, episode reward: -190.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.785 [0.000, 2.000],  loss: 0.746023, mae: 19.738057, mean_q: -29.139559, mean_eps: 0.247150\n",
      " 42100/50000: episode: 111, duration: 2.243s, episode steps: 179, steps per second:  80, episode reward: -178.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.783728, mae: 19.927212, mean_q: -29.413024, mean_eps: 0.243820\n",
      " 42274/50000: episode: 112, duration: 2.265s, episode steps: 174, steps per second:  77, episode reward: -173.000, mean reward: -0.994 [-1.000,  0.000], mean action: 0.718 [0.000, 2.000],  loss: 0.854850, mae: 20.056906, mean_q: -29.574361, mean_eps: 0.240643\n",
      " 42546/50000: episode: 113, duration: 3.668s, episode steps: 272, steps per second:  74, episode reward: -271.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.801 [0.000, 2.000],  loss: 0.859776, mae: 19.993034, mean_q: -29.500198, mean_eps: 0.236629\n",
      " 42855/50000: episode: 114, duration: 3.820s, episode steps: 309, steps per second:  81, episode reward: -308.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.515 [0.000, 2.000],  loss: 0.622819, mae: 20.081737, mean_q: -29.657326, mean_eps: 0.231400\n",
      " 43107/50000: episode: 115, duration: 3.394s, episode steps: 252, steps per second:  74, episode reward: -251.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.813 [0.000, 2.000],  loss: 0.765258, mae: 20.141016, mean_q: -29.719395, mean_eps: 0.226351\n",
      " 43411/50000: episode: 116, duration: 4.226s, episode steps: 304, steps per second:  72, episode reward: -303.000, mean reward: -0.997 [-1.000,  0.000], mean action: 0.602 [0.000, 2.000],  loss: 0.927573, mae: 20.314948, mean_q: -29.971171, mean_eps: 0.221347\n",
      " 43610/50000: episode: 117, duration: 2.552s, episode steps: 199, steps per second:  78, episode reward: -198.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.698 [0.000, 2.000],  loss: 0.836014, mae: 20.279234, mean_q: -29.925063, mean_eps: 0.216820\n",
      " 43843/50000: episode: 118, duration: 2.824s, episode steps: 233, steps per second:  82, episode reward: -232.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.751 [0.000, 2.000],  loss: 0.694054, mae: 20.298833, mean_q: -29.999318, mean_eps: 0.212932\n",
      " 44056/50000: episode: 119, duration: 2.899s, episode steps: 213, steps per second:  73, episode reward: -212.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.930 [0.000, 2.000],  loss: 1.058961, mae: 20.327361, mean_q: -29.947496, mean_eps: 0.208918\n",
      " 44307/50000: episode: 120, duration: 3.259s, episode steps: 251, steps per second:  77, episode reward: -250.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.908 [0.000, 2.000],  loss: 0.829019, mae: 20.383304, mean_q: -30.097545, mean_eps: 0.204742\n",
      " 44530/50000: episode: 121, duration: 2.920s, episode steps: 223, steps per second:  76, episode reward: -222.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.951 [0.000, 2.000],  loss: 0.696787, mae: 20.345128, mean_q: -30.058906, mean_eps: 0.200476\n",
      " 44804/50000: episode: 122, duration: 3.732s, episode steps: 274, steps per second:  73, episode reward: -273.000, mean reward: -0.996 [-1.000,  0.000], mean action: 0.894 [0.000, 2.000],  loss: 0.739719, mae: 20.363721, mean_q: -30.077155, mean_eps: 0.196003\n",
      " 45018/50000: episode: 123, duration: 3.005s, episode steps: 214, steps per second:  71, episode reward: -213.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.883 [0.000, 2.000],  loss: 0.699249, mae: 20.396502, mean_q: -30.162699, mean_eps: 0.191611\n",
      " 45193/50000: episode: 124, duration: 2.417s, episode steps: 175, steps per second:  72, episode reward: -174.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.211 [0.000, 2.000],  loss: 0.678965, mae: 20.795924, mean_q: -30.707834, mean_eps: 0.188110\n",
      " 45470/50000: episode: 125, duration: 3.499s, episode steps: 277, steps per second:  79, episode reward: -276.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.155 [0.000, 2.000],  loss: 0.821610, mae: 20.845151, mean_q: -30.803598, mean_eps: 0.184042\n",
      " 45685/50000: episode: 126, duration: 2.646s, episode steps: 215, steps per second:  81, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 0.758 [0.000, 2.000],  loss: 1.006309, mae: 20.840912, mean_q: -30.755525, mean_eps: 0.179614\n",
      " 45819/50000: episode: 127, duration: 1.966s, episode steps: 134, steps per second:  68, episode reward: -133.000, mean reward: -0.993 [-1.000,  0.000], mean action: 0.963 [0.000, 2.000],  loss: 0.965634, mae: 20.796054, mean_q: -30.667480, mean_eps: 0.176473\n",
      " 45966/50000: episode: 128, duration: 1.910s, episode steps: 147, steps per second:  77, episode reward: -146.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.054 [0.000, 2.000],  loss: 0.491015, mae: 20.842668, mean_q: -30.820289, mean_eps: 0.173944\n",
      " 46143/50000: episode: 129, duration: 2.151s, episode steps: 177, steps per second:  82, episode reward: -176.000, mean reward: -0.994 [-1.000,  0.000], mean action: 1.311 [0.000, 2.000],  loss: 0.485706, mae: 20.863719, mean_q: -30.829088, mean_eps: 0.171028\n",
      " 46474/50000: episode: 130, duration: 4.269s, episode steps: 331, steps per second:  78, episode reward: -330.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.378 [0.000, 2.000],  loss: 0.644179, mae: 20.900313, mean_q: -30.876757, mean_eps: 0.166456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46705/50000: episode: 131, duration: 3.079s, episode steps: 231, steps per second:  75, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.100 [0.000, 2.000],  loss: 0.945790, mae: 20.903299, mean_q: -30.869265, mean_eps: 0.161398\n",
      " 46914/50000: episode: 132, duration: 2.973s, episode steps: 209, steps per second:  70, episode reward: -208.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.105 [0.000, 2.000],  loss: 1.068174, mae: 20.950235, mean_q: -30.894279, mean_eps: 0.157438\n",
      " 47067/50000: episode: 133, duration: 2.178s, episode steps: 153, steps per second:  70, episode reward: -152.000, mean reward: -0.993 [-1.000,  0.000], mean action: 1.261 [0.000, 2.000],  loss: 0.834501, mae: 20.905715, mean_q: -30.813558, mean_eps: 0.154180\n",
      " 47279/50000: episode: 134, duration: 2.782s, episode steps: 212, steps per second:  76, episode reward: -211.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.226 [0.000, 2.000],  loss: 0.944045, mae: 20.950751, mean_q: -30.901462, mean_eps: 0.150895\n",
      " 47629/50000: episode: 135, duration: 4.861s, episode steps: 350, steps per second:  72, episode reward: -349.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.329 [0.000, 2.000],  loss: 0.807037, mae: 20.973666, mean_q: -30.939581, mean_eps: 0.145837\n",
      " 47860/50000: episode: 136, duration: 2.829s, episode steps: 231, steps per second:  82, episode reward: -230.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.433 [0.000, 2.000],  loss: 0.724595, mae: 20.966668, mean_q: -30.959634, mean_eps: 0.140608\n",
      " 48335/50000: episode: 137, duration: 5.919s, episode steps: 475, steps per second:  80, episode reward: -474.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.537 [0.000, 2.000],  loss: 0.803376, mae: 21.272981, mean_q: -31.356159, mean_eps: 0.134254\n",
      " 48739/50000: episode: 138, duration: 5.296s, episode steps: 404, steps per second:  76, episode reward: -403.000, mean reward: -0.998 [-1.000,  0.000], mean action: 1.554 [0.000, 2.000],  loss: 0.598218, mae: 21.385191, mean_q: -31.598967, mean_eps: 0.126343\n",
      " 49108/50000: episode: 139, duration: 4.903s, episode steps: 369, steps per second:  75, episode reward: -368.000, mean reward: -0.997 [-1.000,  0.000], mean action: 1.615 [0.000, 2.000],  loss: 0.604548, mae: 21.457703, mean_q: -31.679061, mean_eps: 0.119386\n",
      " 49309/50000: episode: 140, duration: 2.533s, episode steps: 201, steps per second:  79, episode reward: -200.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.368 [0.000, 2.000],  loss: 1.073587, mae: 21.787460, mean_q: -32.143995, mean_eps: 0.114256\n",
      " 49506/50000: episode: 141, duration: 2.538s, episode steps: 197, steps per second:  78, episode reward: -196.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.254 [0.000, 2.000],  loss: 0.818736, mae: 21.736459, mean_q: -32.094650, mean_eps: 0.110674\n",
      " 49754/50000: episode: 142, duration: 3.025s, episode steps: 248, steps per second:  82, episode reward: -247.000, mean reward: -0.996 [-1.000,  0.000], mean action: 1.415 [0.000, 2.000],  loss: 0.688688, mae: 21.715664, mean_q: -32.074820, mean_eps: 0.106669\n",
      " 49969/50000: episode: 143, duration: 2.944s, episode steps: 215, steps per second:  73, episode reward: -214.000, mean reward: -0.995 [-1.000,  0.000], mean action: 1.507 [0.000, 2.000],  loss: 1.097418, mae: 21.735688, mean_q: -32.043762, mean_eps: 0.102502\n",
      "done, took 620.505 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc03f45780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-nightmare",
   "metadata": {},
   "source": [
    "**TASK: Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "framed-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -500.000, steps: 500\n",
      "Episode 2: reward: -500.000, steps: 500\n",
      "Episode 3: reward: -500.000, steps: 500\n",
      "Episode 4: reward: -500.000, steps: 500\n",
      "Episode 5: reward: -500.000, steps: 500\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe1859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
